{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/datasets/home/home-01/26/826/yul358/CSE291E00'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory 'hw2': File exists\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir hw2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/datasets/home/home-01/26/826/yul358/CSE291E00/hw2\n"
     ]
    }
   ],
   "source": [
    "cd hw2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'Sentence-VAE' already exists and is not an empty directory.\r\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/timbmg/Sentence-VAE.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/datasets/home/home-01/26/826/yul358/CSE291E00/hw2/Sentence-VAE\n"
     ]
    }
   ],
   "source": [
    "cd Sentence-VAE/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AE.py           dowloaddata.sh   \u001b[0m\u001b[01;34mlogs\u001b[0m/                simple-examples.tgz\r\n",
      "README.md       \u001b[01;34mdumps\u001b[0m/           model.py             simple-examples.tgz.1\r\n",
      "\u001b[01;34m__pycache__\u001b[0m/    \u001b[01;34mfigs\u001b[0m/            ptb.py               train.py\r\n",
      "baselineRNN.py  getNLLforKN.py   requirements.txt     trainRNN.py\r\n",
      "\u001b[01;34mbin\u001b[0m/            inference.py     sampleZAndGetTSV.py  utils.py\r\n",
      "\u001b[01;34mdata\u001b[0m/           inferenceRNN.py  \u001b[01;34msimple-examples\u001b[0m/\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy==1.14.5 in /datasets/home/home-01/26/826/yul358/.local/lib/python3.6/site-packages (from -r requirements.txt (line 1)) (1.14.5)\n",
      "Requirement already satisfied: nltk==3.3 in /datasets/home/home-01/26/826/yul358/.local/lib/python3.6/site-packages (from -r requirements.txt (line 2)) (3.3)\n",
      "Requirement already satisfied: torch==0.3.1 in /datasets/home/home-01/26/826/yul358/.local/lib/python3.6/site-packages (from -r requirements.txt (line 3)) (0.3.1)\n",
      "Requirement already satisfied: tensorboardX==1.4 in /datasets/home/home-01/26/826/yul358/.local/lib/python3.6/site-packages (from -r requirements.txt (line 4)) (1.4)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from nltk==3.3->-r requirements.txt (line 2)) (1.12.0)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.6/site-packages (from torch==0.3.1->-r requirements.txt (line 3)) (3.13)\n",
      "Requirement already satisfied: protobuf>=3.2.0 in /opt/conda/lib/python3.6/site-packages (from tensorboardX==1.4->-r requirements.txt (line 4)) (3.6.1)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.6/site-packages (from protobuf>=3.2.0->tensorboardX==1.4->-r requirements.txt (line 4)) (39.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory 'data': File exists\n",
      "--2019-05-15 10:02:23--  http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz\n",
      "Resolving web.ucsd.edu (web.ucsd.edu)... 132.239.1.230, 132.239.1.231\n",
      "Connecting to web.ucsd.edu (web.ucsd.edu)|132.239.1.230|:3128... connected.\n",
      "Proxy request sent, awaiting response... 200 OK\n",
      "Length: 34869662 (33M) [application/x-gtar]\n",
      "Saving to: 'simple-examples.tgz.1'\n",
      "\n",
      "simple-examples.tgz 100%[===================>]  33.25M  2.61MB/s    in 14s     \n",
      "\n",
      "2019-05-15 10:02:39 (2.37 MB/s) - 'simple-examples.tgz.1' saved [34869662/34869662]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!bash dowloaddata.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentenceVAE(\n",
      "  (embedding): Embedding(9877, 300)\n",
      "  (embedding_dropout): Dropout(p=0.5)\n",
      "  (encoder_rnn): GRU(300, 256, batch_first=True)\n",
      "  (decoder_rnn): GRU(300, 256, batch_first=True)\n",
      "  (hidden2mean): Linear(in_features=256, out_features=16, bias=True)\n",
      "  (hidden2logv): Linear(in_features=256, out_features=16, bias=True)\n",
      "  (latent2hidden): Linear(in_features=16, out_features=256, bias=True)\n",
      "  (outputs2vocab): Linear(in_features=256, out_features=9877, bias=True)\n",
      ")\n",
      "TRAIN Batch 0000/1314, Loss  217.1793, NLL-Loss  217.1785, KL-Loss    0.3892, KL-Weight  0.002\n",
      "TRAIN Batch 0050/1314, Loss  180.7558, NLL-Loss  180.7545, KL-Loss    0.6001, KL-Weight  0.002\n",
      "TRAIN Batch 0100/1314, Loss  190.2199, NLL-Loss  190.2002, KL-Loss    7.9746, KL-Weight  0.002\n",
      "TRAIN Batch 0150/1314, Loss  163.1640, NLL-Loss  162.9573, KL-Loss   73.7896, KL-Weight  0.003\n",
      "TRAIN Batch 0200/1314, Loss  132.3400, NLL-Loss  132.1474, KL-Loss   60.6743, KL-Weight  0.003\n",
      "TRAIN Batch 0250/1314, Loss  162.3186, NLL-Loss  162.1012, KL-Loss   60.5101, KL-Weight  0.004\n",
      "TRAIN Batch 0300/1314, Loss  144.7698, NLL-Loss  144.5331, KL-Loss   58.1357, KL-Weight  0.004\n",
      "TRAIN Batch 0350/1314, Loss  148.7003, NLL-Loss  148.4489, KL-Loss   54.5455, KL-Weight  0.005\n",
      "TRAIN Batch 0400/1314, Loss  142.7823, NLL-Loss  142.5332, KL-Loss   47.7140, KL-Weight  0.005\n",
      "TRAIN Batch 0450/1314, Loss  139.4980, NLL-Loss  139.2157, KL-Loss   47.7510, KL-Weight  0.006\n",
      "TRAIN Batch 0500/1314, Loss  135.7088, NLL-Loss  135.3988, KL-Loss   46.3250, KL-Weight  0.007\n",
      "TRAIN Batch 0550/1314, Loss  125.6262, NLL-Loss  125.3110, KL-Loss   41.5944, KL-Weight  0.008\n",
      "TRAIN Batch 0600/1314, Loss  149.9098, NLL-Loss  149.5205, KL-Loss   45.3836, KL-Weight  0.009\n",
      "TRAIN Batch 0650/1314, Loss  142.6345, NLL-Loss  142.2144, KL-Loss   43.2722, KL-Weight  0.010\n",
      "TRAIN Batch 0700/1314, Loss  149.5935, NLL-Loss  149.1291, KL-Loss   42.2692, KL-Weight  0.011\n",
      "TRAIN Batch 0750/1314, Loss  134.1472, NLL-Loss  133.6567, KL-Loss   39.4542, KL-Weight  0.012\n",
      "TRAIN Batch 0800/1314, Loss  124.7481, NLL-Loss  124.2138, KL-Loss   37.9914, KL-Weight  0.014\n",
      "TRAIN Batch 0850/1314, Loss  157.4325, NLL-Loss  156.7222, KL-Loss   44.6514, KL-Weight  0.016\n",
      "TRAIN Batch 0900/1314, Loss  125.9205, NLL-Loss  125.2033, KL-Loss   39.8706, KL-Weight  0.018\n",
      "TRAIN Batch 0950/1314, Loss  151.4714, NLL-Loss  150.6412, KL-Loss   40.8327, KL-Weight  0.020\n",
      "TRAIN Batch 1000/1314, Loss  142.1702, NLL-Loss  141.2656, KL-Loss   39.3687, KL-Weight  0.023\n",
      "TRAIN Batch 1050/1314, Loss  146.1790, NLL-Loss  145.0944, KL-Loss   41.7827, KL-Weight  0.026\n",
      "TRAIN Batch 1100/1314, Loss  133.3587, NLL-Loss  132.2837, KL-Loss   36.6739, KL-Weight  0.029\n",
      "TRAIN Batch 1150/1314, Loss  128.1599, NLL-Loss  126.9278, KL-Loss   37.2387, KL-Weight  0.033\n",
      "TRAIN Batch 1200/1314, Loss  132.9595, NLL-Loss  131.6941, KL-Loss   33.9015, KL-Weight  0.037\n",
      "TRAIN Batch 1250/1314, Loss  138.8396, NLL-Loss  137.3468, KL-Loss   35.4689, KL-Weight  0.042\n",
      "TRAIN Batch 1300/1314, Loss  141.5345, NLL-Loss  140.0463, KL-Loss   31.3792, KL-Weight  0.047\n",
      "TRAIN Batch 1314/1314, Loss  138.2597, NLL-Loss  136.6923, KL-Loss   31.9674, KL-Weight  0.049\n",
      "TRAIN Epoch 00/30, Mean ELBO  146.3817\n",
      "KL is : 639.3479614257812\n",
      "Model saved at bin/2019-May-17-02:56:17/E0.pytorch\n",
      "VALID Batch 0000/105, Loss  175.4772, NLL-Loss  146.6654, KL-Loss   28.8118, KL-Weight  1.000\n",
      "VALID Batch 0050/105, Loss  192.4977, NLL-Loss  159.7559, KL-Loss   32.7418, KL-Weight  1.000\n",
      "VALID Batch 0100/105, Loss  135.5250, NLL-Loss  110.2333, KL-Loss   25.2917, KL-Weight  1.000\n",
      "VALID Batch 0105/105, Loss  149.7510, NLL-Loss  116.7596, KL-Loss   32.9914, KL-Weight  1.000\n",
      "VALID Epoch 00/30, Mean ELBO  162.1311\n",
      "KL is : 329.9140930175781\n",
      "TEST Batch 0000/117, Loss  156.6409, NLL-Loss  125.6444, KL-Loss   30.9965, KL-Weight  1.000\n",
      "TEST Batch 0050/117, Loss  161.2457, NLL-Loss  130.3594, KL-Loss   30.8863, KL-Weight  1.000\n",
      "TEST Batch 0100/117, Loss  187.3194, NLL-Loss  150.7004, KL-Loss   36.6189, KL-Weight  1.000\n",
      "TEST Batch 0117/117, Loss  218.4969, NLL-Loss  184.6762, KL-Loss   33.8208, KL-Weight  1.000\n",
      "TEST Epoch 00/30, Mean ELBO  162.0128\n",
      "KL is : 574.9529418945312\n",
      "TRAIN Batch 0000/1314, Loss  151.6609, NLL-Loss  150.0670, KL-Loss   32.4309, KL-Weight  0.049\n",
      "TRAIN Batch 0050/1314, Loss  135.1977, NLL-Loss  133.6275, KL-Loss   28.3785, KL-Weight  0.055\n",
      "TRAIN Batch 0100/1314, Loss  132.7480, NLL-Loss  130.8663, KL-Loss   30.2335, KL-Weight  0.062\n",
      "TRAIN Batch 0150/1314, Loss  125.4936, NLL-Loss  123.7574, KL-Loss   24.8221, KL-Weight  0.070\n",
      "TRAIN Batch 0200/1314, Loss  125.5810, NLL-Loss  123.6489, KL-Loss   24.6033, KL-Weight  0.079\n",
      "TRAIN Batch 0250/1314, Loss  139.9009, NLL-Loss  137.7551, KL-Loss   24.3664, KL-Weight  0.088\n",
      "TRAIN Batch 0300/1314, Loss  141.9448, NLL-Loss  139.7115, KL-Loss   22.6426, KL-Weight  0.099\n",
      "TRAIN Batch 0350/1314, Loss  144.9371, NLL-Loss  142.5534, KL-Loss   21.6079, KL-Weight  0.110\n",
      "TRAIN Batch 0400/1314, Loss  125.8745, NLL-Loss  123.5469, KL-Loss   18.8932, KL-Weight  0.123\n",
      "TRAIN Batch 0450/1314, Loss  141.8561, NLL-Loss  139.6347, KL-Loss   16.1731, KL-Weight  0.137\n",
      "TRAIN Batch 0500/1314, Loss  139.8240, NLL-Loss  137.3518, KL-Loss   16.1750, KL-Weight  0.153\n",
      "TRAIN Batch 0550/1314, Loss  137.7039, NLL-Loss  135.6063, KL-Loss   12.3583, KL-Weight  0.170\n",
      "TRAIN Batch 0600/1314, Loss  129.6450, NLL-Loss  127.2055, KL-Loss   12.9704, KL-Weight  0.188\n",
      "TRAIN Batch 0650/1314, Loss  121.0682, NLL-Loss  118.5996, KL-Loss   11.8723, KL-Weight  0.208\n",
      "TRAIN Batch 0700/1314, Loss  137.1633, NLL-Loss  134.8990, KL-Loss    9.8764, KL-Weight  0.229\n",
      "TRAIN Batch 0750/1314, Loss  136.3210, NLL-Loss  134.0081, KL-Loss    9.1752, KL-Weight  0.252\n",
      "TRAIN Batch 0800/1314, Loss  134.4811, NLL-Loss  131.8848, KL-Loss    9.3940, KL-Weight  0.276\n",
      "TRAIN Batch 0850/1314, Loss  141.5358, NLL-Loss  138.9411, KL-Loss    8.5901, KL-Weight  0.302\n",
      "TRAIN Batch 0900/1314, Loss  140.9839, NLL-Loss  139.0098, KL-Loss    5.9994, KL-Weight  0.329\n",
      "TRAIN Batch 0950/1314, Loss  131.7444, NLL-Loss  129.6599, KL-Loss    5.8355, KL-Weight  0.357\n",
      "TRAIN Batch 1000/1314, Loss  146.1580, NLL-Loss  144.2560, KL-Loss    4.9225, KL-Weight  0.386\n",
      "TRAIN Batch 1050/1314, Loss  165.5439, NLL-Loss  163.6685, KL-Loss    4.5037, KL-Weight  0.416\n",
      "TRAIN Batch 1100/1314, Loss  134.2803, NLL-Loss  132.3613, KL-Loss    4.2923, KL-Weight  0.447\n",
      "TRAIN Batch 1150/1314, Loss  120.7418, NLL-Loss  118.5746, KL-Loss    4.5324, KL-Weight  0.478\n",
      "TRAIN Batch 1200/1314, Loss  140.6542, NLL-Loss  137.9954, KL-Loss    5.2198, KL-Weight  0.509\n",
      "TRAIN Batch 1250/1314, Loss  142.9617, NLL-Loss  141.1572, KL-Loss    3.3382, KL-Weight  0.541\n",
      "TRAIN Batch 1300/1314, Loss  131.5919, NLL-Loss  129.4333, KL-Loss    3.7777, KL-Weight  0.571\n",
      "TRAIN Batch 1314/1314, Loss  110.8871, NLL-Loss  108.8699, KL-Loss    3.4783, KL-Weight  0.580\n",
      "TRAIN Epoch 01/30, Mean ELBO  132.5074\n",
      "KL is : 69.56543731689453\n",
      "Model saved at bin/2019-May-17-02:56:17/E1.pytorch\n",
      "VALID Batch 0000/105, Loss  145.1587, NLL-Loss  142.9351, KL-Loss    2.2236, KL-Weight  1.000\n",
      "VALID Batch 0050/105, Loss  157.1475, NLL-Loss  153.6089, KL-Loss    3.5386, KL-Weight  1.000\n",
      "VALID Batch 0100/105, Loss  109.9798, NLL-Loss  107.7762, KL-Loss    2.2036, KL-Weight  1.000\n",
      "VALID Batch 0105/105, Loss  114.5782, NLL-Loss  111.1322, KL-Loss    3.4461, KL-Weight  1.000\n",
      "VALID Epoch 01/30, Mean ELBO  129.3985\n",
      "KL is : 34.4605712890625\n",
      "TEST Batch 0000/117, Loss  124.3142, NLL-Loss  121.3234, KL-Loss    2.9908, KL-Weight  1.000\n",
      "TEST Batch 0050/117, Loss  130.0954, NLL-Loss  127.5091, KL-Loss    2.5862, KL-Weight  1.000\n",
      "TEST Batch 0100/117, Loss  150.7346, NLL-Loss  146.6948, KL-Loss    4.0398, KL-Weight  1.000\n",
      "TEST Batch 0117/117, Loss  180.3915, NLL-Loss  177.1270, KL-Loss    3.2645, KL-Weight  1.000\n",
      "TEST Epoch 01/30, Mean ELBO  129.6152\n",
      "KL is : 55.4964599609375\n",
      "TRAIN Batch 0000/1314, Loss  118.0217, NLL-Loss  116.3354, KL-Loss    2.9047, KL-Weight  0.581\n",
      "TRAIN Batch 0050/1314, Loss  142.8354, NLL-Loss  141.3284, KL-Loss    2.4679, KL-Weight  0.611\n",
      "TRAIN Batch 0100/1314, Loss  130.8971, NLL-Loss  129.5513, KL-Loss    2.1031, KL-Weight  0.640\n",
      "TRAIN Batch 0150/1314, Loss  145.9973, NLL-Loss  144.2562, KL-Loss    2.6057, KL-Weight  0.668\n",
      "TRAIN Batch 0200/1314, Loss  120.5372, NLL-Loss  118.7339, KL-Loss    2.5935, KL-Weight  0.695\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN Batch 0250/1314, Loss  134.7889, NLL-Loss  132.9646, KL-Loss    2.5297, KL-Weight  0.721\n",
      "TRAIN Batch 0300/1314, Loss  120.7701, NLL-Loss  119.0196, KL-Loss    2.3479, KL-Weight  0.746\n",
      "TRAIN Batch 0350/1314, Loss  120.6449, NLL-Loss  118.9904, KL-Loss    2.1528, KL-Weight  0.769\n",
      "TRAIN Batch 0400/1314, Loss  119.6280, NLL-Loss  118.4606, KL-Loss    1.4777, KL-Weight  0.790\n",
      "TRAIN Batch 0450/1314, Loss  131.5772, NLL-Loss  130.0390, KL-Loss    1.8991, KL-Weight  0.810\n",
      "TRAIN Batch 0500/1314, Loss  141.2370, NLL-Loss  139.5312, KL-Loss    2.0589, KL-Weight  0.828\n",
      "TRAIN Batch 0550/1314, Loss  136.0957, NLL-Loss  134.8476, KL-Loss    1.4761, KL-Weight  0.846\n",
      "TRAIN Batch 0600/1314, Loss  127.4403, NLL-Loss  126.1257, KL-Loss    1.5264, KL-Weight  0.861\n",
      "TRAIN Batch 0650/1314, Loss  114.0744, NLL-Loss  112.6443, KL-Loss    1.6336, KL-Weight  0.875\n",
      "TRAIN Batch 0700/1314, Loss  145.9890, NLL-Loss  144.6019, KL-Loss    1.5613, KL-Weight  0.888\n",
      "TRAIN Batch 0750/1314, Loss  139.1427, NLL-Loss  137.4455, KL-Loss    1.8853, KL-Weight  0.900\n",
      "TRAIN Batch 0800/1314, Loss  105.1842, NLL-Loss  103.8506, KL-Loss    1.4640, KL-Weight  0.911\n",
      "TRAIN Batch 0850/1314, Loss  138.9527, NLL-Loss  137.8522, KL-Loss    1.1954, KL-Weight  0.921\n",
      "TRAIN Batch 0900/1314, Loss  123.5413, NLL-Loss  121.6853, KL-Loss    1.9973, KL-Weight  0.929\n",
      "TRAIN Batch 0950/1314, Loss  121.5375, NLL-Loss  120.0585, KL-Loss    1.5784, KL-Weight  0.937\n",
      "TRAIN Batch 1000/1314, Loss  124.6013, NLL-Loss  122.8957, KL-Loss    1.8067, KL-Weight  0.944\n",
      "TRAIN Batch 1050/1314, Loss  125.3763, NLL-Loss  124.3905, KL-Loss    1.0374, KL-Weight  0.950\n",
      "TRAIN Batch 1100/1314, Loss  111.3279, NLL-Loss  109.8936, KL-Loss    1.5005, KL-Weight  0.956\n",
      "TRAIN Batch 1150/1314, Loss  127.2798, NLL-Loss  125.7142, KL-Loss    1.6294, KL-Weight  0.961\n",
      "TRAIN Batch 1200/1314, Loss  142.6165, NLL-Loss  141.2460, KL-Loss    1.4199, KL-Weight  0.965\n",
      "TRAIN Batch 1250/1314, Loss  152.0203, NLL-Loss  150.8555, KL-Loss    1.2018, KL-Weight  0.969\n",
      "TRAIN Batch 1300/1314, Loss  135.0975, NLL-Loss  133.3327, KL-Loss    1.8142, KL-Weight  0.973\n",
      "TRAIN Batch 1314/1314, Loss  104.1591, NLL-Loss  102.8157, KL-Loss    1.3797, KL-Weight  0.974\n",
      "TRAIN Epoch 02/30, Mean ELBO  129.3269\n",
      "KL is : 27.5948486328125\n",
      "Model saved at bin/2019-May-17-02:56:17/E2.pytorch\n",
      "VALID Batch 0000/105, Loss  142.8784, NLL-Loss  141.9544, KL-Loss    0.9240, KL-Weight  1.000\n",
      "VALID Batch 0050/105, Loss  151.1405, NLL-Loss  149.4066, KL-Loss    1.7339, KL-Weight  1.000\n",
      "VALID Batch 0100/105, Loss  107.2571, NLL-Loss  106.2202, KL-Loss    1.0369, KL-Weight  1.000\n",
      "VALID Batch 0105/105, Loss  109.7970, NLL-Loss  107.9987, KL-Loss    1.7983, KL-Weight  1.000\n",
      "VALID Epoch 02/30, Mean ELBO  125.2173\n",
      "KL is : 17.982967376708984\n",
      "TEST Batch 0000/117, Loss  119.8203, NLL-Loss  118.5126, KL-Loss    1.3077, KL-Weight  1.000\n",
      "TEST Batch 0050/117, Loss  127.0077, NLL-Loss  125.9686, KL-Loss    1.0391, KL-Weight  1.000\n",
      "TEST Batch 0100/117, Loss  147.2178, NLL-Loss  145.2991, KL-Loss    1.9188, KL-Weight  1.000\n",
      "TEST Batch 0117/117, Loss  175.6996, NLL-Loss  174.0536, KL-Loss    1.6461, KL-Weight  1.000\n",
      "TEST Epoch 02/30, Mean ELBO  125.5268\n",
      "KL is : 27.982900619506836\n",
      "TRAIN Batch 0000/1314, Loss  138.5766, NLL-Loss  137.4701, KL-Loss    1.1363, KL-Weight  0.974\n",
      "TRAIN Batch 0050/1314, Loss  123.2417, NLL-Loss  121.8040, KL-Loss    1.4719, KL-Weight  0.977\n",
      "TRAIN Batch 0100/1314, Loss  140.1000, NLL-Loss  138.4566, KL-Loss    1.6779, KL-Weight  0.979\n",
      "TRAIN Batch 0150/1314, Loss  120.0743, NLL-Loss  118.6954, KL-Loss    1.4045, KL-Weight  0.982\n",
      "TRAIN Batch 0200/1314, Loss  119.8382, NLL-Loss  118.2854, KL-Loss    1.5782, KL-Weight  0.984\n",
      "TRAIN Batch 0250/1314, Loss  120.4709, NLL-Loss  119.1977, KL-Loss    1.2916, KL-Weight  0.986\n",
      "TRAIN Batch 0300/1314, Loss  123.7299, NLL-Loss  122.4569, KL-Loss    1.2893, KL-Weight  0.987\n",
      "TRAIN Batch 0350/1314, Loss  119.4902, NLL-Loss  118.0863, KL-Loss    1.4197, KL-Weight  0.989\n",
      "TRAIN Batch 0400/1314, Loss  118.5761, NLL-Loss  117.1854, KL-Loss    1.4045, KL-Weight  0.990\n",
      "TRAIN Batch 0450/1314, Loss  143.5824, NLL-Loss  142.2685, KL-Loss    1.3255, KL-Weight  0.991\n",
      "TRAIN Batch 0500/1314, Loss  133.1236, NLL-Loss  131.3043, KL-Loss    1.8333, KL-Weight  0.992\n",
      "TRAIN Batch 0550/1314, Loss  137.0413, NLL-Loss  135.3382, KL-Loss    1.7148, KL-Weight  0.993\n",
      "TRAIN Batch 0600/1314, Loss  115.8400, NLL-Loss  114.4624, KL-Loss    1.3859, KL-Weight  0.994\n",
      "TRAIN Batch 0650/1314, Loss  136.5869, NLL-Loss  135.2651, KL-Loss    1.3288, KL-Weight  0.995\n",
      "TRAIN Batch 0700/1314, Loss  120.9671, NLL-Loss  119.6094, KL-Loss    1.3640, KL-Weight  0.995\n",
      "TRAIN Batch 0750/1314, Loss  135.0899, NLL-Loss  133.6797, KL-Loss    1.4160, KL-Weight  0.996\n",
      "TRAIN Batch 0800/1314, Loss  121.3166, NLL-Loss  120.3892, KL-Loss    0.9308, KL-Weight  0.996\n",
      "TRAIN Batch 0850/1314, Loss  115.9954, NLL-Loss  114.4861, KL-Loss    1.5142, KL-Weight  0.997\n",
      "TRAIN Batch 0900/1314, Loss  127.5860, NLL-Loss  126.1937, KL-Loss    1.3963, KL-Weight  0.997\n",
      "TRAIN Batch 0950/1314, Loss  149.1030, NLL-Loss  147.6424, KL-Loss    1.4642, KL-Weight  0.997\n",
      "TRAIN Batch 1000/1314, Loss  136.5872, NLL-Loss  135.2331, KL-Loss    1.3571, KL-Weight  0.998\n",
      "TRAIN Batch 1050/1314, Loss  120.2467, NLL-Loss  119.0028, KL-Loss    1.2464, KL-Weight  0.998\n",
      "TRAIN Batch 1100/1314, Loss  125.9521, NLL-Loss  124.7348, KL-Loss    1.2194, KL-Weight  0.998\n",
      "TRAIN Batch 1150/1314, Loss  121.8157, NLL-Loss  120.6360, KL-Loss    1.1815, KL-Weight  0.998\n",
      "TRAIN Batch 1200/1314, Loss  133.3926, NLL-Loss  131.5781, KL-Loss    1.8169, KL-Weight  0.999\n",
      "TRAIN Batch 1250/1314, Loss  131.3034, NLL-Loss  130.1764, KL-Loss    1.1283, KL-Weight  0.999\n",
      "TRAIN Batch 1300/1314, Loss  112.2448, NLL-Loss  110.9394, KL-Loss    1.3068, KL-Weight  0.999\n",
      "TRAIN Batch 1314/1314, Loss  136.0388, NLL-Loss  134.8794, KL-Loss    1.1607, KL-Weight  0.999\n",
      "TRAIN Epoch 03/30, Mean ELBO  125.8676\n",
      "KL is : 23.213048934936523\n",
      "Model saved at bin/2019-May-17-02:56:17/E3.pytorch\n",
      "^C\n",
      "Process Process-482:\n",
      "Process Process-481:\n",
      "Process Process-483:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/datasets/home/26/826/yul358/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 50, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/datasets/home/26/826/yul358/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 50, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/datasets/home/26/826/yul358/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 50, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
      "    with self._rlock:\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
      "    return self._semlock.__enter__()\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"train.py\", line 256, in <module>\n",
      "    parser.add_argument('-x0', '--x0', type=int, default=2500)\n",
      "  File \"train.py\", line 127, in main\n",
      "    data_loader = DataLoader(\n",
      "  File \"/datasets/home/26/826/yul358/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 417, in __iter__\n",
      "    return DataLoaderIter(self)\n",
      "  File \"/datasets/home/26/826/yul358/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 234, in __init__\n",
      "    w.start()\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/process.py\", line 105, in start\n",
      "    self._popen = self._Popen(self)\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/context.py\", line 223, in _Popen\n",
      "    return _default_context.get_context().Process._Popen(process_obj)\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/context.py\", line 277, in _Popen\n",
      "    return Popen(process_obj)\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/popen_fork.py\", line 19, in __init__\n",
      "    self._launch(process_obj)\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/popen_fork.py\", line 66, in _launch\n",
      "    self.pid = os.fork()\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python train.py --test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from ./bin/2019-May-15-20:08:13/E11.pytorch\n",
      "----------SAMPLES----------\n",
      "in a <unk> with n n for instance has been pouring upward in the past decade <eos>\n",
      "although he is n't likely to be <unk> for the first time and a few years ago <eos>\n",
      "there ' s no reason for about n minutes is going to be <eos>\n",
      "he said the company will resume the n million remaining n n stakes in january <eos>\n",
      "<unk> inc . said its new york city ' s largest cable network will acquire n n of its own business and beer brands <eos>\n",
      "we ' re going to do anything says mr . <unk> ' s vice president <eos>\n",
      "loral said it will help boost its <unk> and <unk> operations to the company ' s restructuring <eos>\n",
      "last week ' s ruling that mr . peterson says his departure is n't <unk> <eos>\n",
      "they ' re not encouraging their own <unk> <eos>\n",
      "separately federal express co . minneapolis <unk> its own account for its own account in its own account for its own account that they would have to keep their <unk> <unk> and <unk> far-reaching <eos>\n",
      "-------INTERPOLATION-------\n",
      "the sale of the company is <unk> and <unk> by the national wildlife of the national wildlife refuge and <unk> by the company ' s <unk> <eos>\n",
      "the sale of the company is <unk> by <unk> and <unk> by the national wildlife federation of <unk> and <unk> by a joint venture <eos>\n",
      "the sale of the company is <unk> by a major pharmaceutical concern and to <unk> its own <unk> <eos>\n",
      "the sale of the company is <unk> by a major pharmaceutical concern and to prevent <unk> of the <unk> <eos>\n",
      "a major chunk of the <unk> is expected to be used by the national association of new york and <unk> <eos>\n",
      "a major factor of the <unk> is expected to be used by the national association of japan and japan <eos>\n",
      "most of a major effort to the economy is <unk> by the national association of american medical devices <eos>\n",
      "most of a shift on the economy to expand the economy from the economy is a leading <unk> problem in the u . s . <eos>\n",
      "westmoreland are a <unk> of the economy to decline in the next few years to predict the economy <eos>\n",
      "westmoreland are a <unk> to the seismic of gross national product in the economy <eos>\n"
     ]
    }
   ],
   "source": [
    "# use simple cyclic annealing\n",
    "!python inference.py -c ./bin/2019-May-16-05\\:46\\:54/E09.pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaselineRNN(\n",
      "  (embedding): Embedding(9877, 300)\n",
      "  (embedding_dropout): Dropout(p=0.5)\n",
      "  (outputs2vocab): Linear(in_features=512, out_features=9877, bias=True)\n",
      "  (decoder_rnn): RNN(300, 512, batch_first=True)\n",
      ")\n",
      "TRAIN Batch 0000/1314, Loss  214.4639, NLL-Loss  214.4639\n",
      "TRAIN Batch 0050/1314, Loss  232.1380, NLL-Loss  232.1380\n",
      "TRAIN Batch 0100/1314, Loss  190.6628, NLL-Loss  190.6628\n",
      "TRAIN Batch 0150/1314, Loss  235.4485, NLL-Loss  235.4485\n",
      "TRAIN Batch 0200/1314, Loss  180.1174, NLL-Loss  180.1174\n",
      "TRAIN Batch 0250/1314, Loss  150.2220, NLL-Loss  150.2220\n",
      "TRAIN Batch 0300/1314, Loss  165.8660, NLL-Loss  165.8660\n",
      "TRAIN Batch 0350/1314, Loss  146.4641, NLL-Loss  146.4641\n",
      "TRAIN Batch 0400/1314, Loss  142.6047, NLL-Loss  142.6047\n",
      "TRAIN Batch 0450/1314, Loss  153.3403, NLL-Loss  153.3403\n",
      "TRAIN Batch 0500/1314, Loss  125.4196, NLL-Loss  125.4196\n",
      "TRAIN Batch 0550/1314, Loss  144.1549, NLL-Loss  144.1549\n",
      "TRAIN Batch 0600/1314, Loss  156.5652, NLL-Loss  156.5652\n",
      "TRAIN Batch 0650/1314, Loss  141.2595, NLL-Loss  141.2595\n",
      "TRAIN Batch 0700/1314, Loss  140.6056, NLL-Loss  140.6056\n",
      "TRAIN Batch 0750/1314, Loss  137.5407, NLL-Loss  137.5407\n",
      "TRAIN Batch 0800/1314, Loss  136.4058, NLL-Loss  136.4058\n",
      "TRAIN Batch 0850/1314, Loss  148.2850, NLL-Loss  148.2850\n",
      "TRAIN Batch 0900/1314, Loss  132.1879, NLL-Loss  132.1879\n",
      "TRAIN Batch 0950/1314, Loss  145.3660, NLL-Loss  145.3660\n",
      "TRAIN Batch 1000/1314, Loss  132.3845, NLL-Loss  132.3845\n",
      "TRAIN Batch 1050/1314, Loss  151.3346, NLL-Loss  151.3346\n",
      "TRAIN Batch 1100/1314, Loss  128.0748, NLL-Loss  128.0748\n",
      "TRAIN Batch 1150/1314, Loss  132.7231, NLL-Loss  132.7231\n",
      "TRAIN Batch 1200/1314, Loss  144.3464, NLL-Loss  144.3464\n",
      "TRAIN Batch 1250/1314, Loss  133.9877, NLL-Loss  133.9877\n",
      "TRAIN Batch 1300/1314, Loss  142.5635, NLL-Loss  142.5635\n",
      "TRAIN Batch 1314/1314, Loss  151.3314, NLL-Loss  151.3314\n",
      "TRAIN Epoch 00/300, Mean NLL  154.2435\n",
      "Model saved at bin/2019-May-17-21:02:23/E0.pytorch\n",
      "VALID Batch 0000/105, Loss  152.9691, NLL-Loss  152.9691\n",
      "VALID Batch 0050/105, Loss  169.7093, NLL-Loss  169.7093\n",
      "VALID Batch 0100/105, Loss  116.0286, NLL-Loss  116.0286\n",
      "VALID Batch 0105/105, Loss  128.7739, NLL-Loss  128.7739\n",
      "VALID Epoch 00/300, Mean NLL  138.9447\n",
      "TEST Batch 0000/117, Loss  133.2195, NLL-Loss  133.2195\n",
      "TEST Batch 0050/117, Loss  137.1516, NLL-Loss  137.1516\n",
      "TEST Batch 0100/117, Loss  160.1055, NLL-Loss  160.1055\n",
      "TEST Batch 0117/117, Loss  195.0816, NLL-Loss  195.0816\n",
      "TEST Epoch 00/300, Mean NLL  139.0754\n",
      "TRAIN Batch 0000/1314, Loss  133.9346, NLL-Loss  133.9346\n",
      "TRAIN Batch 0050/1314, Loss  147.3027, NLL-Loss  147.3027\n",
      "TRAIN Batch 0100/1314, Loss  127.4035, NLL-Loss  127.4035\n",
      "TRAIN Batch 0150/1314, Loss  130.7975, NLL-Loss  130.7975\n",
      "TRAIN Batch 0200/1314, Loss  125.6393, NLL-Loss  125.6393\n",
      "TRAIN Batch 0250/1314, Loss  135.2307, NLL-Loss  135.2307\n",
      "TRAIN Batch 0300/1314, Loss  145.0002, NLL-Loss  145.0002\n",
      "TRAIN Batch 0350/1314, Loss  137.5730, NLL-Loss  137.5730\n",
      "TRAIN Batch 0400/1314, Loss  142.3211, NLL-Loss  142.3211\n",
      "TRAIN Batch 0450/1314, Loss  144.1383, NLL-Loss  144.1383\n",
      "TRAIN Batch 0500/1314, Loss  147.3248, NLL-Loss  147.3248\n",
      "TRAIN Batch 0550/1314, Loss  122.2888, NLL-Loss  122.2888\n",
      "TRAIN Batch 0600/1314, Loss  139.8094, NLL-Loss  139.8094\n",
      "TRAIN Batch 0650/1314, Loss  126.8454, NLL-Loss  126.8454\n",
      "TRAIN Batch 0700/1314, Loss  152.3841, NLL-Loss  152.3841\n",
      "TRAIN Batch 0750/1314, Loss  132.6350, NLL-Loss  132.6350\n",
      "TRAIN Batch 0800/1314, Loss  123.7820, NLL-Loss  123.7820\n",
      "TRAIN Batch 0850/1314, Loss  127.6770, NLL-Loss  127.6770\n",
      "TRAIN Batch 0900/1314, Loss  126.9212, NLL-Loss  126.9212\n",
      "TRAIN Batch 0950/1314, Loss  124.4351, NLL-Loss  124.4351\n",
      "TRAIN Batch 1000/1314, Loss  146.9525, NLL-Loss  146.9525\n",
      "TRAIN Batch 1050/1314, Loss  125.9621, NLL-Loss  125.9621\n",
      "TRAIN Batch 1100/1314, Loss  145.6297, NLL-Loss  145.6297\n",
      "TRAIN Batch 1150/1314, Loss  125.0380, NLL-Loss  125.0380\n",
      "TRAIN Batch 1200/1314, Loss  143.8132, NLL-Loss  143.8132\n",
      "TRAIN Batch 1250/1314, Loss  122.2635, NLL-Loss  122.2635\n",
      "TRAIN Batch 1300/1314, Loss  136.8299, NLL-Loss  136.8299\n",
      "TRAIN Batch 1314/1314, Loss  130.8906, NLL-Loss  130.8906\n",
      "TRAIN Epoch 01/300, Mean NLL  139.0881\n",
      "Model saved at bin/2019-May-17-21:02:23/E1.pytorch\n",
      "VALID Batch 0000/105, Loss  149.1452, NLL-Loss  149.1452\n",
      "VALID Batch 0050/105, Loss  163.2638, NLL-Loss  163.2638\n",
      "VALID Batch 0100/105, Loss  112.5848, NLL-Loss  112.5848\n",
      "VALID Batch 0105/105, Loss  121.9080, NLL-Loss  121.9080\n",
      "VALID Epoch 01/300, Mean NLL  133.8819\n",
      "TEST Batch 0000/117, Loss  128.6158, NLL-Loss  128.6158\n",
      "TEST Batch 0050/117, Loss  133.9998, NLL-Loss  133.9998\n",
      "TEST Batch 0100/117, Loss  155.6614, NLL-Loss  155.6614\n",
      "TEST Batch 0117/117, Loss  188.1561, NLL-Loss  188.1561\n",
      "TEST Epoch 01/300, Mean NLL  134.0971\n",
      "TRAIN Batch 0000/1314, Loss  138.2020, NLL-Loss  138.2020\n",
      "TRAIN Batch 0050/1314, Loss  124.7346, NLL-Loss  124.7346\n",
      "TRAIN Batch 0100/1314, Loss  130.1327, NLL-Loss  130.1327\n",
      "TRAIN Batch 0150/1314, Loss  119.8232, NLL-Loss  119.8232\n",
      "TRAIN Batch 0200/1314, Loss  138.8574, NLL-Loss  138.8574\n",
      "TRAIN Batch 0250/1314, Loss  125.2928, NLL-Loss  125.2928\n",
      "TRAIN Batch 0300/1314, Loss  130.2313, NLL-Loss  130.2313\n",
      "TRAIN Batch 0350/1314, Loss  132.2824, NLL-Loss  132.2824\n",
      "TRAIN Batch 0400/1314, Loss  132.9114, NLL-Loss  132.9114\n",
      "TRAIN Batch 0450/1314, Loss  137.9509, NLL-Loss  137.9509\n",
      "TRAIN Batch 0500/1314, Loss  129.9304, NLL-Loss  129.9304\n",
      "TRAIN Batch 0550/1314, Loss  146.6478, NLL-Loss  146.6478\n",
      "TRAIN Batch 0600/1314, Loss  137.7861, NLL-Loss  137.7861\n",
      "TRAIN Batch 0650/1314, Loss  122.5101, NLL-Loss  122.5101\n",
      "TRAIN Batch 0700/1314, Loss  149.7324, NLL-Loss  149.7324\n",
      "TRAIN Batch 0750/1314, Loss  150.4477, NLL-Loss  150.4477\n",
      "TRAIN Batch 0800/1314, Loss  139.5200, NLL-Loss  139.5200\n",
      "TRAIN Batch 0850/1314, Loss  131.1395, NLL-Loss  131.1395\n",
      "TRAIN Batch 0900/1314, Loss  127.8613, NLL-Loss  127.8613\n",
      "TRAIN Batch 0950/1314, Loss  134.1436, NLL-Loss  134.1436\n",
      "TRAIN Batch 1000/1314, Loss  125.6827, NLL-Loss  125.6827\n",
      "TRAIN Batch 1050/1314, Loss  122.8395, NLL-Loss  122.8395\n",
      "TRAIN Batch 1100/1314, Loss  128.7274, NLL-Loss  128.7274\n",
      "TRAIN Batch 1150/1314, Loss  131.4046, NLL-Loss  131.4046\n",
      "TRAIN Batch 1200/1314, Loss  147.9344, NLL-Loss  147.9344\n",
      "TRAIN Batch 1250/1314, Loss  145.0961, NLL-Loss  145.0961\n",
      "TRAIN Batch 1300/1314, Loss  139.4408, NLL-Loss  139.4408\n",
      "TRAIN Batch 1314/1314, Loss  136.1755, NLL-Loss  136.1755\n",
      "TRAIN Epoch 02/300, Mean NLL  135.1655\n",
      "Model saved at bin/2019-May-17-21:02:23/E2.pytorch\n",
      "VALID Batch 0000/105, Loss  146.6265, NLL-Loss  146.6265\n",
      "VALID Batch 0050/105, Loss  159.3265, NLL-Loss  159.3265\n",
      "VALID Batch 0100/105, Loss  110.3882, NLL-Loss  110.3882\n",
      "VALID Batch 0105/105, Loss  117.3807, NLL-Loss  117.3807\n",
      "VALID Epoch 02/300, Mean NLL  130.7509\n",
      "TEST Batch 0000/117, Loss  125.4744, NLL-Loss  125.4744\n",
      "TEST Batch 0050/117, Loss  131.8982, NLL-Loss  131.8982\n",
      "TEST Batch 0100/117, Loss  152.9080, NLL-Loss  152.9080\n",
      "TEST Batch 0117/117, Loss  183.9039, NLL-Loss  183.9040\n",
      "TEST Epoch 02/300, Mean NLL  130.9665\n",
      "TRAIN Batch 0000/1314, Loss  143.0113, NLL-Loss  143.0113\n",
      "TRAIN Batch 0050/1314, Loss  135.5618, NLL-Loss  135.5618\n",
      "TRAIN Batch 0100/1314, Loss  155.7370, NLL-Loss  155.7370\n",
      "TRAIN Batch 0150/1314, Loss  138.8707, NLL-Loss  138.8707\n",
      "TRAIN Batch 0200/1314, Loss  120.9754, NLL-Loss  120.9754\n",
      "TRAIN Batch 0250/1314, Loss  114.7722, NLL-Loss  114.7722\n",
      "TRAIN Batch 0300/1314, Loss  146.3656, NLL-Loss  146.3656\n",
      "TRAIN Batch 0350/1314, Loss  138.0898, NLL-Loss  138.0898\n",
      "TRAIN Batch 0400/1314, Loss  117.4674, NLL-Loss  117.4674\n",
      "TRAIN Batch 0450/1314, Loss  123.2386, NLL-Loss  123.2386\n",
      "TRAIN Batch 0500/1314, Loss  140.8652, NLL-Loss  140.8652\n",
      "TRAIN Batch 0550/1314, Loss  122.4872, NLL-Loss  122.4872\n",
      "TRAIN Batch 0600/1314, Loss  140.5412, NLL-Loss  140.5412\n",
      "TRAIN Batch 0650/1314, Loss  129.1015, NLL-Loss  129.1015\n",
      "TRAIN Batch 0700/1314, Loss  143.6746, NLL-Loss  143.6746\n",
      "TRAIN Batch 0750/1314, Loss  123.3136, NLL-Loss  123.3136\n",
      "TRAIN Batch 0800/1314, Loss  136.5726, NLL-Loss  136.5726\n",
      "TRAIN Batch 0850/1314, Loss  123.3568, NLL-Loss  123.3568\n",
      "TRAIN Batch 0900/1314, Loss  131.0436, NLL-Loss  131.0436\n",
      "TRAIN Batch 0950/1314, Loss  119.5120, NLL-Loss  119.5120\n",
      "TRAIN Batch 1000/1314, Loss  130.1107, NLL-Loss  130.1107\n",
      "TRAIN Batch 1050/1314, Loss  146.1573, NLL-Loss  146.1573\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN Batch 1100/1314, Loss  130.1626, NLL-Loss  130.1626\n",
      "TRAIN Batch 1150/1314, Loss  142.8142, NLL-Loss  142.8142\n",
      "TRAIN Batch 1200/1314, Loss  124.3728, NLL-Loss  124.3728\n",
      "TRAIN Batch 1250/1314, Loss  132.3349, NLL-Loss  132.3349\n",
      "TRAIN Batch 1300/1314, Loss  136.5941, NLL-Loss  136.5941\n",
      "TRAIN Batch 1314/1314, Loss  114.3079, NLL-Loss  114.3079\n",
      "TRAIN Epoch 03/300, Mean NLL  132.4232\n",
      "Model saved at bin/2019-May-17-21:02:23/E3.pytorch\n",
      "VALID Batch 0000/105, Loss  144.7527, NLL-Loss  144.7527\n",
      "VALID Batch 0050/105, Loss  156.2233, NLL-Loss  156.2233\n",
      "VALID Batch 0100/105, Loss  108.7649, NLL-Loss  108.7649\n",
      "VALID Batch 0105/105, Loss  114.7210, NLL-Loss  114.7210\n",
      "VALID Epoch 03/300, Mean NLL  128.4419\n",
      "TEST Batch 0000/117, Loss  122.6778, NLL-Loss  122.6778\n",
      "TEST Batch 0050/117, Loss  130.6186, NLL-Loss  130.6186\n",
      "TEST Batch 0100/117, Loss  150.9678, NLL-Loss  150.9678\n",
      "TEST Batch 0117/117, Loss  181.0482, NLL-Loss  181.0483\n",
      "TEST Epoch 03/300, Mean NLL  128.6481\n",
      "TRAIN Batch 0000/1314, Loss  128.1825, NLL-Loss  128.1825\n",
      "TRAIN Batch 0050/1314, Loss  131.2253, NLL-Loss  131.2253\n",
      "TRAIN Batch 0100/1314, Loss  152.4429, NLL-Loss  152.4429\n",
      "TRAIN Batch 0150/1314, Loss  137.5030, NLL-Loss  137.5030\n",
      "TRAIN Batch 0200/1314, Loss  128.4486, NLL-Loss  128.4486\n",
      "TRAIN Batch 0250/1314, Loss  130.0336, NLL-Loss  130.0336\n",
      "TRAIN Batch 0300/1314, Loss  120.6640, NLL-Loss  120.6640\n",
      "TRAIN Batch 0350/1314, Loss  136.4890, NLL-Loss  136.4890\n",
      "TRAIN Batch 0400/1314, Loss  125.4028, NLL-Loss  125.4028\n",
      "TRAIN Batch 0450/1314, Loss  146.8434, NLL-Loss  146.8434\n",
      "TRAIN Batch 0500/1314, Loss  148.3365, NLL-Loss  148.3365\n",
      "TRAIN Batch 0550/1314, Loss  111.2933, NLL-Loss  111.2933\n",
      "TRAIN Batch 0600/1314, Loss  130.6419, NLL-Loss  130.6419\n",
      "TRAIN Batch 0650/1314, Loss  142.7165, NLL-Loss  142.7165\n",
      "TRAIN Batch 0700/1314, Loss  136.5590, NLL-Loss  136.5590\n",
      "TRAIN Batch 0750/1314, Loss  148.9429, NLL-Loss  148.9429\n",
      "TRAIN Batch 0800/1314, Loss  134.0575, NLL-Loss  134.0575\n",
      "TRAIN Batch 0850/1314, Loss  141.8149, NLL-Loss  141.8149\n",
      "TRAIN Batch 0900/1314, Loss  117.7753, NLL-Loss  117.7753\n",
      "TRAIN Batch 0950/1314, Loss  129.0977, NLL-Loss  129.0977\n",
      "TRAIN Batch 1000/1314, Loss  129.5898, NLL-Loss  129.5898\n",
      "TRAIN Batch 1050/1314, Loss  121.1533, NLL-Loss  121.1533\n",
      "TRAIN Batch 1100/1314, Loss  119.9624, NLL-Loss  119.9624\n",
      "TRAIN Batch 1150/1314, Loss  128.9395, NLL-Loss  128.9395\n",
      "TRAIN Batch 1200/1314, Loss  124.6506, NLL-Loss  124.6506\n",
      "TRAIN Batch 1250/1314, Loss  148.9871, NLL-Loss  148.9871\n",
      "TRAIN Batch 1300/1314, Loss  117.0772, NLL-Loss  117.0772\n",
      "TRAIN Batch 1314/1314, Loss  128.4893, NLL-Loss  128.4893\n",
      "TRAIN Epoch 04/300, Mean NLL  130.3089\n",
      "Model saved at bin/2019-May-17-21:02:23/E4.pytorch\n",
      "VALID Batch 0000/105, Loss  143.3945, NLL-Loss  143.3945\n",
      "VALID Batch 0050/105, Loss  153.5684, NLL-Loss  153.5684\n",
      "VALID Batch 0100/105, Loss  107.7494, NLL-Loss  107.7494\n",
      "VALID Batch 0105/105, Loss  112.8120, NLL-Loss  112.8120\n",
      "VALID Epoch 04/300, Mean NLL  126.7327\n",
      "TEST Batch 0000/117, Loss  120.9955, NLL-Loss  120.9955\n",
      "TEST Batch 0050/117, Loss  129.4598, NLL-Loss  129.4598\n",
      "TEST Batch 0100/117, Loss  149.5661, NLL-Loss  149.5661\n",
      "TEST Batch 0117/117, Loss  178.9115, NLL-Loss  178.9115\n",
      "TEST Epoch 04/300, Mean NLL  126.8970\n",
      "TRAIN Batch 0000/1314, Loss  111.4340, NLL-Loss  111.4340\n",
      "TRAIN Batch 0050/1314, Loss  122.7075, NLL-Loss  122.7075\n",
      "TRAIN Batch 0100/1314, Loss  139.1971, NLL-Loss  139.1971\n",
      "TRAIN Batch 0150/1314, Loss  121.0197, NLL-Loss  121.0197\n",
      "TRAIN Batch 0200/1314, Loss  127.3466, NLL-Loss  127.3466\n",
      "TRAIN Batch 0250/1314, Loss  123.9160, NLL-Loss  123.9160\n",
      "TRAIN Batch 0300/1314, Loss  138.0603, NLL-Loss  138.0603\n",
      "TRAIN Batch 0350/1314, Loss  137.9240, NLL-Loss  137.9240\n",
      "TRAIN Batch 0400/1314, Loss  138.7044, NLL-Loss  138.7044\n",
      "TRAIN Batch 0450/1314, Loss  147.6816, NLL-Loss  147.6816\n",
      "TRAIN Batch 0500/1314, Loss  133.3762, NLL-Loss  133.3762\n",
      "TRAIN Batch 0550/1314, Loss  139.4922, NLL-Loss  139.4922\n",
      "TRAIN Batch 0600/1314, Loss  135.1759, NLL-Loss  135.1759\n",
      "TRAIN Batch 0650/1314, Loss  128.3412, NLL-Loss  128.3412\n",
      "TRAIN Batch 0700/1314, Loss  116.8860, NLL-Loss  116.8860\n",
      "TRAIN Batch 0750/1314, Loss  137.8202, NLL-Loss  137.8202\n",
      "TRAIN Batch 0800/1314, Loss  115.0823, NLL-Loss  115.0823\n",
      "TRAIN Batch 0850/1314, Loss  138.7148, NLL-Loss  138.7148\n",
      "TRAIN Batch 0900/1314, Loss  131.9359, NLL-Loss  131.9359\n",
      "TRAIN Batch 0950/1314, Loss  128.4753, NLL-Loss  128.4753\n",
      "TRAIN Batch 1000/1314, Loss  135.0249, NLL-Loss  135.0249\n",
      "TRAIN Batch 1050/1314, Loss  129.0609, NLL-Loss  129.0609\n",
      "TRAIN Batch 1100/1314, Loss  117.0592, NLL-Loss  117.0592\n",
      "TRAIN Batch 1150/1314, Loss  120.5919, NLL-Loss  120.5919\n",
      "TRAIN Batch 1200/1314, Loss  118.2681, NLL-Loss  118.2681\n",
      "TRAIN Batch 1250/1314, Loss  154.5742, NLL-Loss  154.5742\n",
      "TRAIN Batch 1300/1314, Loss  137.3325, NLL-Loss  137.3325\n",
      "TRAIN Batch 1314/1314, Loss  140.7042, NLL-Loss  140.7042\n",
      "TRAIN Epoch 05/300, Mean NLL  128.6081\n",
      "Model saved at bin/2019-May-17-21:02:23/E5.pytorch\n",
      "VALID Batch 0000/105, Loss  142.6270, NLL-Loss  142.6270\n",
      "VALID Batch 0050/105, Loss  152.0750, NLL-Loss  152.0750\n",
      "VALID Batch 0100/105, Loss  106.9345, NLL-Loss  106.9345\n",
      "VALID Batch 0105/105, Loss  111.1804, NLL-Loss  111.1804\n",
      "VALID Epoch 05/300, Mean NLL  125.5068\n",
      "TEST Batch 0000/117, Loss  119.8210, NLL-Loss  119.8210\n",
      "TEST Batch 0050/117, Loss  128.6072, NLL-Loss  128.6072\n",
      "TEST Batch 0100/117, Loss  148.8002, NLL-Loss  148.8002\n",
      "TEST Batch 0117/117, Loss  177.6633, NLL-Loss  177.6633\n",
      "TEST Epoch 05/300, Mean NLL  125.6986\n",
      "TRAIN Batch 0000/1314, Loss  136.2822, NLL-Loss  136.2822\n",
      "TRAIN Batch 0050/1314, Loss  112.2264, NLL-Loss  112.2264\n",
      "TRAIN Batch 0100/1314, Loss  124.9573, NLL-Loss  124.9573\n",
      "TRAIN Batch 0150/1314, Loss  120.4001, NLL-Loss  120.4001\n",
      "TRAIN Batch 0200/1314, Loss  144.7800, NLL-Loss  144.7800\n",
      "TRAIN Batch 0250/1314, Loss  118.2557, NLL-Loss  118.2557\n",
      "TRAIN Batch 0300/1314, Loss  126.0056, NLL-Loss  126.0056\n",
      "TRAIN Batch 0350/1314, Loss  125.3794, NLL-Loss  125.3794\n",
      "TRAIN Batch 0400/1314, Loss  123.6387, NLL-Loss  123.6387\n",
      "TRAIN Batch 0450/1314, Loss  130.4563, NLL-Loss  130.4563\n",
      "TRAIN Batch 0500/1314, Loss  145.4239, NLL-Loss  145.4239\n",
      "TRAIN Batch 0550/1314, Loss  108.0445, NLL-Loss  108.0445\n",
      "TRAIN Batch 0600/1314, Loss  127.1591, NLL-Loss  127.1591\n",
      "TRAIN Batch 0650/1314, Loss  142.7892, NLL-Loss  142.7892\n",
      "TRAIN Batch 0700/1314, Loss  131.2339, NLL-Loss  131.2339\n",
      "TRAIN Batch 0750/1314, Loss  129.1796, NLL-Loss  129.1796\n",
      "TRAIN Batch 0800/1314, Loss  134.5278, NLL-Loss  134.5278\n",
      "TRAIN Batch 0850/1314, Loss  113.8344, NLL-Loss  113.8344\n",
      "TRAIN Batch 0900/1314, Loss  130.2992, NLL-Loss  130.2992\n",
      "TRAIN Batch 0950/1314, Loss  130.8657, NLL-Loss  130.8657\n",
      "TRAIN Batch 1000/1314, Loss  113.3457, NLL-Loss  113.3457\n",
      "TRAIN Batch 1050/1314, Loss  140.9369, NLL-Loss  140.9369\n",
      "TRAIN Batch 1100/1314, Loss  133.7662, NLL-Loss  133.7662\n",
      "TRAIN Batch 1150/1314, Loss  117.3643, NLL-Loss  117.3643\n",
      "TRAIN Batch 1200/1314, Loss  137.9443, NLL-Loss  137.9443\n",
      "TRAIN Batch 1250/1314, Loss  109.7131, NLL-Loss  109.7131\n",
      "TRAIN Batch 1300/1314, Loss  124.8002, NLL-Loss  124.8002\n",
      "TRAIN Batch 1314/1314, Loss  135.8137, NLL-Loss  135.8137\n",
      "TRAIN Epoch 06/300, Mean NLL  127.1551\n",
      "Model saved at bin/2019-May-17-21:02:23/E6.pytorch\n",
      "VALID Batch 0000/105, Loss  141.3345, NLL-Loss  141.3345\n",
      "VALID Batch 0050/105, Loss  149.9703, NLL-Loss  149.9703\n",
      "VALID Batch 0100/105, Loss  106.1720, NLL-Loss  106.1720\n",
      "VALID Batch 0105/105, Loss  109.8460, NLL-Loss  109.8460\n",
      "VALID Epoch 06/300, Mean NLL  124.2103\n",
      "TEST Batch 0000/117, Loss  118.4411, NLL-Loss  118.4411\n",
      "TEST Batch 0050/117, Loss  127.7272, NLL-Loss  127.7272\n",
      "TEST Batch 0100/117, Loss  147.7080, NLL-Loss  147.7080\n",
      "TEST Batch 0117/117, Loss  175.4917, NLL-Loss  175.4917\n",
      "TEST Epoch 06/300, Mean NLL  124.3621\n",
      "TRAIN Batch 0000/1314, Loss  124.2714, NLL-Loss  124.2714\n",
      "TRAIN Batch 0050/1314, Loss  126.1378, NLL-Loss  126.1378\n",
      "TRAIN Batch 0100/1314, Loss  114.1324, NLL-Loss  114.1324\n",
      "TRAIN Batch 0150/1314, Loss  115.0106, NLL-Loss  115.0106\n",
      "TRAIN Batch 0200/1314, Loss  140.0914, NLL-Loss  140.0914\n",
      "TRAIN Batch 0250/1314, Loss  139.9575, NLL-Loss  139.9575\n",
      "TRAIN Batch 0300/1314, Loss  131.3443, NLL-Loss  131.3443\n",
      "TRAIN Batch 0350/1314, Loss  113.2113, NLL-Loss  113.2113\n",
      "TRAIN Batch 0400/1314, Loss  139.4044, NLL-Loss  139.4044\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN Batch 0450/1314, Loss  123.2020, NLL-Loss  123.2020\n",
      "TRAIN Batch 0500/1314, Loss  142.4859, NLL-Loss  142.4859\n",
      "TRAIN Batch 0550/1314, Loss  113.1582, NLL-Loss  113.1582\n",
      "TRAIN Batch 0600/1314, Loss  125.4473, NLL-Loss  125.4473\n",
      "TRAIN Batch 0650/1314, Loss  129.1488, NLL-Loss  129.1488\n",
      "TRAIN Batch 0700/1314, Loss  148.7148, NLL-Loss  148.7148\n",
      "TRAIN Batch 0750/1314, Loss  127.9164, NLL-Loss  127.9164\n",
      "TRAIN Batch 0800/1314, Loss  137.8862, NLL-Loss  137.8862\n",
      "TRAIN Batch 0850/1314, Loss  116.0763, NLL-Loss  116.0763\n",
      "TRAIN Batch 0900/1314, Loss  119.8175, NLL-Loss  119.8175\n",
      "TRAIN Batch 0950/1314, Loss  120.4401, NLL-Loss  120.4401\n",
      "TRAIN Batch 1000/1314, Loss  127.2354, NLL-Loss  127.2354\n",
      "TRAIN Batch 1050/1314, Loss  132.7811, NLL-Loss  132.7811\n",
      "TRAIN Batch 1100/1314, Loss  140.4868, NLL-Loss  140.4868\n",
      "TRAIN Batch 1150/1314, Loss  126.1993, NLL-Loss  126.1993\n",
      "TRAIN Batch 1200/1314, Loss  117.6308, NLL-Loss  117.6308\n",
      "TRAIN Batch 1250/1314, Loss  145.0236, NLL-Loss  145.0236\n",
      "TRAIN Batch 1300/1314, Loss  119.8494, NLL-Loss  119.8494\n",
      "TRAIN Batch 1314/1314, Loss  101.6185, NLL-Loss  101.6185\n",
      "TRAIN Epoch 07/300, Mean NLL  125.9108\n",
      "Model saved at bin/2019-May-17-21:02:23/E7.pytorch\n",
      "VALID Batch 0000/105, Loss  140.5856, NLL-Loss  140.5856\n",
      "VALID Batch 0050/105, Loss  148.0327, NLL-Loss  148.0327\n",
      "VALID Batch 0100/105, Loss  105.4874, NLL-Loss  105.4874\n",
      "VALID Batch 0105/105, Loss  108.6818, NLL-Loss  108.6818\n",
      "VALID Epoch 07/300, Mean NLL  123.1261\n",
      "TEST Batch 0000/117, Loss  117.3684, NLL-Loss  117.3684\n",
      "TEST Batch 0050/117, Loss  126.7350, NLL-Loss  126.7350\n",
      "TEST Batch 0100/117, Loss  146.5650, NLL-Loss  146.5650\n",
      "TEST Batch 0117/117, Loss  173.9766, NLL-Loss  173.9766\n",
      "TEST Epoch 07/300, Mean NLL  123.2343\n",
      "TRAIN Batch 0000/1314, Loss  140.1727, NLL-Loss  140.1727\n",
      "TRAIN Batch 0050/1314, Loss  124.0729, NLL-Loss  124.0729\n",
      "TRAIN Batch 0100/1314, Loss  118.6857, NLL-Loss  118.6857\n",
      "TRAIN Batch 0150/1314, Loss  122.8952, NLL-Loss  122.8952\n",
      "TRAIN Batch 0200/1314, Loss  104.4037, NLL-Loss  104.4037\n",
      "TRAIN Batch 0250/1314, Loss  123.0459, NLL-Loss  123.0459\n",
      "TRAIN Batch 0300/1314, Loss  125.5756, NLL-Loss  125.5756\n",
      "TRAIN Batch 0350/1314, Loss  136.9464, NLL-Loss  136.9464\n",
      "TRAIN Batch 0400/1314, Loss  122.3529, NLL-Loss  122.3529\n",
      "TRAIN Batch 0450/1314, Loss  117.5761, NLL-Loss  117.5761\n",
      "TRAIN Batch 0500/1314, Loss  125.9001, NLL-Loss  125.9001\n",
      "TRAIN Batch 0550/1314, Loss  125.5705, NLL-Loss  125.5705\n",
      "TRAIN Batch 0600/1314, Loss  117.1749, NLL-Loss  117.1749\n",
      "TRAIN Batch 0650/1314, Loss  123.4518, NLL-Loss  123.4518\n",
      "TRAIN Batch 0700/1314, Loss  135.0178, NLL-Loss  135.0178\n",
      "TRAIN Batch 0750/1314, Loss  134.4367, NLL-Loss  134.4367\n",
      "TRAIN Batch 0800/1314, Loss  119.9025, NLL-Loss  119.9025\n",
      "TRAIN Batch 0850/1314, Loss  119.8106, NLL-Loss  119.8106\n",
      "TRAIN Batch 0900/1314, Loss  128.7878, NLL-Loss  128.7878\n",
      "TRAIN Batch 0950/1314, Loss  130.6926, NLL-Loss  130.6926\n",
      "TRAIN Batch 1000/1314, Loss  119.5563, NLL-Loss  119.5563\n",
      "TRAIN Batch 1050/1314, Loss  142.4978, NLL-Loss  142.4978\n",
      "TRAIN Batch 1100/1314, Loss  137.3998, NLL-Loss  137.3998\n",
      "TRAIN Batch 1150/1314, Loss  120.3548, NLL-Loss  120.3548\n",
      "TRAIN Batch 1200/1314, Loss  132.9492, NLL-Loss  132.9492\n",
      "TRAIN Batch 1250/1314, Loss  126.8208, NLL-Loss  126.8208\n",
      "TRAIN Batch 1300/1314, Loss  136.6466, NLL-Loss  136.6466\n",
      "TRAIN Batch 1314/1314, Loss  112.1365, NLL-Loss  112.1365\n",
      "TRAIN Epoch 08/300, Mean NLL  124.8327\n",
      "Model saved at bin/2019-May-17-21:02:23/E8.pytorch\n",
      "VALID Batch 0000/105, Loss  140.3222, NLL-Loss  140.3222\n",
      "VALID Batch 0050/105, Loss  146.9288, NLL-Loss  146.9288\n",
      "VALID Batch 0100/105, Loss  105.0689, NLL-Loss  105.0689\n",
      "VALID Batch 0105/105, Loss  107.8571, NLL-Loss  107.8571\n",
      "VALID Epoch 08/300, Mean NLL  122.4368\n",
      "TEST Batch 0000/117, Loss  116.5646, NLL-Loss  116.5646\n",
      "TEST Batch 0050/117, Loss  126.1121, NLL-Loss  126.1121\n",
      "TEST Batch 0100/117, Loss  146.2667, NLL-Loss  146.2667\n",
      "TEST Batch 0117/117, Loss  173.3700, NLL-Loss  173.3700\n",
      "TEST Epoch 08/300, Mean NLL  122.5433\n",
      "TRAIN Batch 0000/1314, Loss  135.7524, NLL-Loss  135.7524\n",
      "TRAIN Batch 0050/1314, Loss  135.3181, NLL-Loss  135.3181\n",
      "TRAIN Batch 0100/1314, Loss  110.3307, NLL-Loss  110.3307\n",
      "TRAIN Batch 0150/1314, Loss  120.3810, NLL-Loss  120.3810\n",
      "TRAIN Batch 0200/1314, Loss  121.4068, NLL-Loss  121.4068\n",
      "TRAIN Batch 0250/1314, Loss  119.9220, NLL-Loss  119.9220\n",
      "TRAIN Batch 0300/1314, Loss  122.7418, NLL-Loss  122.7418\n",
      "TRAIN Batch 0350/1314, Loss  124.3786, NLL-Loss  124.3786\n",
      "TRAIN Batch 0400/1314, Loss   95.4781, NLL-Loss   95.4781\n",
      "TRAIN Batch 0450/1314, Loss  135.3546, NLL-Loss  135.3546\n",
      "TRAIN Batch 0500/1314, Loss  126.5397, NLL-Loss  126.5397\n",
      "TRAIN Batch 0550/1314, Loss  139.8073, NLL-Loss  139.8073\n",
      "TRAIN Batch 0600/1314, Loss  115.5599, NLL-Loss  115.5599\n",
      "TRAIN Batch 0650/1314, Loss  113.5148, NLL-Loss  113.5148\n",
      "TRAIN Batch 0700/1314, Loss  117.4837, NLL-Loss  117.4837\n",
      "TRAIN Batch 0750/1314, Loss  113.7027, NLL-Loss  113.7027\n",
      "TRAIN Batch 0800/1314, Loss  115.2315, NLL-Loss  115.2315\n",
      "TRAIN Batch 0850/1314, Loss  122.9595, NLL-Loss  122.9595\n",
      "TRAIN Batch 0900/1314, Loss  118.9162, NLL-Loss  118.9162\n",
      "TRAIN Batch 0950/1314, Loss  148.2296, NLL-Loss  148.2296\n",
      "TRAIN Batch 1000/1314, Loss  117.0238, NLL-Loss  117.0238\n",
      "TRAIN Batch 1050/1314, Loss  120.1277, NLL-Loss  120.1277\n",
      "TRAIN Batch 1100/1314, Loss  129.7347, NLL-Loss  129.7347\n",
      "TRAIN Batch 1150/1314, Loss  124.4903, NLL-Loss  124.4903\n",
      "TRAIN Batch 1200/1314, Loss  117.6018, NLL-Loss  117.6018\n",
      "TRAIN Batch 1250/1314, Loss  122.8243, NLL-Loss  122.8243\n",
      "TRAIN Batch 1300/1314, Loss  121.0923, NLL-Loss  121.0923\n",
      "TRAIN Batch 1314/1314, Loss  140.4811, NLL-Loss  140.4811\n",
      "TRAIN Epoch 09/300, Mean NLL  123.8587\n",
      "Model saved at bin/2019-May-17-21:02:23/E9.pytorch\n",
      "VALID Batch 0000/105, Loss  139.3629, NLL-Loss  139.3629\n",
      "VALID Batch 0050/105, Loss  145.7755, NLL-Loss  145.7755\n",
      "VALID Batch 0100/105, Loss  104.6070, NLL-Loss  104.6070\n",
      "VALID Batch 0105/105, Loss  107.0913, NLL-Loss  107.0913\n",
      "VALID Epoch 09/300, Mean NLL  121.6249\n",
      "TEST Batch 0000/117, Loss  115.8008, NLL-Loss  115.8008\n",
      "TEST Batch 0050/117, Loss  125.6699, NLL-Loss  125.6699\n",
      "TEST Batch 0100/117, Loss  145.5194, NLL-Loss  145.5194\n",
      "TEST Batch 0117/117, Loss  171.6564, NLL-Loss  171.6564\n",
      "TEST Epoch 09/300, Mean NLL  121.7071\n",
      "TRAIN Batch 0000/1314, Loss  107.5905, NLL-Loss  107.5905\n",
      "TRAIN Batch 0050/1314, Loss  122.8236, NLL-Loss  122.8236\n",
      "TRAIN Batch 0100/1314, Loss  124.4112, NLL-Loss  124.4112\n",
      "TRAIN Batch 0150/1314, Loss  105.1267, NLL-Loss  105.1267\n",
      "TRAIN Batch 0200/1314, Loss  139.9687, NLL-Loss  139.9687\n",
      "TRAIN Batch 0250/1314, Loss  118.7183, NLL-Loss  118.7183\n",
      "TRAIN Batch 0300/1314, Loss  132.7114, NLL-Loss  132.7114\n",
      "TRAIN Batch 0350/1314, Loss  117.0463, NLL-Loss  117.0463\n",
      "TRAIN Batch 0400/1314, Loss  120.5789, NLL-Loss  120.5789\n",
      "TRAIN Batch 0450/1314, Loss  125.9001, NLL-Loss  125.9001\n",
      "TRAIN Batch 0500/1314, Loss  127.2494, NLL-Loss  127.2494\n",
      "TRAIN Batch 0550/1314, Loss  118.3827, NLL-Loss  118.3827\n",
      "TRAIN Batch 0600/1314, Loss  120.3285, NLL-Loss  120.3285\n",
      "TRAIN Batch 0650/1314, Loss  126.1502, NLL-Loss  126.1502\n",
      "TRAIN Batch 0700/1314, Loss  128.8549, NLL-Loss  128.8549\n",
      "TRAIN Batch 0750/1314, Loss  137.6840, NLL-Loss  137.6840\n",
      "TRAIN Batch 0800/1314, Loss  113.0588, NLL-Loss  113.0588\n",
      "TRAIN Batch 0850/1314, Loss  118.8016, NLL-Loss  118.8016\n",
      "TRAIN Batch 0900/1314, Loss  115.4194, NLL-Loss  115.4194\n",
      "TRAIN Batch 0950/1314, Loss  121.4062, NLL-Loss  121.4062\n",
      "TRAIN Batch 1000/1314, Loss  127.8167, NLL-Loss  127.8167\n",
      "TRAIN Batch 1050/1314, Loss  105.7778, NLL-Loss  105.7778\n",
      "TRAIN Batch 1100/1314, Loss  111.5244, NLL-Loss  111.5244\n",
      "TRAIN Batch 1150/1314, Loss  119.8611, NLL-Loss  119.8611\n",
      "TRAIN Batch 1200/1314, Loss  111.4512, NLL-Loss  111.4512\n",
      "TRAIN Batch 1250/1314, Loss  124.2508, NLL-Loss  124.2508\n",
      "TRAIN Batch 1300/1314, Loss  112.0783, NLL-Loss  112.0783\n",
      "TRAIN Batch 1314/1314, Loss  160.6501, NLL-Loss  160.6501\n",
      "TRAIN Epoch 10/300, Mean NLL  122.9984\n",
      "Model saved at bin/2019-May-17-21:02:23/E10.pytorch\n",
      "VALID Batch 0000/105, Loss  138.9846, NLL-Loss  138.9846\n",
      "VALID Batch 0050/105, Loss  144.0995, NLL-Loss  144.0995\n",
      "VALID Batch 0100/105, Loss  103.9629, NLL-Loss  103.9629\n",
      "VALID Batch 0105/105, Loss  106.3379, NLL-Loss  106.3379\n",
      "VALID Epoch 10/300, Mean NLL  120.8290\n",
      "TEST Batch 0000/117, Loss  114.8293, NLL-Loss  114.8293\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST Batch 0050/117, Loss  124.8054, NLL-Loss  124.8054\n",
      "TEST Batch 0100/117, Loss  144.9788, NLL-Loss  144.9788\n",
      "TEST Batch 0117/117, Loss  171.1870, NLL-Loss  171.1870\n",
      "TEST Epoch 10/300, Mean NLL  120.9166\n",
      "TRAIN Batch 0000/1314, Loss  131.4079, NLL-Loss  131.4079\n",
      "TRAIN Batch 0050/1314, Loss  102.6282, NLL-Loss  102.6282\n",
      "TRAIN Batch 0100/1314, Loss  119.3943, NLL-Loss  119.3943\n",
      "TRAIN Batch 0150/1314, Loss  134.8435, NLL-Loss  134.8435\n",
      "TRAIN Batch 0200/1314, Loss  121.1515, NLL-Loss  121.1515\n",
      "TRAIN Batch 0250/1314, Loss  102.4592, NLL-Loss  102.4592\n",
      "TRAIN Batch 0300/1314, Loss  113.2596, NLL-Loss  113.2596\n",
      "TRAIN Batch 0350/1314, Loss  115.0448, NLL-Loss  115.0448\n",
      "TRAIN Batch 0400/1314, Loss  110.9618, NLL-Loss  110.9618\n",
      "TRAIN Batch 0450/1314, Loss  107.5590, NLL-Loss  107.5590\n",
      "TRAIN Batch 0500/1314, Loss  121.4664, NLL-Loss  121.4664\n",
      "TRAIN Batch 0550/1314, Loss  121.1671, NLL-Loss  121.1671\n",
      "TRAIN Batch 0600/1314, Loss  123.0631, NLL-Loss  123.0631\n",
      "TRAIN Batch 0650/1314, Loss  117.4472, NLL-Loss  117.4472\n",
      "TRAIN Batch 0700/1314, Loss  118.7232, NLL-Loss  118.7232\n",
      "TRAIN Batch 0750/1314, Loss  112.1789, NLL-Loss  112.1789\n",
      "TRAIN Batch 0800/1314, Loss  101.5504, NLL-Loss  101.5504\n",
      "TRAIN Batch 0850/1314, Loss  126.9566, NLL-Loss  126.9566\n",
      "TRAIN Batch 0900/1314, Loss  123.7500, NLL-Loss  123.7500\n",
      "TRAIN Batch 0950/1314, Loss  135.5379, NLL-Loss  135.5379\n",
      "TRAIN Batch 1000/1314, Loss  123.6027, NLL-Loss  123.6027\n",
      "TRAIN Batch 1050/1314, Loss  110.7359, NLL-Loss  110.7359\n",
      "TRAIN Batch 1100/1314, Loss  133.4105, NLL-Loss  133.4105\n",
      "TRAIN Batch 1150/1314, Loss  144.1703, NLL-Loss  144.1703\n",
      "TRAIN Batch 1200/1314, Loss  108.2162, NLL-Loss  108.2162\n",
      "TRAIN Batch 1250/1314, Loss  124.7273, NLL-Loss  124.7273\n",
      "TRAIN Batch 1300/1314, Loss  128.4933, NLL-Loss  128.4933\n",
      "TRAIN Batch 1314/1314, Loss  111.8208, NLL-Loss  111.8208\n",
      "TRAIN Epoch 11/300, Mean NLL  122.1978\n",
      "Model saved at bin/2019-May-17-21:02:23/E11.pytorch\n",
      "VALID Batch 0000/105, Loss  138.5270, NLL-Loss  138.5270\n",
      "VALID Batch 0050/105, Loss  143.0413, NLL-Loss  143.0413\n",
      "VALID Batch 0100/105, Loss  103.6295, NLL-Loss  103.6295\n",
      "VALID Batch 0105/105, Loss  105.7325, NLL-Loss  105.7325\n",
      "VALID Epoch 11/300, Mean NLL  120.2753\n",
      "TEST Batch 0000/117, Loss  114.0339, NLL-Loss  114.0339\n",
      "TEST Batch 0050/117, Loss  124.4055, NLL-Loss  124.4055\n",
      "TEST Batch 0100/117, Loss  144.6888, NLL-Loss  144.6888\n",
      "TEST Batch 0117/117, Loss  170.3021, NLL-Loss  170.3021\n",
      "TEST Epoch 11/300, Mean NLL  120.3456\n",
      "TRAIN Batch 0000/1314, Loss  130.3827, NLL-Loss  130.3827\n",
      "TRAIN Batch 0050/1314, Loss  112.1464, NLL-Loss  112.1464\n",
      "TRAIN Batch 0100/1314, Loss  109.4640, NLL-Loss  109.4640\n",
      "TRAIN Batch 0150/1314, Loss  129.9033, NLL-Loss  129.9033\n",
      "TRAIN Batch 0200/1314, Loss  128.3928, NLL-Loss  128.3928\n",
      "TRAIN Batch 0250/1314, Loss  115.7943, NLL-Loss  115.7943\n",
      "TRAIN Batch 0300/1314, Loss   97.1388, NLL-Loss   97.1388\n",
      "TRAIN Batch 0350/1314, Loss  119.5166, NLL-Loss  119.5166\n",
      "TRAIN Batch 0400/1314, Loss  114.9772, NLL-Loss  114.9772\n",
      "TRAIN Batch 0450/1314, Loss  114.4142, NLL-Loss  114.4142\n",
      "TRAIN Batch 0500/1314, Loss   97.2276, NLL-Loss   97.2276\n",
      "TRAIN Batch 0550/1314, Loss  114.4791, NLL-Loss  114.4791\n",
      "TRAIN Batch 0600/1314, Loss  121.7845, NLL-Loss  121.7845\n",
      "TRAIN Batch 0650/1314, Loss  105.3370, NLL-Loss  105.3370\n",
      "TRAIN Batch 0700/1314, Loss  130.4844, NLL-Loss  130.4844\n",
      "TRAIN Batch 0750/1314, Loss  137.6523, NLL-Loss  137.6523\n",
      "TRAIN Batch 0800/1314, Loss  127.5648, NLL-Loss  127.5648\n",
      "TRAIN Batch 0850/1314, Loss  123.7570, NLL-Loss  123.7570\n",
      "TRAIN Batch 0900/1314, Loss  127.9092, NLL-Loss  127.9092\n",
      "TRAIN Batch 0950/1314, Loss  131.4468, NLL-Loss  131.4468\n",
      "TRAIN Batch 1000/1314, Loss  121.9907, NLL-Loss  121.9907\n",
      "TRAIN Batch 1050/1314, Loss  122.1972, NLL-Loss  122.1972\n",
      "TRAIN Batch 1100/1314, Loss  123.1579, NLL-Loss  123.1579\n",
      "TRAIN Batch 1150/1314, Loss  132.2398, NLL-Loss  132.2398\n",
      "TRAIN Batch 1200/1314, Loss  110.0556, NLL-Loss  110.0556\n",
      "TRAIN Batch 1250/1314, Loss  112.6360, NLL-Loss  112.6360\n",
      "TRAIN Batch 1300/1314, Loss  121.6224, NLL-Loss  121.6224\n",
      "TRAIN Batch 1314/1314, Loss  108.0412, NLL-Loss  108.0412\n",
      "TRAIN Epoch 12/300, Mean NLL  121.4651\n",
      "Model saved at bin/2019-May-17-21:02:23/E12.pytorch\n",
      "VALID Batch 0000/105, Loss  138.1763, NLL-Loss  138.1763\n",
      "VALID Batch 0050/105, Loss  141.8912, NLL-Loss  141.8912\n",
      "VALID Batch 0100/105, Loss  103.1208, NLL-Loss  103.1208\n",
      "VALID Batch 0105/105, Loss  105.4815, NLL-Loss  105.4815\n",
      "VALID Epoch 12/300, Mean NLL  119.7053\n",
      "TEST Batch 0000/117, Loss  113.4745, NLL-Loss  113.4745\n",
      "TEST Batch 0050/117, Loss  124.1785, NLL-Loss  124.1785\n",
      "TEST Batch 0100/117, Loss  144.2637, NLL-Loss  144.2637\n",
      "TEST Batch 0117/117, Loss  169.6429, NLL-Loss  169.6429\n",
      "TEST Epoch 12/300, Mean NLL  119.7601\n",
      "TRAIN Batch 0000/1314, Loss  128.5814, NLL-Loss  128.5814\n",
      "TRAIN Batch 0050/1314, Loss  105.6545, NLL-Loss  105.6545\n",
      "TRAIN Batch 0100/1314, Loss  116.7296, NLL-Loss  116.7296\n",
      "TRAIN Batch 0150/1314, Loss  116.7362, NLL-Loss  116.7362\n",
      "TRAIN Batch 0200/1314, Loss  106.8571, NLL-Loss  106.8571\n",
      "TRAIN Batch 0250/1314, Loss  136.0192, NLL-Loss  136.0192\n",
      "TRAIN Batch 0300/1314, Loss  118.5639, NLL-Loss  118.5639\n",
      "TRAIN Batch 0350/1314, Loss  117.3717, NLL-Loss  117.3717\n",
      "TRAIN Batch 0400/1314, Loss  125.4026, NLL-Loss  125.4026\n",
      "TRAIN Batch 0450/1314, Loss  102.8658, NLL-Loss  102.8658\n",
      "TRAIN Batch 0500/1314, Loss  121.9085, NLL-Loss  121.9085\n",
      "TRAIN Batch 0550/1314, Loss  130.0076, NLL-Loss  130.0076\n",
      "TRAIN Batch 0600/1314, Loss  131.0969, NLL-Loss  131.0969\n",
      "TRAIN Batch 0650/1314, Loss  101.3841, NLL-Loss  101.3841\n",
      "TRAIN Batch 0700/1314, Loss  120.4265, NLL-Loss  120.4265\n",
      "TRAIN Batch 0750/1314, Loss  123.1806, NLL-Loss  123.1806\n",
      "TRAIN Batch 0800/1314, Loss  127.7813, NLL-Loss  127.7813\n",
      "TRAIN Batch 0850/1314, Loss  114.3724, NLL-Loss  114.3724\n",
      "TRAIN Batch 0900/1314, Loss  108.4256, NLL-Loss  108.4256\n",
      "TRAIN Batch 0950/1314, Loss  121.9285, NLL-Loss  121.9285\n",
      "TRAIN Batch 1000/1314, Loss  144.5596, NLL-Loss  144.5596\n",
      "TRAIN Batch 1050/1314, Loss  117.2136, NLL-Loss  117.2136\n",
      "TRAIN Batch 1100/1314, Loss  108.1710, NLL-Loss  108.1710\n",
      "TRAIN Batch 1150/1314, Loss  123.6768, NLL-Loss  123.6768\n",
      "TRAIN Batch 1200/1314, Loss  128.5237, NLL-Loss  128.5237\n",
      "TRAIN Batch 1250/1314, Loss  119.2502, NLL-Loss  119.2502\n",
      "TRAIN Batch 1300/1314, Loss  113.1287, NLL-Loss  113.1287\n",
      "TRAIN Batch 1314/1314, Loss  124.5272, NLL-Loss  124.5272\n",
      "TRAIN Epoch 13/300, Mean NLL  120.7889\n",
      "Model saved at bin/2019-May-17-21:02:23/E13.pytorch\n",
      "VALID Batch 0000/105, Loss  137.8626, NLL-Loss  137.8626\n",
      "VALID Batch 0050/105, Loss  141.0950, NLL-Loss  141.0950\n",
      "VALID Batch 0100/105, Loss  102.8958, NLL-Loss  102.8958\n",
      "VALID Batch 0105/105, Loss  104.9256, NLL-Loss  104.9255\n",
      "VALID Epoch 13/300, Mean NLL  119.2935\n",
      "TEST Batch 0000/117, Loss  113.0602, NLL-Loss  113.0602\n",
      "TEST Batch 0050/117, Loss  123.5734, NLL-Loss  123.5734\n",
      "TEST Batch 0100/117, Loss  144.0660, NLL-Loss  144.0660\n",
      "TEST Batch 0117/117, Loss  169.2721, NLL-Loss  169.2721\n",
      "TEST Epoch 13/300, Mean NLL  119.3405\n",
      "TRAIN Batch 0000/1314, Loss  120.8135, NLL-Loss  120.8135\n",
      "TRAIN Batch 0050/1314, Loss  110.5907, NLL-Loss  110.5907\n",
      "TRAIN Batch 0100/1314, Loss  118.6043, NLL-Loss  118.6043\n",
      "TRAIN Batch 0150/1314, Loss  118.5963, NLL-Loss  118.5963\n",
      "TRAIN Batch 0200/1314, Loss  105.7093, NLL-Loss  105.7093\n",
      "TRAIN Batch 0250/1314, Loss  128.2626, NLL-Loss  128.2626\n",
      "TRAIN Batch 0300/1314, Loss  119.4153, NLL-Loss  119.4153\n",
      "TRAIN Batch 0350/1314, Loss  131.3166, NLL-Loss  131.3166\n",
      "TRAIN Batch 0400/1314, Loss  133.4855, NLL-Loss  133.4855\n",
      "TRAIN Batch 0450/1314, Loss   97.2984, NLL-Loss   97.2984\n",
      "TRAIN Batch 0500/1314, Loss  130.3453, NLL-Loss  130.3453\n",
      "TRAIN Batch 0550/1314, Loss  119.2196, NLL-Loss  119.2196\n",
      "TRAIN Batch 0600/1314, Loss  126.2514, NLL-Loss  126.2514\n",
      "TRAIN Batch 0650/1314, Loss  118.6973, NLL-Loss  118.6973\n",
      "TRAIN Batch 0700/1314, Loss  104.6594, NLL-Loss  104.6594\n",
      "TRAIN Batch 0750/1314, Loss  121.3172, NLL-Loss  121.3172\n",
      "TRAIN Batch 0800/1314, Loss  131.8359, NLL-Loss  131.8359\n",
      "TRAIN Batch 0850/1314, Loss  108.6905, NLL-Loss  108.6905\n",
      "TRAIN Batch 0900/1314, Loss  127.3645, NLL-Loss  127.3645\n",
      "TRAIN Batch 0950/1314, Loss  116.4196, NLL-Loss  116.4196\n",
      "TRAIN Batch 1000/1314, Loss  101.0270, NLL-Loss  101.0270\n",
      "TRAIN Batch 1050/1314, Loss  118.3956, NLL-Loss  118.3956\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN Batch 1100/1314, Loss  112.7545, NLL-Loss  112.7545\n",
      "TRAIN Batch 1150/1314, Loss  138.2312, NLL-Loss  138.2312\n",
      "TRAIN Batch 1200/1314, Loss  119.5946, NLL-Loss  119.5946\n",
      "TRAIN Batch 1250/1314, Loss  127.9386, NLL-Loss  127.9386\n",
      "TRAIN Batch 1300/1314, Loss  112.3614, NLL-Loss  112.3614\n",
      "TRAIN Batch 1314/1314, Loss  104.0126, NLL-Loss  104.0126\n",
      "TRAIN Epoch 14/300, Mean NLL  120.1421\n",
      "Model saved at bin/2019-May-17-21:02:23/E14.pytorch\n",
      "VALID Batch 0000/105, Loss  137.2786, NLL-Loss  137.2786\n",
      "VALID Batch 0050/105, Loss  140.2623, NLL-Loss  140.2623\n",
      "VALID Batch 0100/105, Loss  102.5345, NLL-Loss  102.5345\n",
      "VALID Batch 0105/105, Loss  104.4288, NLL-Loss  104.4288\n",
      "VALID Epoch 14/300, Mean NLL  118.7431\n",
      "TEST Batch 0000/117, Loss  112.3345, NLL-Loss  112.3345\n",
      "TEST Batch 0050/117, Loss  123.3312, NLL-Loss  123.3312\n",
      "TEST Batch 0100/117, Loss  143.6227, NLL-Loss  143.6227\n",
      "TEST Batch 0117/117, Loss  168.2404, NLL-Loss  168.2403\n",
      "TEST Epoch 14/300, Mean NLL  118.8006\n",
      "TRAIN Batch 0000/1314, Loss  113.1686, NLL-Loss  113.1686\n",
      "TRAIN Batch 0050/1314, Loss  128.5690, NLL-Loss  128.5690\n",
      "TRAIN Batch 0100/1314, Loss   99.5372, NLL-Loss   99.5372\n",
      "TRAIN Batch 0150/1314, Loss  136.2492, NLL-Loss  136.2492\n",
      "TRAIN Batch 0200/1314, Loss  107.6405, NLL-Loss  107.6405\n",
      "TRAIN Batch 0250/1314, Loss  122.8761, NLL-Loss  122.8761\n",
      "TRAIN Batch 0300/1314, Loss  110.2457, NLL-Loss  110.2457\n",
      "TRAIN Batch 0350/1314, Loss  112.8798, NLL-Loss  112.8798\n",
      "TRAIN Batch 0400/1314, Loss  121.7507, NLL-Loss  121.7507\n",
      "TRAIN Batch 0450/1314, Loss  134.8794, NLL-Loss  134.8794\n",
      "TRAIN Batch 0500/1314, Loss  108.6291, NLL-Loss  108.6291\n",
      "TRAIN Batch 0550/1314, Loss  121.8460, NLL-Loss  121.8460\n",
      "TRAIN Batch 0600/1314, Loss  126.7269, NLL-Loss  126.7269\n",
      "TRAIN Batch 0650/1314, Loss  115.9537, NLL-Loss  115.9537\n",
      "TRAIN Batch 0700/1314, Loss  113.4037, NLL-Loss  113.4037\n",
      "TRAIN Batch 0750/1314, Loss  141.5442, NLL-Loss  141.5442\n",
      "TRAIN Batch 0800/1314, Loss  132.3018, NLL-Loss  132.3018\n",
      "TRAIN Batch 0850/1314, Loss  124.8867, NLL-Loss  124.8867\n",
      "TRAIN Batch 0900/1314, Loss  103.6055, NLL-Loss  103.6055\n",
      "TRAIN Batch 0950/1314, Loss  121.5266, NLL-Loss  121.5266\n",
      "TRAIN Batch 1000/1314, Loss  104.8916, NLL-Loss  104.8916\n",
      "TRAIN Batch 1050/1314, Loss  121.4930, NLL-Loss  121.4930\n",
      "TRAIN Batch 1100/1314, Loss  118.1010, NLL-Loss  118.1010\n",
      "TRAIN Batch 1150/1314, Loss  124.3388, NLL-Loss  124.3388\n",
      "TRAIN Batch 1200/1314, Loss  110.5193, NLL-Loss  110.5193\n",
      "TRAIN Batch 1250/1314, Loss  115.1090, NLL-Loss  115.1090\n",
      "TRAIN Batch 1300/1314, Loss  112.2920, NLL-Loss  112.2920\n",
      "TRAIN Batch 1314/1314, Loss  111.1480, NLL-Loss  111.1480\n",
      "TRAIN Epoch 15/300, Mean NLL  119.5685\n",
      "Model saved at bin/2019-May-17-21:02:23/E15.pytorch\n",
      "VALID Batch 0000/105, Loss  137.2542, NLL-Loss  137.2542\n",
      "VALID Batch 0050/105, Loss  139.9110, NLL-Loss  139.9110\n",
      "VALID Batch 0100/105, Loss  102.4209, NLL-Loss  102.4209\n",
      "VALID Batch 0105/105, Loss  104.2841, NLL-Loss  104.2841\n",
      "VALID Epoch 15/300, Mean NLL  118.5067\n",
      "TEST Batch 0000/117, Loss  112.0821, NLL-Loss  112.0821\n",
      "TEST Batch 0050/117, Loss  123.0599, NLL-Loss  123.0599\n",
      "TEST Batch 0100/117, Loss  143.4348, NLL-Loss  143.4348\n",
      "TEST Batch 0117/117, Loss  168.2935, NLL-Loss  168.2935\n",
      "TEST Epoch 15/300, Mean NLL  118.5704\n",
      "TRAIN Batch 0000/1314, Loss  134.6621, NLL-Loss  134.6621\n",
      "TRAIN Batch 0050/1314, Loss  114.9509, NLL-Loss  114.9509\n",
      "TRAIN Batch 0100/1314, Loss  118.8535, NLL-Loss  118.8535\n",
      "TRAIN Batch 0150/1314, Loss  116.6839, NLL-Loss  116.6839\n",
      "TRAIN Batch 0200/1314, Loss  113.2240, NLL-Loss  113.2240\n",
      "TRAIN Batch 0250/1314, Loss  103.1075, NLL-Loss  103.1075\n",
      "TRAIN Batch 0300/1314, Loss  118.1370, NLL-Loss  118.1370\n",
      "TRAIN Batch 0350/1314, Loss  130.3262, NLL-Loss  130.3262\n",
      "TRAIN Batch 0400/1314, Loss  111.9199, NLL-Loss  111.9199\n",
      "TRAIN Batch 0450/1314, Loss  115.8150, NLL-Loss  115.8150\n",
      "TRAIN Batch 0500/1314, Loss  118.6242, NLL-Loss  118.6242\n",
      "TRAIN Batch 0550/1314, Loss  117.7104, NLL-Loss  117.7104\n",
      "TRAIN Batch 0600/1314, Loss  147.2098, NLL-Loss  147.2098\n",
      "TRAIN Batch 0650/1314, Loss  121.9948, NLL-Loss  121.9948\n",
      "TRAIN Batch 0700/1314, Loss  123.8362, NLL-Loss  123.8362\n",
      "TRAIN Batch 0750/1314, Loss  127.8070, NLL-Loss  127.8070\n",
      "TRAIN Batch 0800/1314, Loss  119.3852, NLL-Loss  119.3852\n",
      "TRAIN Batch 0850/1314, Loss  115.7898, NLL-Loss  115.7898\n",
      "TRAIN Batch 0900/1314, Loss  125.1211, NLL-Loss  125.1211\n",
      "TRAIN Batch 0950/1314, Loss  127.1645, NLL-Loss  127.1645\n",
      "TRAIN Batch 1000/1314, Loss  117.3221, NLL-Loss  117.3221\n",
      "TRAIN Batch 1050/1314, Loss  134.0954, NLL-Loss  134.0954\n",
      "TRAIN Batch 1100/1314, Loss  121.4519, NLL-Loss  121.4519\n",
      "TRAIN Batch 1150/1314, Loss  146.0379, NLL-Loss  146.0379\n",
      "TRAIN Batch 1200/1314, Loss  105.6902, NLL-Loss  105.6902\n",
      "TRAIN Batch 1250/1314, Loss  129.6137, NLL-Loss  129.6137\n",
      "TRAIN Batch 1300/1314, Loss  127.9600, NLL-Loss  127.9600\n",
      "TRAIN Batch 1314/1314, Loss  111.4566, NLL-Loss  111.4566\n",
      "TRAIN Epoch 16/300, Mean NLL  119.0003\n",
      "Model saved at bin/2019-May-17-21:02:23/E16.pytorch\n",
      "VALID Batch 0000/105, Loss  137.0730, NLL-Loss  137.0730\n",
      "VALID Batch 0050/105, Loss  138.6109, NLL-Loss  138.6109\n",
      "VALID Batch 0100/105, Loss  102.0161, NLL-Loss  102.0161\n",
      "VALID Batch 0105/105, Loss  104.1170, NLL-Loss  104.1170\n",
      "VALID Epoch 16/300, Mean NLL  118.0256\n",
      "TEST Batch 0000/117, Loss  111.4046, NLL-Loss  111.4046\n",
      "TEST Batch 0050/117, Loss  122.6144, NLL-Loss  122.6144\n",
      "TEST Batch 0100/117, Loss  143.1959, NLL-Loss  143.1959\n",
      "TEST Batch 0117/117, Loss  167.7469, NLL-Loss  167.7469\n",
      "TEST Epoch 16/300, Mean NLL  118.0984\n",
      "TRAIN Batch 0000/1314, Loss  119.7303, NLL-Loss  119.7303\n",
      "TRAIN Batch 0050/1314, Loss  124.9181, NLL-Loss  124.9181\n",
      "TRAIN Batch 0100/1314, Loss  117.9382, NLL-Loss  117.9382\n",
      "TRAIN Batch 0150/1314, Loss  113.4907, NLL-Loss  113.4907\n",
      "TRAIN Batch 0200/1314, Loss  130.7056, NLL-Loss  130.7056\n",
      "TRAIN Batch 0250/1314, Loss  133.4807, NLL-Loss  133.4807\n",
      "TRAIN Batch 0300/1314, Loss  125.6123, NLL-Loss  125.6123\n",
      "TRAIN Batch 0350/1314, Loss  108.2029, NLL-Loss  108.2029\n",
      "TRAIN Batch 0400/1314, Loss  124.1545, NLL-Loss  124.1545\n",
      "TRAIN Batch 0450/1314, Loss  123.2531, NLL-Loss  123.2531\n",
      "TRAIN Batch 0500/1314, Loss  103.2272, NLL-Loss  103.2272\n",
      "TRAIN Batch 0550/1314, Loss  141.3159, NLL-Loss  141.3159\n",
      "TRAIN Batch 0600/1314, Loss  114.0258, NLL-Loss  114.0258\n",
      "TRAIN Batch 0650/1314, Loss  124.0357, NLL-Loss  124.0357\n",
      "TRAIN Batch 0700/1314, Loss  110.8921, NLL-Loss  110.8921\n",
      "TRAIN Batch 0750/1314, Loss  112.1265, NLL-Loss  112.1265\n",
      "TRAIN Batch 0800/1314, Loss  107.5265, NLL-Loss  107.5265\n",
      "TRAIN Batch 0850/1314, Loss  140.1842, NLL-Loss  140.1842\n",
      "TRAIN Batch 0900/1314, Loss  110.9472, NLL-Loss  110.9472\n",
      "TRAIN Batch 0950/1314, Loss  107.6250, NLL-Loss  107.6250\n",
      "TRAIN Batch 1000/1314, Loss  124.6493, NLL-Loss  124.6493\n",
      "TRAIN Batch 1050/1314, Loss  112.6001, NLL-Loss  112.6001\n",
      "TRAIN Batch 1100/1314, Loss  121.2050, NLL-Loss  121.2050\n",
      "TRAIN Batch 1150/1314, Loss  111.5641, NLL-Loss  111.5641\n",
      "TRAIN Batch 1200/1314, Loss  118.5398, NLL-Loss  118.5398\n",
      "TRAIN Batch 1250/1314, Loss  126.4318, NLL-Loss  126.4318\n",
      "TRAIN Batch 1300/1314, Loss  120.5701, NLL-Loss  120.5701\n",
      "TRAIN Batch 1314/1314, Loss  118.8471, NLL-Loss  118.8471\n",
      "TRAIN Epoch 17/300, Mean NLL  118.4986\n",
      "Model saved at bin/2019-May-17-21:02:23/E17.pytorch\n",
      "VALID Batch 0000/105, Loss  136.7873, NLL-Loss  136.7873\n",
      "VALID Batch 0050/105, Loss  138.0027, NLL-Loss  138.0027\n",
      "VALID Batch 0100/105, Loss  101.9157, NLL-Loss  101.9157\n",
      "VALID Batch 0105/105, Loss  103.7461, NLL-Loss  103.7461\n",
      "VALID Epoch 17/300, Mean NLL  117.7129\n",
      "TEST Batch 0000/117, Loss  111.1300, NLL-Loss  111.1300\n",
      "TEST Batch 0050/117, Loss  122.2254, NLL-Loss  122.2254\n",
      "TEST Batch 0100/117, Loss  142.9199, NLL-Loss  142.9199\n",
      "TEST Batch 0117/117, Loss  167.4737, NLL-Loss  167.4737\n",
      "TEST Epoch 17/300, Mean NLL  117.7726\n",
      "TRAIN Batch 0000/1314, Loss  102.3177, NLL-Loss  102.3177\n",
      "TRAIN Batch 0050/1314, Loss  117.5797, NLL-Loss  117.5797\n",
      "TRAIN Batch 0100/1314, Loss  109.7715, NLL-Loss  109.7715\n",
      "TRAIN Batch 0150/1314, Loss  124.8707, NLL-Loss  124.8707\n",
      "TRAIN Batch 0200/1314, Loss  110.9012, NLL-Loss  110.9012\n",
      "TRAIN Batch 0250/1314, Loss  131.1195, NLL-Loss  131.1195\n",
      "TRAIN Batch 0300/1314, Loss  120.8688, NLL-Loss  120.8688\n",
      "TRAIN Batch 0350/1314, Loss  105.1492, NLL-Loss  105.1492\n",
      "TRAIN Batch 0400/1314, Loss  109.8047, NLL-Loss  109.8047\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN Batch 0450/1314, Loss  122.7615, NLL-Loss  122.7615\n",
      "TRAIN Batch 0500/1314, Loss  127.6701, NLL-Loss  127.6701\n",
      "TRAIN Batch 0550/1314, Loss  118.7749, NLL-Loss  118.7749\n",
      "TRAIN Batch 0600/1314, Loss  102.8919, NLL-Loss  102.8919\n",
      "TRAIN Batch 0650/1314, Loss  109.4062, NLL-Loss  109.4062\n",
      "TRAIN Batch 0700/1314, Loss  128.9947, NLL-Loss  128.9947\n",
      "TRAIN Batch 0750/1314, Loss  114.5945, NLL-Loss  114.5945\n",
      "TRAIN Batch 0800/1314, Loss  122.7688, NLL-Loss  122.7688\n",
      "TRAIN Batch 0850/1314, Loss  125.4552, NLL-Loss  125.4552\n",
      "TRAIN Batch 0900/1314, Loss  116.9783, NLL-Loss  116.9783\n",
      "TRAIN Batch 0950/1314, Loss  117.0820, NLL-Loss  117.0820\n",
      "TRAIN Batch 1000/1314, Loss  135.9837, NLL-Loss  135.9837\n",
      "TRAIN Batch 1050/1314, Loss  109.5287, NLL-Loss  109.5287\n",
      "TRAIN Batch 1100/1314, Loss   95.9979, NLL-Loss   95.9979\n",
      "TRAIN Batch 1150/1314, Loss  124.8408, NLL-Loss  124.8408\n",
      "TRAIN Batch 1200/1314, Loss  134.9852, NLL-Loss  134.9852\n",
      "TRAIN Batch 1250/1314, Loss  126.8724, NLL-Loss  126.8724\n",
      "TRAIN Batch 1300/1314, Loss  129.4826, NLL-Loss  129.4826\n",
      "TRAIN Batch 1314/1314, Loss  111.7683, NLL-Loss  111.7683\n",
      "TRAIN Epoch 18/300, Mean NLL  117.9911\n",
      "Model saved at bin/2019-May-17-21:02:23/E18.pytorch\n",
      "VALID Batch 0000/105, Loss  136.5219, NLL-Loss  136.5219\n",
      "VALID Batch 0050/105, Loss  137.3272, NLL-Loss  137.3272\n",
      "VALID Batch 0100/105, Loss  101.7212, NLL-Loss  101.7212\n",
      "VALID Batch 0105/105, Loss  103.4574, NLL-Loss  103.4574\n",
      "VALID Epoch 18/300, Mean NLL  117.3208\n",
      "TEST Batch 0000/117, Loss  110.8119, NLL-Loss  110.8119\n",
      "TEST Batch 0050/117, Loss  121.9261, NLL-Loss  121.9261\n",
      "TEST Batch 0100/117, Loss  142.4530, NLL-Loss  142.4530\n",
      "TEST Batch 0117/117, Loss  166.9677, NLL-Loss  166.9677\n",
      "TEST Epoch 18/300, Mean NLL  117.3865\n",
      "TRAIN Batch 0000/1314, Loss  113.1019, NLL-Loss  113.1019\n",
      "TRAIN Batch 0050/1314, Loss  113.3369, NLL-Loss  113.3369\n",
      "TRAIN Batch 0100/1314, Loss  114.4307, NLL-Loss  114.4307\n",
      "TRAIN Batch 0150/1314, Loss  115.9565, NLL-Loss  115.9565\n",
      "TRAIN Batch 0200/1314, Loss  127.9100, NLL-Loss  127.9100\n",
      "TRAIN Batch 0250/1314, Loss  121.2825, NLL-Loss  121.2825\n",
      "TRAIN Batch 0300/1314, Loss  115.8189, NLL-Loss  115.8189\n",
      "TRAIN Batch 0350/1314, Loss  116.1781, NLL-Loss  116.1781\n",
      "TRAIN Batch 0400/1314, Loss  127.4648, NLL-Loss  127.4648\n",
      "TRAIN Batch 0450/1314, Loss  125.1227, NLL-Loss  125.1227\n",
      "TRAIN Batch 0500/1314, Loss  124.8468, NLL-Loss  124.8468\n",
      "TRAIN Batch 0550/1314, Loss  134.8256, NLL-Loss  134.8256\n",
      "TRAIN Batch 0600/1314, Loss  112.7555, NLL-Loss  112.7555\n",
      "TRAIN Batch 0650/1314, Loss  107.4652, NLL-Loss  107.4652\n",
      "TRAIN Batch 0700/1314, Loss  114.3283, NLL-Loss  114.3283\n",
      "TRAIN Batch 0750/1314, Loss  109.4552, NLL-Loss  109.4552\n",
      "TRAIN Batch 0800/1314, Loss  113.9555, NLL-Loss  113.9555\n",
      "TRAIN Batch 0850/1314, Loss  145.5249, NLL-Loss  145.5249\n",
      "TRAIN Batch 0900/1314, Loss  123.4740, NLL-Loss  123.4740\n",
      "TRAIN Batch 0950/1314, Loss  113.2696, NLL-Loss  113.2696\n",
      "TRAIN Batch 1000/1314, Loss  108.9987, NLL-Loss  108.9987\n",
      "TRAIN Batch 1050/1314, Loss   87.4607, NLL-Loss   87.4607\n",
      "TRAIN Batch 1100/1314, Loss  119.3899, NLL-Loss  119.3899\n",
      "TRAIN Batch 1150/1314, Loss  125.1713, NLL-Loss  125.1713\n",
      "TRAIN Batch 1200/1314, Loss  116.7051, NLL-Loss  116.7051\n",
      "TRAIN Batch 1250/1314, Loss  121.3722, NLL-Loss  121.3722\n",
      "TRAIN Batch 1300/1314, Loss  102.5557, NLL-Loss  102.5557\n",
      "TRAIN Batch 1314/1314, Loss  128.3073, NLL-Loss  128.3073\n",
      "TRAIN Epoch 19/300, Mean NLL  117.5231\n",
      "Model saved at bin/2019-May-17-21:02:23/E19.pytorch\n",
      "VALID Batch 0000/105, Loss  136.4750, NLL-Loss  136.4750\n",
      "VALID Batch 0050/105, Loss  136.7586, NLL-Loss  136.7586\n",
      "VALID Batch 0100/105, Loss  101.3110, NLL-Loss  101.3110\n",
      "VALID Batch 0105/105, Loss  103.5237, NLL-Loss  103.5237\n",
      "VALID Epoch 19/300, Mean NLL  117.0701\n",
      "TEST Batch 0000/117, Loss  110.2842, NLL-Loss  110.2842\n",
      "TEST Batch 0050/117, Loss  121.7220, NLL-Loss  121.7220\n",
      "TEST Batch 0100/117, Loss  142.6431, NLL-Loss  142.6431\n",
      "TEST Batch 0117/117, Loss  167.0810, NLL-Loss  167.0810\n",
      "TEST Epoch 19/300, Mean NLL  117.1467\n",
      "TRAIN Batch 0000/1314, Loss  118.9097, NLL-Loss  118.9097\n",
      "TRAIN Batch 0050/1314, Loss  118.6148, NLL-Loss  118.6148\n",
      "TRAIN Batch 0100/1314, Loss  127.3693, NLL-Loss  127.3693\n",
      "TRAIN Batch 0150/1314, Loss  109.8055, NLL-Loss  109.8055\n",
      "TRAIN Batch 0200/1314, Loss  109.7879, NLL-Loss  109.7879\n",
      "TRAIN Batch 0250/1314, Loss   96.9695, NLL-Loss   96.9695\n",
      "TRAIN Batch 0300/1314, Loss  111.1563, NLL-Loss  111.1563\n",
      "TRAIN Batch 0350/1314, Loss  139.7104, NLL-Loss  139.7104\n",
      "TRAIN Batch 0400/1314, Loss  129.1234, NLL-Loss  129.1234\n",
      "TRAIN Batch 0450/1314, Loss  109.5848, NLL-Loss  109.5848\n",
      "TRAIN Batch 0500/1314, Loss  105.3994, NLL-Loss  105.3994\n",
      "TRAIN Batch 0550/1314, Loss  127.8438, NLL-Loss  127.8438\n",
      "TRAIN Batch 0600/1314, Loss  113.5452, NLL-Loss  113.5452\n",
      "TRAIN Batch 0650/1314, Loss  117.2301, NLL-Loss  117.2301\n",
      "TRAIN Batch 0700/1314, Loss  105.0451, NLL-Loss  105.0451\n",
      "TRAIN Batch 0750/1314, Loss  124.9004, NLL-Loss  124.9004\n",
      "TRAIN Batch 0800/1314, Loss  119.9352, NLL-Loss  119.9352\n",
      "TRAIN Batch 0850/1314, Loss  110.1135, NLL-Loss  110.1135\n",
      "TRAIN Batch 0900/1314, Loss  117.4437, NLL-Loss  117.4437\n",
      "TRAIN Batch 0950/1314, Loss  117.2766, NLL-Loss  117.2766\n",
      "TRAIN Batch 1000/1314, Loss  106.6127, NLL-Loss  106.6127\n",
      "TRAIN Batch 1050/1314, Loss  107.5293, NLL-Loss  107.5293\n",
      "TRAIN Batch 1100/1314, Loss  115.8740, NLL-Loss  115.8740\n",
      "TRAIN Batch 1150/1314, Loss  119.0070, NLL-Loss  119.0070\n",
      "TRAIN Batch 1200/1314, Loss  119.1360, NLL-Loss  119.1360\n",
      "TRAIN Batch 1250/1314, Loss   96.2015, NLL-Loss   96.2015\n",
      "TRAIN Batch 1300/1314, Loss  104.9666, NLL-Loss  104.9666\n",
      "TRAIN Batch 1314/1314, Loss   96.0918, NLL-Loss   96.0918\n",
      "TRAIN Epoch 20/300, Mean NLL  117.0565\n",
      "Model saved at bin/2019-May-17-21:02:23/E20.pytorch\n",
      "VALID Batch 0000/105, Loss  136.0087, NLL-Loss  136.0087\n",
      "VALID Batch 0050/105, Loss  136.3649, NLL-Loss  136.3649\n",
      "VALID Batch 0100/105, Loss  101.5322, NLL-Loss  101.5322\n",
      "VALID Batch 0105/105, Loss  102.9226, NLL-Loss  102.9226\n",
      "VALID Epoch 20/300, Mean NLL  116.6746\n",
      "TEST Batch 0000/117, Loss  110.0945, NLL-Loss  110.0945\n",
      "TEST Batch 0050/117, Loss  121.6549, NLL-Loss  121.6549\n",
      "TEST Batch 0100/117, Loss  141.8729, NLL-Loss  141.8729\n",
      "TEST Batch 0117/117, Loss  166.0660, NLL-Loss  166.0660\n",
      "TEST Epoch 20/300, Mean NLL  116.7519\n",
      "TRAIN Batch 0000/1314, Loss  123.2118, NLL-Loss  123.2118\n",
      "TRAIN Batch 0050/1314, Loss  118.9906, NLL-Loss  118.9906\n",
      "TRAIN Batch 0100/1314, Loss  109.5833, NLL-Loss  109.5833\n",
      "TRAIN Batch 0150/1314, Loss  102.2472, NLL-Loss  102.2472\n",
      "TRAIN Batch 0200/1314, Loss  111.4658, NLL-Loss  111.4658\n",
      "TRAIN Batch 0250/1314, Loss  126.8157, NLL-Loss  126.8157\n",
      "TRAIN Batch 0300/1314, Loss  122.5176, NLL-Loss  122.5176\n",
      "TRAIN Batch 0350/1314, Loss  123.1896, NLL-Loss  123.1896\n",
      "TRAIN Batch 0400/1314, Loss  103.1886, NLL-Loss  103.1886\n",
      "TRAIN Batch 0450/1314, Loss  130.3519, NLL-Loss  130.3519\n",
      "TRAIN Batch 0500/1314, Loss  121.2203, NLL-Loss  121.2203\n",
      "TRAIN Batch 0550/1314, Loss  128.6721, NLL-Loss  128.6721\n",
      "TRAIN Batch 0600/1314, Loss  114.4776, NLL-Loss  114.4776\n",
      "TRAIN Batch 0650/1314, Loss  118.1253, NLL-Loss  118.1253\n",
      "TRAIN Batch 0700/1314, Loss  119.3499, NLL-Loss  119.3499\n",
      "TRAIN Batch 0750/1314, Loss  131.9027, NLL-Loss  131.9027\n",
      "TRAIN Batch 0800/1314, Loss  117.1101, NLL-Loss  117.1101\n",
      "TRAIN Batch 0850/1314, Loss  109.0520, NLL-Loss  109.0520\n",
      "TRAIN Batch 0900/1314, Loss  125.8338, NLL-Loss  125.8338\n",
      "TRAIN Batch 0950/1314, Loss  135.0067, NLL-Loss  135.0067\n",
      "TRAIN Batch 1000/1314, Loss  120.1014, NLL-Loss  120.1014\n",
      "TRAIN Batch 1050/1314, Loss  130.1532, NLL-Loss  130.1532\n",
      "TRAIN Batch 1100/1314, Loss   99.4859, NLL-Loss   99.4859\n",
      "TRAIN Batch 1150/1314, Loss  129.8804, NLL-Loss  129.8804\n",
      "TRAIN Batch 1200/1314, Loss  115.5095, NLL-Loss  115.5095\n",
      "TRAIN Batch 1250/1314, Loss  102.2512, NLL-Loss  102.2512\n",
      "TRAIN Batch 1300/1314, Loss  111.8220, NLL-Loss  111.8220\n",
      "TRAIN Batch 1314/1314, Loss  117.9889, NLL-Loss  117.9889\n",
      "TRAIN Epoch 21/300, Mean NLL  116.6387\n",
      "Model saved at bin/2019-May-17-21:02:23/E21.pytorch\n",
      "VALID Batch 0000/105, Loss  136.2731, NLL-Loss  136.2731\n",
      "VALID Batch 0050/105, Loss  135.7704, NLL-Loss  135.7704\n",
      "VALID Batch 0100/105, Loss  101.2676, NLL-Loss  101.2676\n",
      "VALID Batch 0105/105, Loss  103.1022, NLL-Loss  103.1022\n",
      "VALID Epoch 21/300, Mean NLL  116.4949\n",
      "TEST Batch 0000/117, Loss  109.6955, NLL-Loss  109.6955\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST Batch 0050/117, Loss  121.1302, NLL-Loss  121.1302\n",
      "TEST Batch 0100/117, Loss  142.0768, NLL-Loss  142.0768\n",
      "TEST Batch 0117/117, Loss  166.4302, NLL-Loss  166.4302\n",
      "TEST Epoch 21/300, Mean NLL  116.5565\n",
      "TRAIN Batch 0000/1314, Loss  115.8319, NLL-Loss  115.8319\n",
      "TRAIN Batch 0050/1314, Loss  112.7462, NLL-Loss  112.7462\n",
      "TRAIN Batch 0100/1314, Loss  130.2806, NLL-Loss  130.2806\n",
      "TRAIN Batch 0150/1314, Loss  116.0655, NLL-Loss  116.0655\n",
      "TRAIN Batch 0200/1314, Loss  109.4200, NLL-Loss  109.4200\n",
      "TRAIN Batch 0250/1314, Loss  133.7733, NLL-Loss  133.7733\n",
      "TRAIN Batch 0300/1314, Loss  101.5438, NLL-Loss  101.5438\n",
      "TRAIN Batch 0350/1314, Loss  113.3163, NLL-Loss  113.3163\n",
      "TRAIN Batch 0400/1314, Loss  134.6412, NLL-Loss  134.6412\n",
      "TRAIN Batch 0450/1314, Loss  106.8488, NLL-Loss  106.8488\n",
      "TRAIN Batch 0500/1314, Loss  113.8297, NLL-Loss  113.8297\n",
      "TRAIN Batch 0550/1314, Loss   98.5656, NLL-Loss   98.5656\n",
      "TRAIN Batch 0600/1314, Loss  128.6030, NLL-Loss  128.6030\n",
      "TRAIN Batch 0650/1314, Loss  108.9353, NLL-Loss  108.9353\n",
      "TRAIN Batch 0700/1314, Loss  127.8883, NLL-Loss  127.8883\n",
      "TRAIN Batch 0750/1314, Loss  114.0204, NLL-Loss  114.0204\n",
      "TRAIN Batch 0800/1314, Loss  135.5719, NLL-Loss  135.5719\n",
      "TRAIN Batch 0850/1314, Loss  107.1925, NLL-Loss  107.1925\n",
      "TRAIN Batch 0900/1314, Loss  125.6853, NLL-Loss  125.6853\n",
      "TRAIN Batch 0950/1314, Loss  108.2805, NLL-Loss  108.2805\n",
      "TRAIN Batch 1000/1314, Loss  119.8257, NLL-Loss  119.8257\n",
      "TRAIN Batch 1050/1314, Loss  109.6178, NLL-Loss  109.6178\n",
      "TRAIN Batch 1100/1314, Loss  104.0431, NLL-Loss  104.0431\n",
      "TRAIN Batch 1150/1314, Loss  109.8892, NLL-Loss  109.8892\n",
      "TRAIN Batch 1200/1314, Loss  105.2928, NLL-Loss  105.2928\n",
      "TRAIN Batch 1250/1314, Loss  124.2470, NLL-Loss  124.2470\n",
      "TRAIN Batch 1300/1314, Loss  105.5081, NLL-Loss  105.5081\n",
      "TRAIN Batch 1314/1314, Loss  107.6241, NLL-Loss  107.6241\n",
      "TRAIN Epoch 22/300, Mean NLL  116.2205\n",
      "Model saved at bin/2019-May-17-21:02:23/E22.pytorch\n",
      "VALID Batch 0000/105, Loss  135.9149, NLL-Loss  135.9149\n",
      "VALID Batch 0050/105, Loss  135.3997, NLL-Loss  135.3997\n",
      "VALID Batch 0100/105, Loss  101.0822, NLL-Loss  101.0822\n",
      "VALID Batch 0105/105, Loss  102.5213, NLL-Loss  102.5213\n",
      "VALID Epoch 22/300, Mean NLL  116.1718\n",
      "TEST Batch 0000/117, Loss  109.4812, NLL-Loss  109.4812\n",
      "TEST Batch 0050/117, Loss  120.8672, NLL-Loss  120.8672\n",
      "TEST Batch 0100/117, Loss  141.6184, NLL-Loss  141.6184\n",
      "TEST Batch 0117/117, Loss  165.8178, NLL-Loss  165.8178\n",
      "TEST Epoch 22/300, Mean NLL  116.2501\n",
      "TRAIN Batch 0000/1314, Loss  125.9661, NLL-Loss  125.9661\n",
      "TRAIN Batch 0050/1314, Loss  113.7981, NLL-Loss  113.7981\n",
      "TRAIN Batch 0100/1314, Loss  111.6892, NLL-Loss  111.6892\n",
      "TRAIN Batch 0150/1314, Loss  118.2846, NLL-Loss  118.2846\n",
      "TRAIN Batch 0200/1314, Loss  119.9095, NLL-Loss  119.9095\n",
      "TRAIN Batch 0250/1314, Loss  129.0223, NLL-Loss  129.0223\n",
      "TRAIN Batch 0300/1314, Loss  117.2899, NLL-Loss  117.2899\n",
      "TRAIN Batch 0350/1314, Loss  113.2391, NLL-Loss  113.2391\n",
      "TRAIN Batch 0400/1314, Loss  128.4193, NLL-Loss  128.4193\n",
      "TRAIN Batch 0450/1314, Loss  121.5397, NLL-Loss  121.5397\n",
      "TRAIN Batch 0500/1314, Loss  120.1925, NLL-Loss  120.1925\n",
      "TRAIN Batch 0550/1314, Loss  110.8410, NLL-Loss  110.8410\n",
      "TRAIN Batch 0600/1314, Loss  112.0743, NLL-Loss  112.0743\n",
      "TRAIN Batch 0650/1314, Loss  109.0159, NLL-Loss  109.0159\n",
      "TRAIN Batch 0700/1314, Loss  105.9338, NLL-Loss  105.9338\n",
      "TRAIN Batch 0750/1314, Loss  122.8541, NLL-Loss  122.8541\n",
      "TRAIN Batch 0800/1314, Loss  112.5420, NLL-Loss  112.5420\n",
      "TRAIN Batch 0850/1314, Loss  110.7336, NLL-Loss  110.7336\n",
      "TRAIN Batch 0900/1314, Loss  132.9891, NLL-Loss  132.9891\n",
      "TRAIN Batch 0950/1314, Loss  110.3060, NLL-Loss  110.3060\n",
      "TRAIN Batch 1000/1314, Loss  130.2560, NLL-Loss  130.2560\n",
      "TRAIN Batch 1050/1314, Loss  128.2601, NLL-Loss  128.2601\n",
      "TRAIN Batch 1100/1314, Loss  120.4961, NLL-Loss  120.4961\n",
      "TRAIN Batch 1150/1314, Loss  114.9952, NLL-Loss  114.9952\n",
      "TRAIN Batch 1200/1314, Loss  114.5143, NLL-Loss  114.5143\n",
      "TRAIN Batch 1250/1314, Loss  124.4054, NLL-Loss  124.4054\n",
      "TRAIN Batch 1300/1314, Loss  121.1443, NLL-Loss  121.1443\n",
      "TRAIN Batch 1314/1314, Loss   94.5181, NLL-Loss   94.5181\n",
      "TRAIN Epoch 23/300, Mean NLL  115.8356\n",
      "Model saved at bin/2019-May-17-21:02:23/E23.pytorch\n",
      "VALID Batch 0000/105, Loss  135.5732, NLL-Loss  135.5732\n",
      "VALID Batch 0050/105, Loss  134.8769, NLL-Loss  134.8769\n",
      "VALID Batch 0100/105, Loss  100.8799, NLL-Loss  100.8799\n",
      "VALID Batch 0105/105, Loss  102.4937, NLL-Loss  102.4937\n",
      "VALID Epoch 23/300, Mean NLL  115.8487\n",
      "TEST Batch 0000/117, Loss  109.0505, NLL-Loss  109.0505\n",
      "TEST Batch 0050/117, Loss  120.8064, NLL-Loss  120.8064\n",
      "TEST Batch 0100/117, Loss  141.3506, NLL-Loss  141.3506\n",
      "TEST Batch 0117/117, Loss  165.3671, NLL-Loss  165.3671\n",
      "TEST Epoch 23/300, Mean NLL  115.8995\n",
      "TRAIN Batch 0000/1314, Loss  111.4184, NLL-Loss  111.4184\n",
      "TRAIN Batch 0050/1314, Loss  106.9153, NLL-Loss  106.9153\n",
      "TRAIN Batch 0100/1314, Loss  106.5524, NLL-Loss  106.5524\n",
      "TRAIN Batch 0150/1314, Loss   97.9921, NLL-Loss   97.9921\n",
      "TRAIN Batch 0200/1314, Loss  126.6517, NLL-Loss  126.6517\n",
      "TRAIN Batch 0250/1314, Loss  100.2482, NLL-Loss  100.2482\n",
      "TRAIN Batch 0300/1314, Loss  116.6154, NLL-Loss  116.6154\n",
      "TRAIN Batch 0350/1314, Loss  123.3120, NLL-Loss  123.3120\n",
      "TRAIN Batch 0400/1314, Loss  122.9772, NLL-Loss  122.9772\n",
      "TRAIN Batch 0450/1314, Loss  106.3700, NLL-Loss  106.3700\n",
      "TRAIN Batch 0500/1314, Loss  113.9989, NLL-Loss  113.9989\n",
      "TRAIN Batch 0550/1314, Loss  119.5904, NLL-Loss  119.5904\n",
      "TRAIN Batch 0600/1314, Loss  102.7613, NLL-Loss  102.7613\n",
      "TRAIN Batch 0650/1314, Loss  108.7882, NLL-Loss  108.7882\n",
      "TRAIN Batch 0700/1314, Loss  130.5483, NLL-Loss  130.5483\n",
      "TRAIN Batch 0750/1314, Loss  112.8123, NLL-Loss  112.8123\n",
      "TRAIN Batch 0800/1314, Loss  123.4424, NLL-Loss  123.4424\n",
      "TRAIN Batch 0850/1314, Loss  121.3077, NLL-Loss  121.3077\n",
      "TRAIN Batch 0900/1314, Loss  115.4180, NLL-Loss  115.4180\n",
      "TRAIN Batch 0950/1314, Loss  104.9254, NLL-Loss  104.9254\n",
      "TRAIN Batch 1000/1314, Loss  111.8226, NLL-Loss  111.8226\n",
      "TRAIN Batch 1050/1314, Loss  114.0143, NLL-Loss  114.0143\n",
      "TRAIN Batch 1100/1314, Loss  103.0710, NLL-Loss  103.0710\n",
      "TRAIN Batch 1150/1314, Loss  121.6593, NLL-Loss  121.6593\n",
      "TRAIN Batch 1200/1314, Loss  100.7996, NLL-Loss  100.7996\n",
      "TRAIN Batch 1250/1314, Loss  124.6584, NLL-Loss  124.6584\n",
      "TRAIN Batch 1300/1314, Loss  123.3746, NLL-Loss  123.3746\n",
      "TRAIN Batch 1314/1314, Loss  126.8811, NLL-Loss  126.8811\n",
      "TRAIN Epoch 24/300, Mean NLL  115.4584\n",
      "Model saved at bin/2019-May-17-21:02:23/E24.pytorch\n",
      "VALID Batch 0000/105, Loss  135.6592, NLL-Loss  135.6592\n",
      "VALID Batch 0050/105, Loss  134.5298, NLL-Loss  134.5298\n",
      "VALID Batch 0100/105, Loss  100.7505, NLL-Loss  100.7505\n",
      "VALID Batch 0105/105, Loss  102.3166, NLL-Loss  102.3166\n",
      "VALID Epoch 24/300, Mean NLL  115.6455\n",
      "TEST Batch 0000/117, Loss  108.7052, NLL-Loss  108.7052\n",
      "TEST Batch 0050/117, Loss  120.5703, NLL-Loss  120.5703\n",
      "TEST Batch 0100/117, Loss  141.2456, NLL-Loss  141.2456\n",
      "TEST Batch 0117/117, Loss  165.3106, NLL-Loss  165.3106\n",
      "TEST Epoch 24/300, Mean NLL  115.7336\n",
      "TRAIN Batch 0000/1314, Loss  107.3726, NLL-Loss  107.3726\n",
      "TRAIN Batch 0050/1314, Loss  110.2612, NLL-Loss  110.2612\n",
      "TRAIN Batch 0100/1314, Loss  116.3373, NLL-Loss  116.3373\n",
      "TRAIN Batch 0150/1314, Loss  100.2735, NLL-Loss  100.2735\n",
      "TRAIN Batch 0200/1314, Loss  118.5595, NLL-Loss  118.5595\n",
      "TRAIN Batch 0250/1314, Loss   98.5742, NLL-Loss   98.5742\n",
      "TRAIN Batch 0300/1314, Loss  114.4540, NLL-Loss  114.4540\n",
      "TRAIN Batch 0350/1314, Loss  118.9108, NLL-Loss  118.9108\n",
      "TRAIN Batch 0400/1314, Loss  104.5743, NLL-Loss  104.5743\n",
      "TRAIN Batch 0450/1314, Loss  124.7479, NLL-Loss  124.7479\n",
      "TRAIN Batch 0500/1314, Loss  113.1259, NLL-Loss  113.1259\n",
      "TRAIN Batch 0550/1314, Loss  129.6618, NLL-Loss  129.6618\n",
      "TRAIN Batch 0600/1314, Loss  111.1943, NLL-Loss  111.1943\n",
      "TRAIN Batch 0650/1314, Loss  100.6520, NLL-Loss  100.6520\n",
      "TRAIN Batch 0700/1314, Loss  120.6143, NLL-Loss  120.6143\n",
      "TRAIN Batch 0750/1314, Loss  127.2382, NLL-Loss  127.2382\n",
      "TRAIN Batch 0800/1314, Loss  120.9407, NLL-Loss  120.9407\n",
      "TRAIN Batch 0850/1314, Loss  108.9573, NLL-Loss  108.9573\n",
      "TRAIN Batch 0900/1314, Loss  102.3122, NLL-Loss  102.3122\n",
      "TRAIN Batch 0950/1314, Loss  104.6960, NLL-Loss  104.6960\n",
      "TRAIN Batch 1000/1314, Loss  117.2449, NLL-Loss  117.2449\n",
      "TRAIN Batch 1050/1314, Loss  108.8965, NLL-Loss  108.8965\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN Batch 1100/1314, Loss  115.3468, NLL-Loss  115.3468\n",
      "TRAIN Batch 1150/1314, Loss   98.5968, NLL-Loss   98.5968\n",
      "TRAIN Batch 1200/1314, Loss  126.4633, NLL-Loss  126.4633\n",
      "TRAIN Batch 1250/1314, Loss  133.0255, NLL-Loss  133.0255\n",
      "TRAIN Batch 1300/1314, Loss  113.9447, NLL-Loss  113.9447\n",
      "TRAIN Batch 1314/1314, Loss  119.7797, NLL-Loss  119.7797\n",
      "TRAIN Epoch 25/300, Mean NLL  115.1056\n",
      "Model saved at bin/2019-May-17-21:02:23/E25.pytorch\n",
      "VALID Batch 0000/105, Loss  135.4937, NLL-Loss  135.4937\n",
      "VALID Batch 0050/105, Loss  133.9593, NLL-Loss  133.9593\n",
      "VALID Batch 0100/105, Loss  100.5029, NLL-Loss  100.5029\n",
      "VALID Batch 0105/105, Loss  102.1250, NLL-Loss  102.1250\n",
      "VALID Epoch 25/300, Mean NLL  115.3688\n",
      "TEST Batch 0000/117, Loss  108.3849, NLL-Loss  108.3849\n",
      "TEST Batch 0050/117, Loss  120.3427, NLL-Loss  120.3427\n",
      "TEST Batch 0100/117, Loss  140.9789, NLL-Loss  140.9789\n",
      "TEST Batch 0117/117, Loss  165.0296, NLL-Loss  165.0296\n",
      "TEST Epoch 25/300, Mean NLL  115.4618\n",
      "TRAIN Batch 0000/1314, Loss  102.4718, NLL-Loss  102.4718\n",
      "TRAIN Batch 0050/1314, Loss  103.6254, NLL-Loss  103.6254\n",
      "TRAIN Batch 0100/1314, Loss  126.1543, NLL-Loss  126.1543\n",
      "TRAIN Batch 0150/1314, Loss  113.8952, NLL-Loss  113.8952\n",
      "TRAIN Batch 0200/1314, Loss  106.6485, NLL-Loss  106.6485\n",
      "TRAIN Batch 0250/1314, Loss  114.6819, NLL-Loss  114.6819\n",
      "TRAIN Batch 0300/1314, Loss  113.7666, NLL-Loss  113.7666\n",
      "TRAIN Batch 0350/1314, Loss   98.1795, NLL-Loss   98.1795\n",
      "TRAIN Batch 0400/1314, Loss  113.1130, NLL-Loss  113.1130\n",
      "TRAIN Batch 0450/1314, Loss  133.2234, NLL-Loss  133.2234\n",
      "TRAIN Batch 0500/1314, Loss  142.3227, NLL-Loss  142.3227\n",
      "TRAIN Batch 0550/1314, Loss  103.4614, NLL-Loss  103.4614\n",
      "TRAIN Batch 0600/1314, Loss  115.4708, NLL-Loss  115.4708\n",
      "TRAIN Batch 0650/1314, Loss  122.4144, NLL-Loss  122.4144\n",
      "TRAIN Batch 0700/1314, Loss  114.5736, NLL-Loss  114.5736\n",
      "TRAIN Batch 0750/1314, Loss  110.4507, NLL-Loss  110.4507\n",
      "TRAIN Batch 0800/1314, Loss  124.6494, NLL-Loss  124.6494\n",
      "TRAIN Batch 0850/1314, Loss  121.1731, NLL-Loss  121.1731\n",
      "TRAIN Batch 0900/1314, Loss  119.8406, NLL-Loss  119.8406\n",
      "TRAIN Batch 0950/1314, Loss  112.9896, NLL-Loss  112.9896\n",
      "TRAIN Batch 1000/1314, Loss  109.1150, NLL-Loss  109.1150\n",
      "TRAIN Batch 1050/1314, Loss  118.6225, NLL-Loss  118.6225\n",
      "TRAIN Batch 1100/1314, Loss  113.3258, NLL-Loss  113.3258\n",
      "TRAIN Batch 1150/1314, Loss  127.2590, NLL-Loss  127.2590\n",
      "TRAIN Batch 1200/1314, Loss  109.3773, NLL-Loss  109.3773\n",
      "TRAIN Batch 1250/1314, Loss  113.3270, NLL-Loss  113.3270\n",
      "TRAIN Batch 1300/1314, Loss  111.8254, NLL-Loss  111.8254\n",
      "TRAIN Batch 1314/1314, Loss  110.7581, NLL-Loss  110.7581\n",
      "TRAIN Epoch 26/300, Mean NLL  114.7502\n",
      "Model saved at bin/2019-May-17-21:02:23/E26.pytorch\n",
      "VALID Batch 0000/105, Loss  135.3400, NLL-Loss  135.3400\n",
      "VALID Batch 0050/105, Loss  133.9070, NLL-Loss  133.9070\n",
      "VALID Batch 0100/105, Loss  100.5888, NLL-Loss  100.5888\n",
      "VALID Batch 0105/105, Loss  101.9893, NLL-Loss  101.9893\n",
      "VALID Epoch 26/300, Mean NLL  115.2245\n",
      "TEST Batch 0000/117, Loss  108.3085, NLL-Loss  108.3085\n",
      "TEST Batch 0050/117, Loss  120.5121, NLL-Loss  120.5121\n",
      "TEST Batch 0100/117, Loss  140.8145, NLL-Loss  140.8145\n",
      "TEST Batch 0117/117, Loss  164.4834, NLL-Loss  164.4834\n",
      "TEST Epoch 26/300, Mean NLL  115.3404\n",
      "TRAIN Batch 0000/1314, Loss  134.4746, NLL-Loss  134.4746\n",
      "TRAIN Batch 0050/1314, Loss  114.9031, NLL-Loss  114.9031\n",
      "TRAIN Batch 0100/1314, Loss  107.0053, NLL-Loss  107.0053\n",
      "TRAIN Batch 0150/1314, Loss  114.4712, NLL-Loss  114.4712\n",
      "TRAIN Batch 0200/1314, Loss  112.8637, NLL-Loss  112.8637\n",
      "TRAIN Batch 0250/1314, Loss  122.7777, NLL-Loss  122.7777\n",
      "TRAIN Batch 0300/1314, Loss  115.5457, NLL-Loss  115.5457\n",
      "TRAIN Batch 0350/1314, Loss  116.9142, NLL-Loss  116.9142\n",
      "TRAIN Batch 0400/1314, Loss  101.2187, NLL-Loss  101.2187\n",
      "TRAIN Batch 0450/1314, Loss  110.0169, NLL-Loss  110.0169\n",
      "TRAIN Batch 0500/1314, Loss  120.1512, NLL-Loss  120.1512\n",
      "TRAIN Batch 0550/1314, Loss  105.7510, NLL-Loss  105.7510\n",
      "TRAIN Batch 0600/1314, Loss  115.8381, NLL-Loss  115.8381\n",
      "TRAIN Batch 0650/1314, Loss  128.5374, NLL-Loss  128.5374\n",
      "TRAIN Batch 0700/1314, Loss  129.6608, NLL-Loss  129.6608\n",
      "TRAIN Batch 0750/1314, Loss  103.3787, NLL-Loss  103.3787\n",
      "TRAIN Batch 0800/1314, Loss  107.7132, NLL-Loss  107.7132\n",
      "TRAIN Batch 0850/1314, Loss  117.2983, NLL-Loss  117.2983\n",
      "TRAIN Batch 0900/1314, Loss  105.1263, NLL-Loss  105.1263\n",
      "TRAIN Batch 0950/1314, Loss  112.5511, NLL-Loss  112.5511\n",
      "TRAIN Batch 1000/1314, Loss  111.6573, NLL-Loss  111.6573\n",
      "TRAIN Batch 1050/1314, Loss  108.2557, NLL-Loss  108.2557\n",
      "TRAIN Batch 1100/1314, Loss  107.6778, NLL-Loss  107.6778\n",
      "TRAIN Batch 1150/1314, Loss   93.3664, NLL-Loss   93.3664\n",
      "TRAIN Batch 1200/1314, Loss  145.8230, NLL-Loss  145.8230\n",
      "TRAIN Batch 1250/1314, Loss  106.3235, NLL-Loss  106.3235\n",
      "TRAIN Batch 1300/1314, Loss  129.2843, NLL-Loss  129.2843\n",
      "TRAIN Batch 1314/1314, Loss  109.7191, NLL-Loss  109.7191\n",
      "TRAIN Epoch 27/300, Mean NLL  114.4072\n",
      "Model saved at bin/2019-May-17-21:02:23/E27.pytorch\n",
      "VALID Batch 0000/105, Loss  135.2733, NLL-Loss  135.2733\n",
      "VALID Batch 0050/105, Loss  133.3021, NLL-Loss  133.3021\n",
      "VALID Batch 0100/105, Loss  100.2951, NLL-Loss  100.2951\n",
      "VALID Batch 0105/105, Loss  101.6639, NLL-Loss  101.6639\n",
      "VALID Epoch 27/300, Mean NLL  114.9247\n",
      "TEST Batch 0000/117, Loss  107.9574, NLL-Loss  107.9574\n",
      "TEST Batch 0050/117, Loss  120.1556, NLL-Loss  120.1556\n",
      "TEST Batch 0100/117, Loss  140.6852, NLL-Loss  140.6852\n",
      "TEST Batch 0117/117, Loss  164.1932, NLL-Loss  164.1932\n",
      "TEST Epoch 27/300, Mean NLL  115.0242\n",
      "TRAIN Batch 0000/1314, Loss  100.2041, NLL-Loss  100.2041\n",
      "TRAIN Batch 0050/1314, Loss  112.7144, NLL-Loss  112.7144\n",
      "TRAIN Batch 0100/1314, Loss  128.7463, NLL-Loss  128.7463\n",
      "TRAIN Batch 0150/1314, Loss  114.5461, NLL-Loss  114.5461\n",
      "TRAIN Batch 0200/1314, Loss  112.2670, NLL-Loss  112.2670\n",
      "TRAIN Batch 0250/1314, Loss  119.7406, NLL-Loss  119.7406\n",
      "TRAIN Batch 0300/1314, Loss  140.4075, NLL-Loss  140.4075\n",
      "TRAIN Batch 0350/1314, Loss  117.9511, NLL-Loss  117.9511\n",
      "TRAIN Batch 0400/1314, Loss  136.3270, NLL-Loss  136.3270\n",
      "TRAIN Batch 0450/1314, Loss  128.5232, NLL-Loss  128.5232\n",
      "TRAIN Batch 0500/1314, Loss  108.6515, NLL-Loss  108.6515\n",
      "TRAIN Batch 0550/1314, Loss  132.2752, NLL-Loss  132.2752\n",
      "TRAIN Batch 0600/1314, Loss  106.4227, NLL-Loss  106.4227\n",
      "TRAIN Batch 0650/1314, Loss  106.6334, NLL-Loss  106.6334\n",
      "TRAIN Batch 0700/1314, Loss  119.8668, NLL-Loss  119.8668\n",
      "TRAIN Batch 0750/1314, Loss  119.7522, NLL-Loss  119.7522\n",
      "TRAIN Batch 0800/1314, Loss  123.2958, NLL-Loss  123.2958\n",
      "TRAIN Batch 0850/1314, Loss  112.6026, NLL-Loss  112.6026\n",
      "TRAIN Batch 0900/1314, Loss  111.4344, NLL-Loss  111.4344\n",
      "TRAIN Batch 0950/1314, Loss  107.3772, NLL-Loss  107.3772\n",
      "TRAIN Batch 1000/1314, Loss  118.7388, NLL-Loss  118.7388\n",
      "TRAIN Batch 1050/1314, Loss  109.3494, NLL-Loss  109.3494\n",
      "TRAIN Batch 1100/1314, Loss  129.7332, NLL-Loss  129.7332\n",
      "TRAIN Batch 1150/1314, Loss  119.2202, NLL-Loss  119.2202\n",
      "TRAIN Batch 1200/1314, Loss  101.5929, NLL-Loss  101.5929\n",
      "TRAIN Batch 1250/1314, Loss   95.2628, NLL-Loss   95.2628\n",
      "TRAIN Batch 1300/1314, Loss  104.6724, NLL-Loss  104.6724\n",
      "TRAIN Batch 1314/1314, Loss  128.5888, NLL-Loss  128.5888\n",
      "TRAIN Epoch 28/300, Mean NLL  114.0819\n",
      "Model saved at bin/2019-May-17-21:02:23/E28.pytorch\n",
      "VALID Batch 0000/105, Loss  135.2480, NLL-Loss  135.2480\n",
      "VALID Batch 0050/105, Loss  132.7944, NLL-Loss  132.7944\n",
      "VALID Batch 0100/105, Loss  100.1415, NLL-Loss  100.1415\n",
      "VALID Batch 0105/105, Loss  101.6970, NLL-Loss  101.6970\n",
      "VALID Epoch 28/300, Mean NLL  114.7604\n",
      "TEST Batch 0000/117, Loss  107.7220, NLL-Loss  107.7220\n",
      "TEST Batch 0050/117, Loss  119.9710, NLL-Loss  119.9710\n",
      "TEST Batch 0100/117, Loss  140.6503, NLL-Loss  140.6503\n",
      "TEST Batch 0117/117, Loss  164.5459, NLL-Loss  164.5459\n",
      "TEST Epoch 28/300, Mean NLL  114.8614\n",
      "TRAIN Batch 0000/1314, Loss  120.0070, NLL-Loss  120.0070\n",
      "TRAIN Batch 0050/1314, Loss  112.7620, NLL-Loss  112.7620\n",
      "TRAIN Batch 0100/1314, Loss  114.4216, NLL-Loss  114.4216\n",
      "TRAIN Batch 0150/1314, Loss  107.5151, NLL-Loss  107.5151\n",
      "TRAIN Batch 0200/1314, Loss  124.8656, NLL-Loss  124.8656\n",
      "TRAIN Batch 0250/1314, Loss  114.7326, NLL-Loss  114.7326\n",
      "TRAIN Batch 0300/1314, Loss  111.4285, NLL-Loss  111.4285\n",
      "TRAIN Batch 0350/1314, Loss  122.2071, NLL-Loss  122.2071\n",
      "TRAIN Batch 0400/1314, Loss  119.1249, NLL-Loss  119.1249\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN Batch 0450/1314, Loss  107.5157, NLL-Loss  107.5157\n",
      "TRAIN Batch 0500/1314, Loss  126.7063, NLL-Loss  126.7063\n",
      "TRAIN Batch 0550/1314, Loss  102.6280, NLL-Loss  102.6280\n",
      "TRAIN Batch 0600/1314, Loss  105.9093, NLL-Loss  105.9093\n",
      "TRAIN Batch 0650/1314, Loss  100.9657, NLL-Loss  100.9657\n",
      "TRAIN Batch 0700/1314, Loss   96.1833, NLL-Loss   96.1833\n",
      "TRAIN Batch 0750/1314, Loss   98.9761, NLL-Loss   98.9761\n",
      "TRAIN Batch 0800/1314, Loss   98.9984, NLL-Loss   98.9984\n",
      "TRAIN Batch 0850/1314, Loss  118.1948, NLL-Loss  118.1948\n",
      "TRAIN Batch 0900/1314, Loss  107.9532, NLL-Loss  107.9532\n",
      "TRAIN Batch 0950/1314, Loss  105.6759, NLL-Loss  105.6759\n",
      "TRAIN Batch 1000/1314, Loss  104.6410, NLL-Loss  104.6410\n",
      "TRAIN Batch 1050/1314, Loss  100.6100, NLL-Loss  100.6100\n",
      "TRAIN Batch 1100/1314, Loss  107.3100, NLL-Loss  107.3100\n",
      "TRAIN Batch 1150/1314, Loss  116.4635, NLL-Loss  116.4635\n",
      "TRAIN Batch 1200/1314, Loss  106.9529, NLL-Loss  106.9529\n",
      "TRAIN Batch 1250/1314, Loss  115.5518, NLL-Loss  115.5518\n",
      "TRAIN Batch 1300/1314, Loss  116.5808, NLL-Loss  116.5808\n",
      "TRAIN Batch 1314/1314, Loss  127.0673, NLL-Loss  127.0673\n",
      "TRAIN Epoch 29/300, Mean NLL  113.7606\n",
      "Model saved at bin/2019-May-17-21:02:23/E29.pytorch\n",
      "VALID Batch 0000/105, Loss  135.0546, NLL-Loss  135.0546\n",
      "VALID Batch 0050/105, Loss  132.5536, NLL-Loss  132.5536\n",
      "VALID Batch 0100/105, Loss  100.1628, NLL-Loss  100.1628\n",
      "VALID Batch 0105/105, Loss  101.3591, NLL-Loss  101.3591\n",
      "VALID Epoch 29/300, Mean NLL  114.5015\n",
      "TEST Batch 0000/117, Loss  107.5040, NLL-Loss  107.5040\n",
      "TEST Batch 0050/117, Loss  119.7613, NLL-Loss  119.7613\n",
      "TEST Batch 0100/117, Loss  140.3549, NLL-Loss  140.3549\n",
      "TEST Batch 0117/117, Loss  163.9669, NLL-Loss  163.9669\n",
      "TEST Epoch 29/300, Mean NLL  114.6155\n",
      "TRAIN Batch 0000/1314, Loss  106.7947, NLL-Loss  106.7947\n",
      "TRAIN Batch 0050/1314, Loss  111.3632, NLL-Loss  111.3632\n",
      "TRAIN Batch 0100/1314, Loss  122.3743, NLL-Loss  122.3743\n",
      "TRAIN Batch 0150/1314, Loss  102.7955, NLL-Loss  102.7955\n",
      "TRAIN Batch 0200/1314, Loss  120.1084, NLL-Loss  120.1084\n",
      "TRAIN Batch 0250/1314, Loss  123.3140, NLL-Loss  123.3140\n",
      "TRAIN Batch 0300/1314, Loss  110.9002, NLL-Loss  110.9002\n",
      "TRAIN Batch 0350/1314, Loss  114.8034, NLL-Loss  114.8034\n",
      "TRAIN Batch 0400/1314, Loss  123.5401, NLL-Loss  123.5401\n",
      "TRAIN Batch 0450/1314, Loss  106.7719, NLL-Loss  106.7719\n",
      "TRAIN Batch 0500/1314, Loss   99.3098, NLL-Loss   99.3098\n",
      "TRAIN Batch 0550/1314, Loss  122.5025, NLL-Loss  122.5025\n",
      "TRAIN Batch 0600/1314, Loss  122.9474, NLL-Loss  122.9474\n",
      "TRAIN Batch 0650/1314, Loss  110.8823, NLL-Loss  110.8823\n",
      "TRAIN Batch 0700/1314, Loss   90.6499, NLL-Loss   90.6499\n",
      "TRAIN Batch 0750/1314, Loss  108.9745, NLL-Loss  108.9745\n",
      "TRAIN Batch 0800/1314, Loss  100.7992, NLL-Loss  100.7992\n",
      "TRAIN Batch 0850/1314, Loss  107.1480, NLL-Loss  107.1480\n",
      "TRAIN Batch 0900/1314, Loss  118.8932, NLL-Loss  118.8932\n",
      "TRAIN Batch 0950/1314, Loss  125.5689, NLL-Loss  125.5689\n",
      "TRAIN Batch 1000/1314, Loss  105.7868, NLL-Loss  105.7868\n",
      "TRAIN Batch 1050/1314, Loss  122.1152, NLL-Loss  122.1152\n",
      "TRAIN Batch 1100/1314, Loss  110.5004, NLL-Loss  110.5004\n",
      "TRAIN Batch 1150/1314, Loss  104.3199, NLL-Loss  104.3199\n",
      "TRAIN Batch 1200/1314, Loss  100.9118, NLL-Loss  100.9118\n",
      "TRAIN Batch 1250/1314, Loss  103.8737, NLL-Loss  103.8737\n",
      "TRAIN Batch 1300/1314, Loss  122.1076, NLL-Loss  122.1076\n",
      "TRAIN Batch 1314/1314, Loss   99.2188, NLL-Loss   99.2188\n",
      "TRAIN Epoch 30/300, Mean NLL  113.4450\n",
      "Model saved at bin/2019-May-17-21:02:23/E30.pytorch\n",
      "VALID Batch 0000/105, Loss  135.0243, NLL-Loss  135.0243\n",
      "VALID Batch 0050/105, Loss  132.2096, NLL-Loss  132.2096\n",
      "VALID Batch 0100/105, Loss   99.9133, NLL-Loss   99.9133\n",
      "VALID Batch 0105/105, Loss  101.2867, NLL-Loss  101.2867\n",
      "VALID Epoch 30/300, Mean NLL  114.3095\n",
      "TEST Batch 0000/117, Loss  107.2445, NLL-Loss  107.2445\n",
      "TEST Batch 0050/117, Loss  119.4265, NLL-Loss  119.4265\n",
      "TEST Batch 0100/117, Loss  140.3989, NLL-Loss  140.3989\n",
      "TEST Batch 0117/117, Loss  164.3347, NLL-Loss  164.3347\n",
      "TEST Epoch 30/300, Mean NLL  114.4275\n",
      "TRAIN Batch 0000/1314, Loss  123.1444, NLL-Loss  123.1444\n",
      "TRAIN Batch 0050/1314, Loss  133.7372, NLL-Loss  133.7372\n",
      "TRAIN Batch 0100/1314, Loss  112.2260, NLL-Loss  112.2260\n",
      "TRAIN Batch 0150/1314, Loss   96.6805, NLL-Loss   96.6805\n",
      "TRAIN Batch 0200/1314, Loss  115.3348, NLL-Loss  115.3348\n",
      "TRAIN Batch 0250/1314, Loss  111.0485, NLL-Loss  111.0485\n",
      "TRAIN Batch 0300/1314, Loss  122.6309, NLL-Loss  122.6309\n",
      "TRAIN Batch 0350/1314, Loss   95.5869, NLL-Loss   95.5869\n",
      "TRAIN Batch 0400/1314, Loss  110.1692, NLL-Loss  110.1692\n",
      "TRAIN Batch 0450/1314, Loss  119.2553, NLL-Loss  119.2553\n",
      "TRAIN Batch 0500/1314, Loss  107.6744, NLL-Loss  107.6744\n",
      "TRAIN Batch 0550/1314, Loss  106.5601, NLL-Loss  106.5601\n",
      "TRAIN Batch 0600/1314, Loss  102.5614, NLL-Loss  102.5614\n",
      "TRAIN Batch 0650/1314, Loss  127.5462, NLL-Loss  127.5462\n",
      "TRAIN Batch 0700/1314, Loss  123.5656, NLL-Loss  123.5656\n",
      "TRAIN Batch 0750/1314, Loss  115.6898, NLL-Loss  115.6898\n",
      "TRAIN Batch 0800/1314, Loss  128.6070, NLL-Loss  128.6070\n",
      "TRAIN Batch 0850/1314, Loss  125.8787, NLL-Loss  125.8787\n",
      "TRAIN Batch 0900/1314, Loss  107.0227, NLL-Loss  107.0227\n",
      "TRAIN Batch 0950/1314, Loss   91.0802, NLL-Loss   91.0802\n",
      "TRAIN Batch 1000/1314, Loss  112.0303, NLL-Loss  112.0303\n",
      "TRAIN Batch 1050/1314, Loss  103.7455, NLL-Loss  103.7455\n",
      "TRAIN Batch 1100/1314, Loss   98.2933, NLL-Loss   98.2933\n",
      "TRAIN Batch 1150/1314, Loss  111.1639, NLL-Loss  111.1639\n",
      "TRAIN Batch 1200/1314, Loss  100.2110, NLL-Loss  100.2110\n",
      "TRAIN Batch 1250/1314, Loss  110.0128, NLL-Loss  110.0128\n",
      "TRAIN Batch 1300/1314, Loss  102.6215, NLL-Loss  102.6215\n",
      "TRAIN Batch 1314/1314, Loss  112.1727, NLL-Loss  112.1727\n",
      "TRAIN Epoch 31/300, Mean NLL  113.1432\n",
      "Model saved at bin/2019-May-17-21:02:23/E31.pytorch\n",
      "VALID Batch 0000/105, Loss  135.0395, NLL-Loss  135.0395\n",
      "VALID Batch 0050/105, Loss  132.0817, NLL-Loss  132.0817\n",
      "VALID Batch 0100/105, Loss   99.9276, NLL-Loss   99.9276\n",
      "VALID Batch 0105/105, Loss  101.2578, NLL-Loss  101.2578\n",
      "VALID Epoch 31/300, Mean NLL  114.2379\n",
      "TEST Batch 0000/117, Loss  107.0723, NLL-Loss  107.0723\n",
      "TEST Batch 0050/117, Loss  119.5291, NLL-Loss  119.5291\n",
      "TEST Batch 0100/117, Loss  140.3493, NLL-Loss  140.3493\n",
      "TEST Batch 0117/117, Loss  164.2742, NLL-Loss  164.2742\n",
      "TEST Epoch 31/300, Mean NLL  114.3360\n",
      "TRAIN Batch 0000/1314, Loss  108.7592, NLL-Loss  108.7592\n",
      "TRAIN Batch 0050/1314, Loss  115.6992, NLL-Loss  115.6992\n",
      "TRAIN Batch 0100/1314, Loss  119.4633, NLL-Loss  119.4633\n",
      "TRAIN Batch 0150/1314, Loss  120.3917, NLL-Loss  120.3917\n",
      "TRAIN Batch 0200/1314, Loss   98.8849, NLL-Loss   98.8849\n",
      "TRAIN Batch 0250/1314, Loss  107.6748, NLL-Loss  107.6748\n",
      "TRAIN Batch 0300/1314, Loss  103.1001, NLL-Loss  103.1001\n",
      "TRAIN Batch 0350/1314, Loss  128.0511, NLL-Loss  128.0511\n",
      "TRAIN Batch 0400/1314, Loss  104.7023, NLL-Loss  104.7023\n",
      "TRAIN Batch 0450/1314, Loss  113.8409, NLL-Loss  113.8409\n",
      "TRAIN Batch 0500/1314, Loss   96.1552, NLL-Loss   96.1552\n",
      "TRAIN Batch 0550/1314, Loss  110.7524, NLL-Loss  110.7524\n",
      "TRAIN Batch 0600/1314, Loss  132.4981, NLL-Loss  132.4981\n",
      "TRAIN Batch 0650/1314, Loss  116.3884, NLL-Loss  116.3884\n",
      "TRAIN Batch 0700/1314, Loss  114.0631, NLL-Loss  114.0631\n",
      "TRAIN Batch 0750/1314, Loss  126.6602, NLL-Loss  126.6602\n",
      "TRAIN Batch 0800/1314, Loss  124.8339, NLL-Loss  124.8339\n",
      "TRAIN Batch 0850/1314, Loss   92.6589, NLL-Loss   92.6589\n",
      "TRAIN Batch 0900/1314, Loss  106.8885, NLL-Loss  106.8885\n",
      "TRAIN Batch 0950/1314, Loss  120.2063, NLL-Loss  120.2063\n",
      "TRAIN Batch 1000/1314, Loss  102.4352, NLL-Loss  102.4352\n",
      "TRAIN Batch 1050/1314, Loss  128.5343, NLL-Loss  128.5343\n",
      "TRAIN Batch 1100/1314, Loss  116.9382, NLL-Loss  116.9382\n",
      "TRAIN Batch 1150/1314, Loss  110.7749, NLL-Loss  110.7749\n",
      "TRAIN Batch 1200/1314, Loss  122.2064, NLL-Loss  122.2064\n",
      "TRAIN Batch 1250/1314, Loss  116.0740, NLL-Loss  116.0740\n",
      "TRAIN Batch 1300/1314, Loss  118.5414, NLL-Loss  118.5414\n",
      "TRAIN Batch 1314/1314, Loss  115.1300, NLL-Loss  115.1300\n",
      "TRAIN Epoch 32/300, Mean NLL  112.8654\n",
      "Model saved at bin/2019-May-17-21:02:23/E32.pytorch\n",
      "VALID Batch 0000/105, Loss  134.7632, NLL-Loss  134.7632\n",
      "VALID Batch 0050/105, Loss  131.7333, NLL-Loss  131.7333\n",
      "VALID Batch 0100/105, Loss   99.7719, NLL-Loss   99.7719\n",
      "VALID Batch 0105/105, Loss  101.0826, NLL-Loss  101.0826\n",
      "VALID Epoch 32/300, Mean NLL  113.9873\n",
      "TEST Batch 0000/117, Loss  106.7232, NLL-Loss  106.7232\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST Batch 0050/117, Loss  119.2997, NLL-Loss  119.2997\n",
      "TEST Batch 0100/117, Loss  139.9532, NLL-Loss  139.9532\n",
      "TEST Batch 0117/117, Loss  163.8170, NLL-Loss  163.8171\n",
      "TEST Epoch 32/300, Mean NLL  114.0765\n",
      "TRAIN Batch 0000/1314, Loss  113.0245, NLL-Loss  113.0245\n",
      "TRAIN Batch 0050/1314, Loss  106.0034, NLL-Loss  106.0034\n",
      "TRAIN Batch 0100/1314, Loss  108.1510, NLL-Loss  108.1510\n",
      "TRAIN Batch 0150/1314, Loss  129.3698, NLL-Loss  129.3698\n",
      "TRAIN Batch 0200/1314, Loss   99.0285, NLL-Loss   99.0285\n",
      "TRAIN Batch 0250/1314, Loss  112.7633, NLL-Loss  112.7633\n",
      "TRAIN Batch 0300/1314, Loss   97.7674, NLL-Loss   97.7674\n",
      "TRAIN Batch 0350/1314, Loss  111.9044, NLL-Loss  111.9044\n",
      "TRAIN Batch 0400/1314, Loss  132.7551, NLL-Loss  132.7551\n",
      "TRAIN Batch 0450/1314, Loss  119.9467, NLL-Loss  119.9467\n",
      "TRAIN Batch 0500/1314, Loss  117.4536, NLL-Loss  117.4536\n",
      "TRAIN Batch 0550/1314, Loss  102.3136, NLL-Loss  102.3136\n",
      "TRAIN Batch 0600/1314, Loss  110.8245, NLL-Loss  110.8245\n",
      "TRAIN Batch 0650/1314, Loss  110.3964, NLL-Loss  110.3964\n",
      "TRAIN Batch 0700/1314, Loss  127.5867, NLL-Loss  127.5867\n",
      "TRAIN Batch 0750/1314, Loss  105.4859, NLL-Loss  105.4859\n",
      "TRAIN Batch 0800/1314, Loss  114.0658, NLL-Loss  114.0658\n",
      "TRAIN Batch 0850/1314, Loss  100.5034, NLL-Loss  100.5034\n",
      "TRAIN Batch 0900/1314, Loss  111.7808, NLL-Loss  111.7808\n",
      "TRAIN Batch 0950/1314, Loss  108.4571, NLL-Loss  108.4571\n",
      "TRAIN Batch 1000/1314, Loss  112.1022, NLL-Loss  112.1022\n",
      "TRAIN Batch 1050/1314, Loss  123.8677, NLL-Loss  123.8677\n",
      "TRAIN Batch 1100/1314, Loss  116.4383, NLL-Loss  116.4383\n",
      "TRAIN Batch 1150/1314, Loss  111.0718, NLL-Loss  111.0718\n",
      "TRAIN Batch 1200/1314, Loss  109.6470, NLL-Loss  109.6470\n",
      "TRAIN Batch 1250/1314, Loss  125.9809, NLL-Loss  125.9809\n",
      "TRAIN Batch 1300/1314, Loss  112.8831, NLL-Loss  112.8831\n",
      "TRAIN Batch 1314/1314, Loss  140.6564, NLL-Loss  140.6564\n",
      "TRAIN Epoch 33/300, Mean NLL  112.5664\n",
      "Model saved at bin/2019-May-17-21:02:23/E33.pytorch\n",
      "VALID Batch 0000/105, Loss  134.6856, NLL-Loss  134.6856\n",
      "VALID Batch 0050/105, Loss  131.3876, NLL-Loss  131.3876\n",
      "VALID Batch 0100/105, Loss   99.7105, NLL-Loss   99.7105\n",
      "VALID Batch 0105/105, Loss  100.9267, NLL-Loss  100.9267\n",
      "VALID Epoch 33/300, Mean NLL  113.8046\n",
      "TEST Batch 0000/117, Loss  106.5987, NLL-Loss  106.5987\n",
      "TEST Batch 0050/117, Loss  119.1558, NLL-Loss  119.1558\n",
      "TEST Batch 0100/117, Loss  139.9767, NLL-Loss  139.9767\n",
      "TEST Batch 0117/117, Loss  163.8825, NLL-Loss  163.8825\n",
      "TEST Epoch 33/300, Mean NLL  113.9083\n",
      "TRAIN Batch 0000/1314, Loss  118.5084, NLL-Loss  118.5084\n",
      "TRAIN Batch 0050/1314, Loss  114.9993, NLL-Loss  114.9993\n",
      "TRAIN Batch 0100/1314, Loss  104.5203, NLL-Loss  104.5203\n",
      "TRAIN Batch 0150/1314, Loss   90.8174, NLL-Loss   90.8174\n",
      "TRAIN Batch 0200/1314, Loss  102.6514, NLL-Loss  102.6514\n",
      "TRAIN Batch 0250/1314, Loss  106.4684, NLL-Loss  106.4684\n",
      "TRAIN Batch 0300/1314, Loss  133.0609, NLL-Loss  133.0609\n",
      "TRAIN Batch 0350/1314, Loss  110.0459, NLL-Loss  110.0459\n",
      "TRAIN Batch 0400/1314, Loss  114.0699, NLL-Loss  114.0699\n",
      "TRAIN Batch 0450/1314, Loss  117.4706, NLL-Loss  117.4706\n",
      "TRAIN Batch 0500/1314, Loss  114.9174, NLL-Loss  114.9174\n",
      "TRAIN Batch 0550/1314, Loss  119.8384, NLL-Loss  119.8384\n",
      "TRAIN Batch 0600/1314, Loss  110.6696, NLL-Loss  110.6696\n",
      "TRAIN Batch 0650/1314, Loss   97.1271, NLL-Loss   97.1271\n",
      "TRAIN Batch 0700/1314, Loss  116.0333, NLL-Loss  116.0333\n",
      "TRAIN Batch 0750/1314, Loss   97.0810, NLL-Loss   97.0810\n",
      "TRAIN Batch 0800/1314, Loss  119.0077, NLL-Loss  119.0077\n",
      "TRAIN Batch 0850/1314, Loss  114.7577, NLL-Loss  114.7577\n",
      "TRAIN Batch 0900/1314, Loss  102.1514, NLL-Loss  102.1514\n",
      "TRAIN Batch 0950/1314, Loss   98.6786, NLL-Loss   98.6786\n",
      "TRAIN Batch 1000/1314, Loss  108.4339, NLL-Loss  108.4339\n",
      "TRAIN Batch 1050/1314, Loss  106.8506, NLL-Loss  106.8506\n",
      "TRAIN Batch 1100/1314, Loss  116.3977, NLL-Loss  116.3977\n",
      "TRAIN Batch 1150/1314, Loss  110.2593, NLL-Loss  110.2593\n",
      "TRAIN Batch 1200/1314, Loss  124.3337, NLL-Loss  124.3337\n",
      "TRAIN Batch 1250/1314, Loss  122.0216, NLL-Loss  122.0216\n",
      "TRAIN Batch 1300/1314, Loss  119.9671, NLL-Loss  119.9671\n",
      "TRAIN Batch 1314/1314, Loss  111.3022, NLL-Loss  111.3022\n",
      "TRAIN Epoch 34/300, Mean NLL  112.3297\n",
      "Model saved at bin/2019-May-17-21:02:23/E34.pytorch\n",
      "VALID Batch 0000/105, Loss  134.5290, NLL-Loss  134.5290\n",
      "VALID Batch 0050/105, Loss  131.4161, NLL-Loss  131.4161\n",
      "VALID Batch 0100/105, Loss   99.7400, NLL-Loss   99.7400\n",
      "VALID Batch 0105/105, Loss  100.8218, NLL-Loss  100.8218\n",
      "VALID Epoch 34/300, Mean NLL  113.6758\n",
      "TEST Batch 0000/117, Loss  106.4647, NLL-Loss  106.4647\n",
      "TEST Batch 0050/117, Loss  119.0212, NLL-Loss  119.0212\n",
      "TEST Batch 0100/117, Loss  139.6521, NLL-Loss  139.6521\n",
      "TEST Batch 0117/117, Loss  163.3007, NLL-Loss  163.3007\n",
      "TEST Epoch 34/300, Mean NLL  113.7719\n",
      "TRAIN Batch 0000/1314, Loss  113.8681, NLL-Loss  113.8681\n",
      "TRAIN Batch 0050/1314, Loss  125.8068, NLL-Loss  125.8068\n",
      "TRAIN Batch 0100/1314, Loss   98.7641, NLL-Loss   98.7641\n",
      "TRAIN Batch 0150/1314, Loss  101.5216, NLL-Loss  101.5216\n",
      "TRAIN Batch 0200/1314, Loss  120.7618, NLL-Loss  120.7618\n",
      "TRAIN Batch 0250/1314, Loss  107.3894, NLL-Loss  107.3894\n",
      "TRAIN Batch 0300/1314, Loss   97.8546, NLL-Loss   97.8546\n",
      "TRAIN Batch 0350/1314, Loss  119.6454, NLL-Loss  119.6454\n",
      "TRAIN Batch 0400/1314, Loss  121.3673, NLL-Loss  121.3673\n",
      "TRAIN Batch 0450/1314, Loss  135.8298, NLL-Loss  135.8298\n",
      "TRAIN Batch 0500/1314, Loss   96.9466, NLL-Loss   96.9466\n",
      "TRAIN Batch 0550/1314, Loss  107.4701, NLL-Loss  107.4701\n",
      "TRAIN Batch 0600/1314, Loss  107.0181, NLL-Loss  107.0181\n",
      "TRAIN Batch 0650/1314, Loss  108.6971, NLL-Loss  108.6971\n",
      "TRAIN Batch 0700/1314, Loss  118.1925, NLL-Loss  118.1925\n",
      "TRAIN Batch 0750/1314, Loss  110.0905, NLL-Loss  110.0905\n",
      "TRAIN Batch 0800/1314, Loss  117.4105, NLL-Loss  117.4105\n",
      "TRAIN Batch 0850/1314, Loss  107.1816, NLL-Loss  107.1816\n",
      "TRAIN Batch 0900/1314, Loss  114.5239, NLL-Loss  114.5239\n",
      "TRAIN Batch 0950/1314, Loss  112.7384, NLL-Loss  112.7384\n",
      "TRAIN Batch 1000/1314, Loss   97.7414, NLL-Loss   97.7414\n",
      "TRAIN Batch 1050/1314, Loss  118.1509, NLL-Loss  118.1509\n",
      "TRAIN Batch 1100/1314, Loss  105.8828, NLL-Loss  105.8828\n",
      "TRAIN Batch 1150/1314, Loss  105.9901, NLL-Loss  105.9901\n",
      "TRAIN Batch 1200/1314, Loss  128.6864, NLL-Loss  128.6864\n",
      "TRAIN Batch 1250/1314, Loss  110.1999, NLL-Loss  110.1999\n",
      "TRAIN Batch 1300/1314, Loss  120.1106, NLL-Loss  120.1106\n",
      "TRAIN Batch 1314/1314, Loss   89.2538, NLL-Loss   89.2538\n",
      "TRAIN Epoch 35/300, Mean NLL  112.0177\n",
      "Model saved at bin/2019-May-17-21:02:23/E35.pytorch\n",
      "VALID Batch 0000/105, Loss  134.5307, NLL-Loss  134.5307\n",
      "VALID Batch 0050/105, Loss  130.9982, NLL-Loss  130.9982\n",
      "VALID Batch 0100/105, Loss   99.4682, NLL-Loss   99.4682\n",
      "VALID Batch 0105/105, Loss  100.5169, NLL-Loss  100.5169\n",
      "VALID Epoch 35/300, Mean NLL  113.4845\n",
      "TEST Batch 0000/117, Loss  106.1925, NLL-Loss  106.1925\n",
      "TEST Batch 0050/117, Loss  118.9081, NLL-Loss  118.9081\n",
      "TEST Batch 0100/117, Loss  139.5702, NLL-Loss  139.5702\n",
      "TEST Batch 0117/117, Loss  163.5565, NLL-Loss  163.5565\n",
      "TEST Epoch 35/300, Mean NLL  113.5822\n",
      "TRAIN Batch 0000/1314, Loss  121.4852, NLL-Loss  121.4852\n",
      "TRAIN Batch 0050/1314, Loss  111.9641, NLL-Loss  111.9641\n",
      "TRAIN Batch 0100/1314, Loss  111.3680, NLL-Loss  111.3680\n",
      "TRAIN Batch 0150/1314, Loss  112.2891, NLL-Loss  112.2891\n",
      "TRAIN Batch 0200/1314, Loss  116.3542, NLL-Loss  116.3542\n",
      "TRAIN Batch 0250/1314, Loss  117.9312, NLL-Loss  117.9312\n",
      "TRAIN Batch 0300/1314, Loss  125.2457, NLL-Loss  125.2457\n",
      "TRAIN Batch 0350/1314, Loss  109.4468, NLL-Loss  109.4468\n",
      "TRAIN Batch 0400/1314, Loss  116.2378, NLL-Loss  116.2378\n",
      "TRAIN Batch 0450/1314, Loss  128.2872, NLL-Loss  128.2872\n",
      "TRAIN Batch 0500/1314, Loss  109.1805, NLL-Loss  109.1805\n",
      "TRAIN Batch 0550/1314, Loss  129.2086, NLL-Loss  129.2086\n",
      "TRAIN Batch 0600/1314, Loss   84.4446, NLL-Loss   84.4446\n",
      "TRAIN Batch 0650/1314, Loss  107.4432, NLL-Loss  107.4432\n",
      "TRAIN Batch 0700/1314, Loss  124.3281, NLL-Loss  124.3281\n",
      "TRAIN Batch 0750/1314, Loss  127.2445, NLL-Loss  127.2445\n",
      "TRAIN Batch 0800/1314, Loss  101.6881, NLL-Loss  101.6881\n",
      "TRAIN Batch 0850/1314, Loss  131.6095, NLL-Loss  131.6095\n",
      "TRAIN Batch 0900/1314, Loss  134.8690, NLL-Loss  134.8690\n",
      "TRAIN Batch 0950/1314, Loss  113.0566, NLL-Loss  113.0566\n",
      "TRAIN Batch 1000/1314, Loss  112.4900, NLL-Loss  112.4900\n",
      "TRAIN Batch 1050/1314, Loss  111.9986, NLL-Loss  111.9986\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN Batch 1100/1314, Loss  122.6397, NLL-Loss  122.6397\n",
      "TRAIN Batch 1150/1314, Loss  125.9713, NLL-Loss  125.9713\n",
      "TRAIN Batch 1200/1314, Loss  117.9833, NLL-Loss  117.9833\n",
      "TRAIN Batch 1250/1314, Loss  110.5518, NLL-Loss  110.5518\n",
      "TRAIN Batch 1300/1314, Loss  110.7735, NLL-Loss  110.7735\n",
      "TRAIN Batch 1314/1314, Loss  111.6012, NLL-Loss  111.6012\n",
      "TRAIN Epoch 36/300, Mean NLL  111.7699\n",
      "Model saved at bin/2019-May-17-21:02:23/E36.pytorch\n",
      "VALID Batch 0000/105, Loss  134.4489, NLL-Loss  134.4489\n",
      "VALID Batch 0050/105, Loss  130.8025, NLL-Loss  130.8025\n",
      "VALID Batch 0100/105, Loss   99.6273, NLL-Loss   99.6273\n",
      "VALID Batch 0105/105, Loss  100.6532, NLL-Loss  100.6532\n",
      "VALID Epoch 36/300, Mean NLL  113.4063\n",
      "TEST Batch 0000/117, Loss  106.3004, NLL-Loss  106.3004\n",
      "TEST Batch 0050/117, Loss  118.8857, NLL-Loss  118.8857\n",
      "TEST Batch 0100/117, Loss  139.5228, NLL-Loss  139.5228\n",
      "TEST Batch 0117/117, Loss  163.2896, NLL-Loss  163.2896\n",
      "TEST Epoch 36/300, Mean NLL  113.5269\n",
      "TRAIN Batch 0000/1314, Loss  117.1835, NLL-Loss  117.1835\n",
      "TRAIN Batch 0050/1314, Loss  122.5998, NLL-Loss  122.5998\n",
      "TRAIN Batch 0100/1314, Loss  121.8050, NLL-Loss  121.8050\n",
      "TRAIN Batch 0150/1314, Loss  131.6113, NLL-Loss  131.6113\n",
      "TRAIN Batch 0200/1314, Loss   97.7045, NLL-Loss   97.7045\n",
      "TRAIN Batch 0250/1314, Loss  114.4825, NLL-Loss  114.4825\n",
      "TRAIN Batch 0300/1314, Loss  103.7742, NLL-Loss  103.7742\n",
      "TRAIN Batch 0350/1314, Loss  103.7084, NLL-Loss  103.7084\n",
      "TRAIN Batch 0400/1314, Loss  100.3420, NLL-Loss  100.3420\n",
      "TRAIN Batch 0450/1314, Loss  107.2694, NLL-Loss  107.2694\n",
      "TRAIN Batch 0500/1314, Loss  106.0270, NLL-Loss  106.0270\n",
      "TRAIN Batch 0550/1314, Loss   94.7677, NLL-Loss   94.7677\n",
      "TRAIN Batch 0600/1314, Loss  101.1791, NLL-Loss  101.1791\n",
      "TRAIN Batch 0650/1314, Loss  111.2655, NLL-Loss  111.2655\n",
      "TRAIN Batch 0700/1314, Loss  112.7024, NLL-Loss  112.7024\n",
      "TRAIN Batch 0750/1314, Loss   97.2580, NLL-Loss   97.2580\n",
      "TRAIN Batch 0800/1314, Loss  126.1639, NLL-Loss  126.1639\n",
      "TRAIN Batch 0850/1314, Loss  103.1588, NLL-Loss  103.1588\n",
      "TRAIN Batch 0900/1314, Loss  117.2178, NLL-Loss  117.2178\n",
      "TRAIN Batch 0950/1314, Loss   97.4672, NLL-Loss   97.4672\n",
      "TRAIN Batch 1000/1314, Loss  120.5960, NLL-Loss  120.5960\n",
      "TRAIN Batch 1050/1314, Loss  109.5840, NLL-Loss  109.5840\n",
      "TRAIN Batch 1100/1314, Loss  105.4303, NLL-Loss  105.4303\n",
      "TRAIN Batch 1150/1314, Loss  121.6051, NLL-Loss  121.6051\n",
      "TRAIN Batch 1200/1314, Loss  109.2525, NLL-Loss  109.2525\n",
      "TRAIN Batch 1250/1314, Loss  114.6190, NLL-Loss  114.6190\n",
      "TRAIN Batch 1300/1314, Loss  108.5839, NLL-Loss  108.5839\n",
      "TRAIN Batch 1314/1314, Loss  102.8309, NLL-Loss  102.8309\n",
      "TRAIN Epoch 37/300, Mean NLL  111.5112\n",
      "Model saved at bin/2019-May-17-21:02:23/E37.pytorch\n",
      "VALID Batch 0000/105, Loss  134.4980, NLL-Loss  134.4980\n",
      "VALID Batch 0050/105, Loss  130.6051, NLL-Loss  130.6051\n",
      "VALID Batch 0100/105, Loss   99.2849, NLL-Loss   99.2849\n",
      "VALID Batch 0105/105, Loss  100.6990, NLL-Loss  100.6990\n",
      "VALID Epoch 37/300, Mean NLL  113.2680\n",
      "TEST Batch 0000/117, Loss  106.0313, NLL-Loss  106.0313\n",
      "TEST Batch 0050/117, Loss  118.6437, NLL-Loss  118.6437\n",
      "TEST Batch 0100/117, Loss  139.7809, NLL-Loss  139.7809\n",
      "TEST Batch 0117/117, Loss  163.8412, NLL-Loss  163.8412\n",
      "TEST Epoch 37/300, Mean NLL  113.3739\n",
      "TRAIN Batch 0000/1314, Loss  105.5613, NLL-Loss  105.5613\n",
      "TRAIN Batch 0050/1314, Loss   92.3818, NLL-Loss   92.3818\n",
      "TRAIN Batch 0100/1314, Loss  110.3073, NLL-Loss  110.3073\n",
      "TRAIN Batch 0150/1314, Loss  106.1319, NLL-Loss  106.1319\n",
      "TRAIN Batch 0200/1314, Loss  123.1158, NLL-Loss  123.1158\n",
      "TRAIN Batch 0250/1314, Loss  119.3182, NLL-Loss  119.3182\n",
      "TRAIN Batch 0300/1314, Loss  111.0393, NLL-Loss  111.0393\n",
      "TRAIN Batch 0350/1314, Loss  120.5156, NLL-Loss  120.5156\n",
      "TRAIN Batch 0400/1314, Loss  111.2849, NLL-Loss  111.2849\n",
      "TRAIN Batch 0450/1314, Loss  121.7338, NLL-Loss  121.7338\n",
      "TRAIN Batch 0500/1314, Loss  115.2212, NLL-Loss  115.2212\n",
      "TRAIN Batch 0550/1314, Loss  103.6584, NLL-Loss  103.6584\n",
      "TRAIN Batch 0600/1314, Loss  126.0420, NLL-Loss  126.0420\n",
      "TRAIN Batch 0650/1314, Loss  124.0130, NLL-Loss  124.0130\n",
      "TRAIN Batch 0700/1314, Loss   93.9020, NLL-Loss   93.9020\n",
      "TRAIN Batch 0750/1314, Loss  120.8713, NLL-Loss  120.8713\n",
      "TRAIN Batch 0800/1314, Loss  115.3144, NLL-Loss  115.3144\n",
      "TRAIN Batch 0850/1314, Loss  101.3671, NLL-Loss  101.3671\n",
      "TRAIN Batch 0900/1314, Loss  119.2649, NLL-Loss  119.2649\n",
      "TRAIN Batch 0950/1314, Loss  128.6232, NLL-Loss  128.6232\n",
      "TRAIN Batch 1000/1314, Loss  128.1238, NLL-Loss  128.1238\n",
      "TRAIN Batch 1050/1314, Loss  116.1008, NLL-Loss  116.1008\n",
      "TRAIN Batch 1100/1314, Loss  108.2786, NLL-Loss  108.2786\n",
      "TRAIN Batch 1150/1314, Loss  121.0481, NLL-Loss  121.0481\n",
      "TRAIN Batch 1200/1314, Loss  108.5699, NLL-Loss  108.5699\n",
      "TRAIN Batch 1250/1314, Loss  114.9915, NLL-Loss  114.9915\n",
      "TRAIN Batch 1300/1314, Loss  124.5100, NLL-Loss  124.5100\n",
      "TRAIN Batch 1314/1314, Loss  117.3649, NLL-Loss  117.3649\n",
      "TRAIN Epoch 38/300, Mean NLL  111.2811\n",
      "Model saved at bin/2019-May-17-21:02:23/E38.pytorch\n",
      "VALID Batch 0000/105, Loss  134.2936, NLL-Loss  134.2936\n",
      "VALID Batch 0050/105, Loss  130.2753, NLL-Loss  130.2753\n",
      "VALID Batch 0100/105, Loss   99.2574, NLL-Loss   99.2574\n",
      "VALID Batch 0105/105, Loss  100.4784, NLL-Loss  100.4784\n",
      "VALID Epoch 38/300, Mean NLL  113.0532\n",
      "TEST Batch 0000/117, Loss  105.7575, NLL-Loss  105.7575\n",
      "TEST Batch 0050/117, Loss  118.5516, NLL-Loss  118.5516\n",
      "TEST Batch 0100/117, Loss  139.3293, NLL-Loss  139.3293\n",
      "TEST Batch 0117/117, Loss  163.2708, NLL-Loss  163.2708\n",
      "TEST Epoch 38/300, Mean NLL  113.1386\n",
      "TRAIN Batch 0000/1314, Loss  104.7738, NLL-Loss  104.7738\n",
      "TRAIN Batch 0050/1314, Loss  115.4982, NLL-Loss  115.4982\n",
      "TRAIN Batch 0100/1314, Loss  112.3099, NLL-Loss  112.3099\n",
      "TRAIN Batch 0150/1314, Loss  112.4671, NLL-Loss  112.4671\n",
      "TRAIN Batch 0200/1314, Loss  119.3994, NLL-Loss  119.3994\n",
      "TRAIN Batch 0250/1314, Loss  102.8460, NLL-Loss  102.8460\n",
      "TRAIN Batch 0300/1314, Loss   94.8895, NLL-Loss   94.8895\n",
      "TRAIN Batch 0350/1314, Loss  123.4831, NLL-Loss  123.4831\n",
      "TRAIN Batch 0400/1314, Loss  112.5156, NLL-Loss  112.5156\n",
      "TRAIN Batch 0450/1314, Loss   98.3677, NLL-Loss   98.3677\n",
      "TRAIN Batch 0500/1314, Loss  112.4679, NLL-Loss  112.4679\n",
      "TRAIN Batch 0550/1314, Loss  117.2044, NLL-Loss  117.2044\n",
      "TRAIN Batch 0600/1314, Loss  115.1065, NLL-Loss  115.1065\n",
      "TRAIN Batch 0650/1314, Loss  125.2868, NLL-Loss  125.2868\n",
      "TRAIN Batch 0700/1314, Loss   99.1249, NLL-Loss   99.1249\n",
      "TRAIN Batch 0750/1314, Loss  108.4312, NLL-Loss  108.4312\n",
      "TRAIN Batch 0800/1314, Loss  105.7852, NLL-Loss  105.7852\n",
      "TRAIN Batch 0850/1314, Loss  125.6305, NLL-Loss  125.6305\n",
      "TRAIN Batch 0900/1314, Loss  108.3706, NLL-Loss  108.3706\n",
      "TRAIN Batch 0950/1314, Loss  120.9301, NLL-Loss  120.9301\n",
      "TRAIN Batch 1000/1314, Loss  100.5198, NLL-Loss  100.5198\n",
      "TRAIN Batch 1050/1314, Loss  115.7447, NLL-Loss  115.7447\n",
      "TRAIN Batch 1100/1314, Loss  122.6263, NLL-Loss  122.6263\n",
      "TRAIN Batch 1150/1314, Loss  102.1125, NLL-Loss  102.1125\n",
      "TRAIN Batch 1200/1314, Loss  115.1802, NLL-Loss  115.1802\n",
      "TRAIN Batch 1250/1314, Loss  135.4645, NLL-Loss  135.4645\n",
      "TRAIN Batch 1300/1314, Loss  111.7044, NLL-Loss  111.7044\n",
      "TRAIN Batch 1314/1314, Loss  111.6658, NLL-Loss  111.6658\n",
      "TRAIN Epoch 39/300, Mean NLL  111.0360\n",
      "Model saved at bin/2019-May-17-21:02:23/E39.pytorch\n",
      "VALID Batch 0000/105, Loss  134.3177, NLL-Loss  134.3177\n",
      "VALID Batch 0050/105, Loss  130.2812, NLL-Loss  130.2812\n",
      "VALID Batch 0100/105, Loss   99.3000, NLL-Loss   99.3000\n",
      "VALID Batch 0105/105, Loss  100.5462, NLL-Loss  100.5462\n",
      "VALID Epoch 39/300, Mean NLL  113.0276\n",
      "TEST Batch 0000/117, Loss  105.6406, NLL-Loss  105.6406\n",
      "TEST Batch 0050/117, Loss  118.6280, NLL-Loss  118.6280\n",
      "TEST Batch 0100/117, Loss  139.5339, NLL-Loss  139.5339\n",
      "TEST Batch 0117/117, Loss  163.2339, NLL-Loss  163.2339\n",
      "TEST Epoch 39/300, Mean NLL  113.1461\n",
      "TRAIN Batch 0000/1314, Loss  107.2637, NLL-Loss  107.2637\n",
      "TRAIN Batch 0050/1314, Loss  108.7364, NLL-Loss  108.7364\n",
      "TRAIN Batch 0100/1314, Loss  109.1198, NLL-Loss  109.1198\n",
      "TRAIN Batch 0150/1314, Loss   94.5612, NLL-Loss   94.5612\n",
      "TRAIN Batch 0200/1314, Loss  112.0027, NLL-Loss  112.0027\n",
      "TRAIN Batch 0250/1314, Loss  107.7710, NLL-Loss  107.7710\n",
      "TRAIN Batch 0300/1314, Loss   98.5685, NLL-Loss   98.5685\n",
      "TRAIN Batch 0350/1314, Loss  119.9145, NLL-Loss  119.9145\n",
      "TRAIN Batch 0400/1314, Loss  136.4113, NLL-Loss  136.4113\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN Batch 0450/1314, Loss  102.9853, NLL-Loss  102.9853\n",
      "TRAIN Batch 0500/1314, Loss  110.7612, NLL-Loss  110.7612\n",
      "TRAIN Batch 0550/1314, Loss  102.4655, NLL-Loss  102.4655\n",
      "TRAIN Batch 0600/1314, Loss   82.1282, NLL-Loss   82.1282\n",
      "TRAIN Batch 0650/1314, Loss  118.5046, NLL-Loss  118.5046\n",
      "TRAIN Batch 0700/1314, Loss  123.5781, NLL-Loss  123.5781\n",
      "TRAIN Batch 0750/1314, Loss   99.4898, NLL-Loss   99.4898\n",
      "TRAIN Batch 0800/1314, Loss  109.4634, NLL-Loss  109.4634\n",
      "TRAIN Batch 0850/1314, Loss  104.1416, NLL-Loss  104.1416\n",
      "TRAIN Batch 0900/1314, Loss  105.7687, NLL-Loss  105.7687\n",
      "TRAIN Batch 0950/1314, Loss  131.2250, NLL-Loss  131.2250\n",
      "TRAIN Batch 1000/1314, Loss  104.7455, NLL-Loss  104.7455\n",
      "TRAIN Batch 1050/1314, Loss  105.4375, NLL-Loss  105.4375\n",
      "TRAIN Batch 1100/1314, Loss  103.8007, NLL-Loss  103.8007\n",
      "TRAIN Batch 1150/1314, Loss  122.1118, NLL-Loss  122.1118\n",
      "TRAIN Batch 1200/1314, Loss  108.0841, NLL-Loss  108.0841\n",
      "TRAIN Batch 1250/1314, Loss  111.7294, NLL-Loss  111.7294\n",
      "TRAIN Batch 1300/1314, Loss  120.0673, NLL-Loss  120.0673\n",
      "TRAIN Batch 1314/1314, Loss  103.7520, NLL-Loss  103.7520\n",
      "TRAIN Epoch 40/300, Mean NLL  110.7972\n",
      "Model saved at bin/2019-May-17-21:02:23/E40.pytorch\n",
      "VALID Batch 0000/105, Loss  134.1743, NLL-Loss  134.1743\n",
      "VALID Batch 0050/105, Loss  130.0531, NLL-Loss  130.0531\n",
      "VALID Batch 0100/105, Loss   99.3055, NLL-Loss   99.3055\n",
      "VALID Batch 0105/105, Loss  100.1798, NLL-Loss  100.1798\n",
      "VALID Epoch 40/300, Mean NLL  112.8956\n",
      "TEST Batch 0000/117, Loss  105.5948, NLL-Loss  105.5948\n",
      "TEST Batch 0050/117, Loss  118.5373, NLL-Loss  118.5373\n",
      "TEST Batch 0100/117, Loss  139.2060, NLL-Loss  139.2060\n",
      "TEST Batch 0117/117, Loss  163.2939, NLL-Loss  163.2939\n",
      "TEST Epoch 40/300, Mean NLL  112.9989\n",
      "TRAIN Batch 0000/1314, Loss  106.3160, NLL-Loss  106.3160\n",
      "TRAIN Batch 0050/1314, Loss  106.6668, NLL-Loss  106.6668\n",
      "TRAIN Batch 0100/1314, Loss  126.2949, NLL-Loss  126.2949\n",
      "TRAIN Batch 0150/1314, Loss  115.8238, NLL-Loss  115.8238\n",
      "TRAIN Batch 0200/1314, Loss  108.6526, NLL-Loss  108.6526\n",
      "TRAIN Batch 0250/1314, Loss  115.6575, NLL-Loss  115.6575\n",
      "TRAIN Batch 0300/1314, Loss  112.8317, NLL-Loss  112.8317\n",
      "TRAIN Batch 0350/1314, Loss   92.5058, NLL-Loss   92.5058\n",
      "TRAIN Batch 0400/1314, Loss  117.6767, NLL-Loss  117.6767\n",
      "TRAIN Batch 0450/1314, Loss  116.0774, NLL-Loss  116.0774\n",
      "TRAIN Batch 0500/1314, Loss  123.1818, NLL-Loss  123.1818\n",
      "TRAIN Batch 0550/1314, Loss  103.4490, NLL-Loss  103.4490\n",
      "TRAIN Batch 0600/1314, Loss  116.9772, NLL-Loss  116.9772\n",
      "TRAIN Batch 0650/1314, Loss  111.4234, NLL-Loss  111.4234\n",
      "TRAIN Batch 0700/1314, Loss  105.8825, NLL-Loss  105.8825\n",
      "TRAIN Batch 0750/1314, Loss   91.5854, NLL-Loss   91.5854\n",
      "TRAIN Batch 0800/1314, Loss  107.7518, NLL-Loss  107.7518\n",
      "TRAIN Batch 0850/1314, Loss  131.7140, NLL-Loss  131.7140\n",
      "TRAIN Batch 0900/1314, Loss  108.7750, NLL-Loss  108.7750\n",
      "TRAIN Batch 0950/1314, Loss  107.3217, NLL-Loss  107.3217\n",
      "TRAIN Batch 1000/1314, Loss  111.4704, NLL-Loss  111.4704\n",
      "TRAIN Batch 1050/1314, Loss   86.3789, NLL-Loss   86.3789\n",
      "TRAIN Batch 1100/1314, Loss  125.8127, NLL-Loss  125.8127\n",
      "TRAIN Batch 1150/1314, Loss  127.3717, NLL-Loss  127.3717\n",
      "TRAIN Batch 1200/1314, Loss  118.3641, NLL-Loss  118.3641\n",
      "TRAIN Batch 1250/1314, Loss  102.0458, NLL-Loss  102.0458\n",
      "TRAIN Batch 1300/1314, Loss  120.3088, NLL-Loss  120.3088\n",
      "TRAIN Batch 1314/1314, Loss  121.3173, NLL-Loss  121.3173\n",
      "TRAIN Epoch 41/300, Mean NLL  110.5698\n",
      "Model saved at bin/2019-May-17-21:02:23/E41.pytorch\n",
      "VALID Batch 0000/105, Loss  134.0563, NLL-Loss  134.0563\n",
      "VALID Batch 0050/105, Loss  129.6304, NLL-Loss  129.6304\n",
      "VALID Batch 0100/105, Loss   99.0963, NLL-Loss   99.0963\n",
      "VALID Batch 0105/105, Loss  100.2341, NLL-Loss  100.2341\n",
      "VALID Epoch 41/300, Mean NLL  112.6631\n",
      "TEST Batch 0000/117, Loss  105.1526, NLL-Loss  105.1526\n",
      "TEST Batch 0050/117, Loss  118.1968, NLL-Loss  118.1968\n",
      "TEST Batch 0100/117, Loss  139.3321, NLL-Loss  139.3321\n",
      "TEST Batch 0117/117, Loss  162.9869, NLL-Loss  162.9869\n",
      "TEST Epoch 41/300, Mean NLL  112.7790\n",
      "TRAIN Batch 0000/1314, Loss  105.3089, NLL-Loss  105.3089\n",
      "TRAIN Batch 0050/1314, Loss  107.8457, NLL-Loss  107.8457\n",
      "TRAIN Batch 0100/1314, Loss  110.9778, NLL-Loss  110.9778\n",
      "TRAIN Batch 0150/1314, Loss  110.9098, NLL-Loss  110.9098\n",
      "TRAIN Batch 0200/1314, Loss   90.0626, NLL-Loss   90.0626\n",
      "TRAIN Batch 0250/1314, Loss  103.2037, NLL-Loss  103.2037\n",
      "TRAIN Batch 0300/1314, Loss  108.6231, NLL-Loss  108.6231\n",
      "TRAIN Batch 0350/1314, Loss   95.3880, NLL-Loss   95.3880\n",
      "TRAIN Batch 0400/1314, Loss  112.7384, NLL-Loss  112.7384\n",
      "TRAIN Batch 0450/1314, Loss  107.7843, NLL-Loss  107.7843\n",
      "TRAIN Batch 0500/1314, Loss  110.3569, NLL-Loss  110.3569\n",
      "TRAIN Batch 0550/1314, Loss   99.3526, NLL-Loss   99.3526\n",
      "TRAIN Batch 0600/1314, Loss  107.1777, NLL-Loss  107.1777\n",
      "TRAIN Batch 0650/1314, Loss  108.5262, NLL-Loss  108.5262\n",
      "TRAIN Batch 0700/1314, Loss  107.6530, NLL-Loss  107.6530\n",
      "TRAIN Batch 0750/1314, Loss  132.1797, NLL-Loss  132.1797\n",
      "TRAIN Batch 0800/1314, Loss  106.5661, NLL-Loss  106.5661\n",
      "TRAIN Batch 0850/1314, Loss  123.1936, NLL-Loss  123.1936\n",
      "TRAIN Batch 0900/1314, Loss  104.5144, NLL-Loss  104.5144\n",
      "TRAIN Batch 0950/1314, Loss  105.4144, NLL-Loss  105.4144\n",
      "TRAIN Batch 1000/1314, Loss  123.1194, NLL-Loss  123.1194\n",
      "TRAIN Batch 1050/1314, Loss  104.8447, NLL-Loss  104.8447\n",
      "TRAIN Batch 1100/1314, Loss  106.1072, NLL-Loss  106.1072\n",
      "TRAIN Batch 1150/1314, Loss  120.8936, NLL-Loss  120.8936\n",
      "TRAIN Batch 1200/1314, Loss  108.9818, NLL-Loss  108.9818\n",
      "TRAIN Batch 1250/1314, Loss   99.8755, NLL-Loss   99.8755\n",
      "TRAIN Batch 1300/1314, Loss   96.5181, NLL-Loss   96.5181\n",
      "TRAIN Batch 1314/1314, Loss  106.1277, NLL-Loss  106.1277\n",
      "TRAIN Epoch 42/300, Mean NLL  110.3244\n",
      "Model saved at bin/2019-May-17-21:02:23/E42.pytorch\n",
      "VALID Batch 0000/105, Loss  134.0129, NLL-Loss  134.0129\n",
      "VALID Batch 0050/105, Loss  129.5726, NLL-Loss  129.5726\n",
      "VALID Batch 0100/105, Loss   99.0167, NLL-Loss   99.0167\n",
      "VALID Batch 0105/105, Loss  100.1784, NLL-Loss  100.1784\n",
      "VALID Epoch 42/300, Mean NLL  112.5897\n",
      "TEST Batch 0000/117, Loss  105.1355, NLL-Loss  105.1355\n",
      "TEST Batch 0050/117, Loss  118.1372, NLL-Loss  118.1372\n",
      "TEST Batch 0100/117, Loss  139.2449, NLL-Loss  139.2449\n",
      "TEST Batch 0117/117, Loss  163.0682, NLL-Loss  163.0682\n",
      "TEST Epoch 42/300, Mean NLL  112.7098\n",
      "TRAIN Batch 0000/1314, Loss  115.1155, NLL-Loss  115.1155\n",
      "TRAIN Batch 0050/1314, Loss  118.8261, NLL-Loss  118.8261\n",
      "TRAIN Batch 0100/1314, Loss  118.1421, NLL-Loss  118.1421\n",
      "TRAIN Batch 0150/1314, Loss  115.6312, NLL-Loss  115.6312\n",
      "TRAIN Batch 0200/1314, Loss  119.7020, NLL-Loss  119.7020\n",
      "TRAIN Batch 0250/1314, Loss  107.4898, NLL-Loss  107.4898\n",
      "TRAIN Batch 0300/1314, Loss  109.6371, NLL-Loss  109.6371\n",
      "TRAIN Batch 0350/1314, Loss   98.2276, NLL-Loss   98.2276\n",
      "TRAIN Batch 0400/1314, Loss   94.5592, NLL-Loss   94.5592\n",
      "TRAIN Batch 0450/1314, Loss  107.6620, NLL-Loss  107.6620\n",
      "TRAIN Batch 0500/1314, Loss  112.8418, NLL-Loss  112.8418\n",
      "TRAIN Batch 0550/1314, Loss  124.9947, NLL-Loss  124.9947\n",
      "TRAIN Batch 0600/1314, Loss  100.1716, NLL-Loss  100.1716\n",
      "TRAIN Batch 0650/1314, Loss  106.0082, NLL-Loss  106.0082\n",
      "TRAIN Batch 0700/1314, Loss  108.8491, NLL-Loss  108.8491\n",
      "TRAIN Batch 0750/1314, Loss  100.8112, NLL-Loss  100.8112\n",
      "TRAIN Batch 0800/1314, Loss  101.3343, NLL-Loss  101.3343\n",
      "TRAIN Batch 0850/1314, Loss  113.8553, NLL-Loss  113.8553\n",
      "TRAIN Batch 0900/1314, Loss   97.5163, NLL-Loss   97.5163\n",
      "TRAIN Batch 0950/1314, Loss  122.9725, NLL-Loss  122.9725\n",
      "TRAIN Batch 1000/1314, Loss  106.6572, NLL-Loss  106.6572\n",
      "TRAIN Batch 1050/1314, Loss  113.9434, NLL-Loss  113.9434\n",
      "TRAIN Batch 1100/1314, Loss  123.2394, NLL-Loss  123.2394\n",
      "TRAIN Batch 1150/1314, Loss  124.7424, NLL-Loss  124.7424\n",
      "TRAIN Batch 1200/1314, Loss  120.3038, NLL-Loss  120.3038\n",
      "TRAIN Batch 1250/1314, Loss  128.0871, NLL-Loss  128.0871\n",
      "TRAIN Batch 1300/1314, Loss  110.2521, NLL-Loss  110.2521\n",
      "TRAIN Batch 1314/1314, Loss  116.7708, NLL-Loss  116.7708\n",
      "TRAIN Epoch 43/300, Mean NLL  110.0963\n",
      "Model saved at bin/2019-May-17-21:02:23/E43.pytorch\n",
      "VALID Batch 0000/105, Loss  134.0368, NLL-Loss  134.0368\n",
      "VALID Batch 0050/105, Loss  129.2927, NLL-Loss  129.2927\n",
      "VALID Batch 0100/105, Loss   98.8271, NLL-Loss   98.8271\n",
      "VALID Batch 0105/105, Loss   99.9423, NLL-Loss   99.9423\n",
      "VALID Epoch 43/300, Mean NLL  112.3920\n",
      "TEST Batch 0000/117, Loss  104.8170, NLL-Loss  104.8170\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST Batch 0050/117, Loss  118.2317, NLL-Loss  118.2317\n",
      "TEST Batch 0100/117, Loss  138.9655, NLL-Loss  138.9655\n",
      "TEST Batch 0117/117, Loss  162.9810, NLL-Loss  162.9810\n",
      "TEST Epoch 43/300, Mean NLL  112.5296\n",
      "TRAIN Batch 0000/1314, Loss  122.9687, NLL-Loss  122.9687\n",
      "TRAIN Batch 0050/1314, Loss  107.1976, NLL-Loss  107.1976\n",
      "TRAIN Batch 0100/1314, Loss   98.6152, NLL-Loss   98.6152\n",
      "TRAIN Batch 0150/1314, Loss  124.4517, NLL-Loss  124.4517\n",
      "TRAIN Batch 0200/1314, Loss  110.5934, NLL-Loss  110.5934\n",
      "TRAIN Batch 0250/1314, Loss  124.4293, NLL-Loss  124.4293\n",
      "TRAIN Batch 0300/1314, Loss  108.0295, NLL-Loss  108.0295\n",
      "TRAIN Batch 0350/1314, Loss  110.5703, NLL-Loss  110.5703\n",
      "TRAIN Batch 0400/1314, Loss  113.8275, NLL-Loss  113.8275\n",
      "TRAIN Batch 0450/1314, Loss  108.1880, NLL-Loss  108.1880\n",
      "TRAIN Batch 0500/1314, Loss  109.4866, NLL-Loss  109.4866\n",
      "TRAIN Batch 0550/1314, Loss  112.3010, NLL-Loss  112.3010\n",
      "TRAIN Batch 0600/1314, Loss  107.3412, NLL-Loss  107.3412\n",
      "TRAIN Batch 0650/1314, Loss   98.2545, NLL-Loss   98.2545\n",
      "TRAIN Batch 0700/1314, Loss  117.6896, NLL-Loss  117.6896\n",
      "TRAIN Batch 0750/1314, Loss  104.7024, NLL-Loss  104.7024\n",
      "TRAIN Batch 0800/1314, Loss  124.8961, NLL-Loss  124.8961\n",
      "TRAIN Batch 0850/1314, Loss   88.0618, NLL-Loss   88.0618\n",
      "TRAIN Batch 0900/1314, Loss  104.6828, NLL-Loss  104.6828\n",
      "TRAIN Batch 0950/1314, Loss  102.5707, NLL-Loss  102.5707\n",
      "TRAIN Batch 1000/1314, Loss  117.6069, NLL-Loss  117.6069\n",
      "TRAIN Batch 1050/1314, Loss  111.1647, NLL-Loss  111.1647\n",
      "TRAIN Batch 1100/1314, Loss  103.0775, NLL-Loss  103.0775\n",
      "TRAIN Batch 1150/1314, Loss  114.6632, NLL-Loss  114.6632\n",
      "TRAIN Batch 1200/1314, Loss   99.4309, NLL-Loss   99.4309\n",
      "TRAIN Batch 1250/1314, Loss   93.2597, NLL-Loss   93.2597\n",
      "TRAIN Batch 1300/1314, Loss  126.3500, NLL-Loss  126.3500\n",
      "TRAIN Batch 1314/1314, Loss  100.9168, NLL-Loss  100.9168\n",
      "TRAIN Epoch 44/300, Mean NLL  109.8840\n",
      "Model saved at bin/2019-May-17-21:02:23/E44.pytorch\n",
      "VALID Batch 0000/105, Loss  133.8955, NLL-Loss  133.8955\n",
      "VALID Batch 0050/105, Loss  128.7858, NLL-Loss  128.7858\n",
      "VALID Batch 0100/105, Loss   98.7044, NLL-Loss   98.7044\n",
      "VALID Batch 0105/105, Loss   99.9546, NLL-Loss   99.9546\n",
      "VALID Epoch 44/300, Mean NLL  112.2631\n",
      "TEST Batch 0000/117, Loss  104.7091, NLL-Loss  104.7091\n",
      "TEST Batch 0050/117, Loss  117.8472, NLL-Loss  117.8472\n",
      "TEST Batch 0100/117, Loss  138.9727, NLL-Loss  138.9727\n",
      "TEST Batch 0117/117, Loss  163.0813, NLL-Loss  163.0813\n",
      "TEST Epoch 44/300, Mean NLL  112.3570\n",
      "TRAIN Batch 0000/1314, Loss   88.4864, NLL-Loss   88.4864\n",
      "TRAIN Batch 0050/1314, Loss  103.1079, NLL-Loss  103.1079\n",
      "TRAIN Batch 0100/1314, Loss  114.4543, NLL-Loss  114.4543\n",
      "TRAIN Batch 0150/1314, Loss  112.8310, NLL-Loss  112.8310\n",
      "TRAIN Batch 0200/1314, Loss  115.5050, NLL-Loss  115.5050\n",
      "TRAIN Batch 0250/1314, Loss  103.9415, NLL-Loss  103.9415\n",
      "TRAIN Batch 0300/1314, Loss  114.7501, NLL-Loss  114.7501\n",
      "TRAIN Batch 0350/1314, Loss  102.1100, NLL-Loss  102.1100\n",
      "TRAIN Batch 0400/1314, Loss  107.6728, NLL-Loss  107.6728\n",
      "TRAIN Batch 0450/1314, Loss  121.1221, NLL-Loss  121.1221\n",
      "TRAIN Batch 0500/1314, Loss  109.3489, NLL-Loss  109.3489\n",
      "TRAIN Batch 0550/1314, Loss  110.6321, NLL-Loss  110.6321\n",
      "TRAIN Batch 0600/1314, Loss  103.6374, NLL-Loss  103.6374\n",
      "TRAIN Batch 0650/1314, Loss   85.0593, NLL-Loss   85.0593\n",
      "TRAIN Batch 0700/1314, Loss  103.6785, NLL-Loss  103.6785\n",
      "TRAIN Batch 0750/1314, Loss  101.8662, NLL-Loss  101.8662\n",
      "TRAIN Batch 0800/1314, Loss  111.6100, NLL-Loss  111.6100\n",
      "TRAIN Batch 0850/1314, Loss  111.4243, NLL-Loss  111.4243\n",
      "TRAIN Batch 0900/1314, Loss  133.1082, NLL-Loss  133.1082\n",
      "TRAIN Batch 0950/1314, Loss  119.6309, NLL-Loss  119.6309\n",
      "TRAIN Batch 1000/1314, Loss  101.6060, NLL-Loss  101.6060\n",
      "TRAIN Batch 1050/1314, Loss  113.3040, NLL-Loss  113.3040\n",
      "TRAIN Batch 1100/1314, Loss  110.3501, NLL-Loss  110.3501\n",
      "TRAIN Batch 1150/1314, Loss  111.1625, NLL-Loss  111.1625\n",
      "TRAIN Batch 1200/1314, Loss  127.8125, NLL-Loss  127.8125\n",
      "TRAIN Batch 1250/1314, Loss  123.3852, NLL-Loss  123.3852\n",
      "TRAIN Batch 1300/1314, Loss  117.0055, NLL-Loss  117.0055\n",
      "TRAIN Batch 1314/1314, Loss  105.2740, NLL-Loss  105.2740\n",
      "TRAIN Epoch 45/300, Mean NLL  109.6627\n",
      "Model saved at bin/2019-May-17-21:02:23/E45.pytorch\n",
      "VALID Batch 0000/105, Loss  133.9947, NLL-Loss  133.9947\n",
      "VALID Batch 0050/105, Loss  128.8517, NLL-Loss  128.8517\n",
      "VALID Batch 0100/105, Loss   98.7713, NLL-Loss   98.7713\n",
      "VALID Batch 0105/105, Loss   99.7465, NLL-Loss   99.7465\n",
      "VALID Epoch 45/300, Mean NLL  112.2540\n",
      "TEST Batch 0000/117, Loss  104.6648, NLL-Loss  104.6648\n",
      "TEST Batch 0050/117, Loss  118.0242, NLL-Loss  118.0242\n",
      "TEST Batch 0100/117, Loss  138.8589, NLL-Loss  138.8589\n",
      "TEST Batch 0117/117, Loss  162.9998, NLL-Loss  162.9998\n",
      "TEST Epoch 45/300, Mean NLL  112.3606\n",
      "TRAIN Batch 0000/1314, Loss  113.6785, NLL-Loss  113.6785\n",
      "TRAIN Batch 0050/1314, Loss  114.5260, NLL-Loss  114.5260\n",
      "TRAIN Batch 0100/1314, Loss  108.5042, NLL-Loss  108.5042\n",
      "TRAIN Batch 0150/1314, Loss  100.5536, NLL-Loss  100.5536\n",
      "TRAIN Batch 0200/1314, Loss  110.1981, NLL-Loss  110.1981\n",
      "TRAIN Batch 0250/1314, Loss  104.2808, NLL-Loss  104.2808\n",
      "TRAIN Batch 0300/1314, Loss  111.9912, NLL-Loss  111.9912\n",
      "TRAIN Batch 0350/1314, Loss  103.6528, NLL-Loss  103.6528\n",
      "TRAIN Batch 0400/1314, Loss   99.8048, NLL-Loss   99.8048\n",
      "TRAIN Batch 0450/1314, Loss  123.6503, NLL-Loss  123.6503\n",
      "TRAIN Batch 0500/1314, Loss  122.0619, NLL-Loss  122.0619\n",
      "TRAIN Batch 0550/1314, Loss  112.4485, NLL-Loss  112.4485\n",
      "TRAIN Batch 0600/1314, Loss  109.8671, NLL-Loss  109.8671\n",
      "TRAIN Batch 0650/1314, Loss  115.7960, NLL-Loss  115.7960\n",
      "TRAIN Batch 0700/1314, Loss  102.8417, NLL-Loss  102.8417\n",
      "TRAIN Batch 0750/1314, Loss  108.2067, NLL-Loss  108.2067\n",
      "TRAIN Batch 0800/1314, Loss  129.9773, NLL-Loss  129.9773\n",
      "TRAIN Batch 0850/1314, Loss  113.6171, NLL-Loss  113.6171\n",
      "TRAIN Batch 0900/1314, Loss  110.1874, NLL-Loss  110.1874\n",
      "TRAIN Batch 0950/1314, Loss  120.0006, NLL-Loss  120.0006\n",
      "TRAIN Batch 1000/1314, Loss  111.3641, NLL-Loss  111.3641\n",
      "TRAIN Batch 1050/1314, Loss  102.4363, NLL-Loss  102.4363\n",
      "TRAIN Batch 1100/1314, Loss  110.3860, NLL-Loss  110.3860\n",
      "TRAIN Batch 1150/1314, Loss   97.2015, NLL-Loss   97.2015\n",
      "TRAIN Batch 1200/1314, Loss   85.7369, NLL-Loss   85.7369\n",
      "TRAIN Batch 1250/1314, Loss  113.6211, NLL-Loss  113.6211\n",
      "TRAIN Batch 1300/1314, Loss  118.0277, NLL-Loss  118.0277\n",
      "TRAIN Batch 1314/1314, Loss  108.9420, NLL-Loss  108.9420\n",
      "TRAIN Epoch 46/300, Mean NLL  109.4758\n",
      "Model saved at bin/2019-May-17-21:02:23/E46.pytorch\n",
      "VALID Batch 0000/105, Loss  133.8726, NLL-Loss  133.8726\n",
      "VALID Batch 0050/105, Loss  128.6738, NLL-Loss  128.6738\n",
      "VALID Batch 0100/105, Loss   98.7303, NLL-Loss   98.7303\n",
      "VALID Batch 0105/105, Loss   99.9628, NLL-Loss   99.9628\n",
      "VALID Epoch 46/300, Mean NLL  112.1628\n",
      "TEST Batch 0000/117, Loss  104.5844, NLL-Loss  104.5844\n",
      "TEST Batch 0050/117, Loss  117.7888, NLL-Loss  117.7888\n",
      "TEST Batch 0100/117, Loss  139.0511, NLL-Loss  139.0511\n",
      "TEST Batch 0117/117, Loss  162.9380, NLL-Loss  162.9380\n",
      "TEST Epoch 46/300, Mean NLL  112.2759\n",
      "TRAIN Batch 0000/1314, Loss  103.2388, NLL-Loss  103.2388\n",
      "TRAIN Batch 0050/1314, Loss  110.3214, NLL-Loss  110.3214\n",
      "TRAIN Batch 0100/1314, Loss   98.9406, NLL-Loss   98.9406\n",
      "TRAIN Batch 0150/1314, Loss  126.1584, NLL-Loss  126.1584\n",
      "TRAIN Batch 0200/1314, Loss  118.2303, NLL-Loss  118.2303\n",
      "TRAIN Batch 0250/1314, Loss  121.1981, NLL-Loss  121.1981\n",
      "TRAIN Batch 0300/1314, Loss  125.3272, NLL-Loss  125.3272\n",
      "TRAIN Batch 0350/1314, Loss  106.5018, NLL-Loss  106.5018\n",
      "TRAIN Batch 0400/1314, Loss  105.1670, NLL-Loss  105.1670\n",
      "TRAIN Batch 0450/1314, Loss  110.0063, NLL-Loss  110.0063\n",
      "TRAIN Batch 0500/1314, Loss  100.9522, NLL-Loss  100.9522\n",
      "TRAIN Batch 0550/1314, Loss  100.9154, NLL-Loss  100.9154\n",
      "TRAIN Batch 0600/1314, Loss   97.7849, NLL-Loss   97.7849\n",
      "TRAIN Batch 0650/1314, Loss  103.5189, NLL-Loss  103.5189\n",
      "TRAIN Batch 0700/1314, Loss  114.7094, NLL-Loss  114.7094\n",
      "TRAIN Batch 0750/1314, Loss  108.7863, NLL-Loss  108.7863\n",
      "TRAIN Batch 0800/1314, Loss   95.0628, NLL-Loss   95.0628\n",
      "TRAIN Batch 0850/1314, Loss  133.1856, NLL-Loss  133.1856\n",
      "TRAIN Batch 0900/1314, Loss  103.4448, NLL-Loss  103.4448\n",
      "TRAIN Batch 0950/1314, Loss  104.6143, NLL-Loss  104.6143\n",
      "TRAIN Batch 1000/1314, Loss  100.2141, NLL-Loss  100.2141\n",
      "TRAIN Batch 1050/1314, Loss  109.2385, NLL-Loss  109.2385\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN Batch 1100/1314, Loss  118.3768, NLL-Loss  118.3768\n",
      "TRAIN Batch 1150/1314, Loss  106.0827, NLL-Loss  106.0827\n",
      "TRAIN Batch 1200/1314, Loss  113.1753, NLL-Loss  113.1753\n",
      "TRAIN Batch 1250/1314, Loss  118.5812, NLL-Loss  118.5812\n",
      "TRAIN Batch 1300/1314, Loss  109.2264, NLL-Loss  109.2264\n",
      "TRAIN Batch 1314/1314, Loss  117.5819, NLL-Loss  117.5819\n",
      "TRAIN Epoch 47/300, Mean NLL  109.2527\n",
      "Model saved at bin/2019-May-17-21:02:23/E47.pytorch\n",
      "VALID Batch 0000/105, Loss  133.8186, NLL-Loss  133.8186\n",
      "VALID Batch 0050/105, Loss  128.1802, NLL-Loss  128.1802\n",
      "VALID Batch 0100/105, Loss   98.5042, NLL-Loss   98.5042\n",
      "VALID Batch 0105/105, Loss   99.9486, NLL-Loss   99.9486\n",
      "VALID Epoch 47/300, Mean NLL  111.9718\n",
      "TEST Batch 0000/117, Loss  104.3280, NLL-Loss  104.3280\n",
      "TEST Batch 0050/117, Loss  117.9312, NLL-Loss  117.9312\n",
      "TEST Batch 0100/117, Loss  138.9014, NLL-Loss  138.9014\n",
      "TEST Batch 0117/117, Loss  162.8579, NLL-Loss  162.8579\n",
      "TEST Epoch 47/300, Mean NLL  112.1059\n",
      "TRAIN Batch 0000/1314, Loss  121.9529, NLL-Loss  121.9529\n",
      "TRAIN Batch 0050/1314, Loss  108.8768, NLL-Loss  108.8768\n",
      "TRAIN Batch 0100/1314, Loss  116.0865, NLL-Loss  116.0865\n",
      "TRAIN Batch 0150/1314, Loss  105.3768, NLL-Loss  105.3768\n",
      "TRAIN Batch 0200/1314, Loss   97.0728, NLL-Loss   97.0728\n",
      "TRAIN Batch 0250/1314, Loss   91.8346, NLL-Loss   91.8346\n",
      "TRAIN Batch 0300/1314, Loss   92.2280, NLL-Loss   92.2280\n",
      "TRAIN Batch 0350/1314, Loss  120.5888, NLL-Loss  120.5888\n",
      "TRAIN Batch 0400/1314, Loss  102.1763, NLL-Loss  102.1763\n",
      "TRAIN Batch 0450/1314, Loss  104.7079, NLL-Loss  104.7079\n",
      "TRAIN Batch 0500/1314, Loss   93.6847, NLL-Loss   93.6847\n",
      "TRAIN Batch 0550/1314, Loss  102.4107, NLL-Loss  102.4107\n",
      "TRAIN Batch 0600/1314, Loss  119.3826, NLL-Loss  119.3826\n",
      "TRAIN Batch 0650/1314, Loss   98.7455, NLL-Loss   98.7455\n",
      "TRAIN Batch 0700/1314, Loss  111.2552, NLL-Loss  111.2552\n",
      "TRAIN Batch 0750/1314, Loss  121.2301, NLL-Loss  121.2301\n",
      "TRAIN Batch 0800/1314, Loss  122.7492, NLL-Loss  122.7492\n",
      "TRAIN Batch 0850/1314, Loss  103.9085, NLL-Loss  103.9085\n",
      "TRAIN Batch 0900/1314, Loss  120.5993, NLL-Loss  120.5993\n",
      "TRAIN Batch 0950/1314, Loss  102.4712, NLL-Loss  102.4712\n",
      "TRAIN Batch 1000/1314, Loss  122.7171, NLL-Loss  122.7171\n",
      "TRAIN Batch 1050/1314, Loss  107.3335, NLL-Loss  107.3335\n",
      "TRAIN Batch 1100/1314, Loss  114.2192, NLL-Loss  114.2192\n",
      "TRAIN Batch 1150/1314, Loss  104.8526, NLL-Loss  104.8526\n",
      "TRAIN Batch 1200/1314, Loss  111.1065, NLL-Loss  111.1065\n",
      "TRAIN Batch 1250/1314, Loss  108.5399, NLL-Loss  108.5399\n",
      "TRAIN Batch 1300/1314, Loss  104.3752, NLL-Loss  104.3752\n",
      "TRAIN Batch 1314/1314, Loss   94.2468, NLL-Loss   94.2468\n",
      "TRAIN Epoch 48/300, Mean NLL  109.0614\n",
      "Model saved at bin/2019-May-17-21:02:23/E48.pytorch\n",
      "VALID Batch 0000/105, Loss  133.6806, NLL-Loss  133.6806\n",
      "VALID Batch 0050/105, Loss  128.1606, NLL-Loss  128.1606\n",
      "VALID Batch 0100/105, Loss   98.6272, NLL-Loss   98.6272\n",
      "VALID Batch 0105/105, Loss   99.7225, NLL-Loss   99.7225\n",
      "VALID Epoch 48/300, Mean NLL  111.8742\n",
      "TEST Batch 0000/117, Loss  104.1991, NLL-Loss  104.1991\n",
      "TEST Batch 0050/117, Loss  117.7597, NLL-Loss  117.7597\n",
      "TEST Batch 0100/117, Loss  138.6042, NLL-Loss  138.6042\n",
      "TEST Batch 0117/117, Loss  162.5156, NLL-Loss  162.5156\n",
      "TEST Epoch 48/300, Mean NLL  111.9812\n",
      "TRAIN Batch 0000/1314, Loss  120.7581, NLL-Loss  120.7581\n",
      "TRAIN Batch 0050/1314, Loss   97.8496, NLL-Loss   97.8496\n",
      "TRAIN Batch 0100/1314, Loss  106.1402, NLL-Loss  106.1402\n",
      "TRAIN Batch 0150/1314, Loss   94.1567, NLL-Loss   94.1567\n",
      "TRAIN Batch 0200/1314, Loss  107.4984, NLL-Loss  107.4984\n",
      "TRAIN Batch 0250/1314, Loss   81.8530, NLL-Loss   81.8530\n",
      "TRAIN Batch 0300/1314, Loss  103.9582, NLL-Loss  103.9582\n",
      "TRAIN Batch 0350/1314, Loss  107.7681, NLL-Loss  107.7681\n",
      "TRAIN Batch 0400/1314, Loss  116.6305, NLL-Loss  116.6305\n",
      "TRAIN Batch 0450/1314, Loss  113.0398, NLL-Loss  113.0398\n",
      "TRAIN Batch 0500/1314, Loss  114.6140, NLL-Loss  114.6140\n",
      "TRAIN Batch 0550/1314, Loss  103.1386, NLL-Loss  103.1386\n",
      "TRAIN Batch 0600/1314, Loss  111.9515, NLL-Loss  111.9515\n",
      "TRAIN Batch 0650/1314, Loss  109.0540, NLL-Loss  109.0540\n",
      "TRAIN Batch 0700/1314, Loss  105.4553, NLL-Loss  105.4553\n",
      "TRAIN Batch 0750/1314, Loss  101.7104, NLL-Loss  101.7104\n",
      "TRAIN Batch 0800/1314, Loss  102.3655, NLL-Loss  102.3655\n",
      "TRAIN Batch 0850/1314, Loss  118.8128, NLL-Loss  118.8128\n",
      "TRAIN Batch 0900/1314, Loss  105.6838, NLL-Loss  105.6838\n",
      "TRAIN Batch 0950/1314, Loss   93.8029, NLL-Loss   93.8029\n",
      "TRAIN Batch 1000/1314, Loss   99.0490, NLL-Loss   99.0490\n",
      "TRAIN Batch 1050/1314, Loss  111.8124, NLL-Loss  111.8124\n",
      "TRAIN Batch 1100/1314, Loss  108.0687, NLL-Loss  108.0687\n",
      "TRAIN Batch 1150/1314, Loss  116.7477, NLL-Loss  116.7477\n",
      "TRAIN Batch 1200/1314, Loss   90.0740, NLL-Loss   90.0740\n",
      "TRAIN Batch 1250/1314, Loss   98.8649, NLL-Loss   98.8649\n",
      "TRAIN Batch 1300/1314, Loss  102.5263, NLL-Loss  102.5263\n",
      "TRAIN Batch 1314/1314, Loss   94.1785, NLL-Loss   94.1785\n",
      "TRAIN Epoch 49/300, Mean NLL  108.8587\n",
      "Model saved at bin/2019-May-17-21:02:23/E49.pytorch\n",
      "VALID Batch 0000/105, Loss  133.6909, NLL-Loss  133.6909\n",
      "VALID Batch 0050/105, Loss  128.2061, NLL-Loss  128.2061\n",
      "VALID Batch 0100/105, Loss   98.4689, NLL-Loss   98.4689\n",
      "VALID Batch 0105/105, Loss   99.4915, NLL-Loss   99.4915\n",
      "VALID Epoch 49/300, Mean NLL  111.8024\n",
      "TEST Batch 0000/117, Loss  104.0485, NLL-Loss  104.0485\n",
      "TEST Batch 0050/117, Loss  117.6838, NLL-Loss  117.6838\n",
      "TEST Batch 0100/117, Loss  138.6696, NLL-Loss  138.6696\n",
      "TEST Batch 0117/117, Loss  162.7015, NLL-Loss  162.7015\n",
      "TEST Epoch 49/300, Mean NLL  111.9217\n",
      "TRAIN Batch 0000/1314, Loss  103.0007, NLL-Loss  103.0007\n",
      "TRAIN Batch 0050/1314, Loss  123.2145, NLL-Loss  123.2145\n",
      "TRAIN Batch 0100/1314, Loss  100.6852, NLL-Loss  100.6852\n",
      "TRAIN Batch 0150/1314, Loss  108.9556, NLL-Loss  108.9556\n",
      "TRAIN Batch 0200/1314, Loss  116.2937, NLL-Loss  116.2937\n",
      "TRAIN Batch 0250/1314, Loss  111.7725, NLL-Loss  111.7725\n",
      "TRAIN Batch 0300/1314, Loss  119.0266, NLL-Loss  119.0266\n",
      "TRAIN Batch 0350/1314, Loss  116.4148, NLL-Loss  116.4148\n",
      "TRAIN Batch 0400/1314, Loss   95.2991, NLL-Loss   95.2991\n",
      "TRAIN Batch 0450/1314, Loss  128.9443, NLL-Loss  128.9443\n",
      "TRAIN Batch 0500/1314, Loss  110.1945, NLL-Loss  110.1945\n",
      "TRAIN Batch 0550/1314, Loss  113.6226, NLL-Loss  113.6226\n",
      "TRAIN Batch 0600/1314, Loss  114.6315, NLL-Loss  114.6315\n",
      "TRAIN Batch 0650/1314, Loss  130.6170, NLL-Loss  130.6170\n",
      "TRAIN Batch 0700/1314, Loss  122.0021, NLL-Loss  122.0021\n",
      "TRAIN Batch 0750/1314, Loss  109.0816, NLL-Loss  109.0816\n",
      "TRAIN Batch 0800/1314, Loss  104.1893, NLL-Loss  104.1893\n",
      "TRAIN Batch 0850/1314, Loss  101.9216, NLL-Loss  101.9216\n",
      "TRAIN Batch 0900/1314, Loss  106.5261, NLL-Loss  106.5261\n",
      "TRAIN Batch 0950/1314, Loss  106.6030, NLL-Loss  106.6030\n",
      "TRAIN Batch 1000/1314, Loss  108.2660, NLL-Loss  108.2660\n",
      "TRAIN Batch 1050/1314, Loss  109.6518, NLL-Loss  109.6518\n",
      "TRAIN Batch 1100/1314, Loss  117.0619, NLL-Loss  117.0619\n",
      "TRAIN Batch 1150/1314, Loss  104.7859, NLL-Loss  104.7859\n",
      "TRAIN Batch 1200/1314, Loss  122.3580, NLL-Loss  122.3580\n",
      "TRAIN Batch 1250/1314, Loss  106.7190, NLL-Loss  106.7190\n",
      "TRAIN Batch 1300/1314, Loss   97.8568, NLL-Loss   97.8568\n",
      "TRAIN Batch 1314/1314, Loss   96.6779, NLL-Loss   96.6779\n",
      "TRAIN Epoch 50/300, Mean NLL  108.6473\n",
      "Model saved at bin/2019-May-17-21:02:23/E50.pytorch\n",
      "VALID Batch 0000/105, Loss  133.6342, NLL-Loss  133.6342\n",
      "VALID Batch 0050/105, Loss  128.2314, NLL-Loss  128.2314\n",
      "VALID Batch 0100/105, Loss   98.6011, NLL-Loss   98.6011\n",
      "VALID Batch 0105/105, Loss   99.4741, NLL-Loss   99.4741\n",
      "VALID Epoch 50/300, Mean NLL  111.7775\n",
      "TEST Batch 0000/117, Loss  104.0703, NLL-Loss  104.0703\n",
      "TEST Batch 0050/117, Loss  117.5975, NLL-Loss  117.5975\n",
      "TEST Batch 0100/117, Loss  138.4293, NLL-Loss  138.4293\n",
      "TEST Batch 0117/117, Loss  162.1841, NLL-Loss  162.1841\n",
      "TEST Epoch 50/300, Mean NLL  111.8915\n",
      "TRAIN Batch 0000/1314, Loss  112.7218, NLL-Loss  112.7218\n",
      "TRAIN Batch 0050/1314, Loss  119.5818, NLL-Loss  119.5818\n",
      "TRAIN Batch 0100/1314, Loss  102.9501, NLL-Loss  102.9501\n",
      "TRAIN Batch 0150/1314, Loss   90.8104, NLL-Loss   90.8104\n",
      "TRAIN Batch 0200/1314, Loss  104.7280, NLL-Loss  104.7280\n",
      "TRAIN Batch 0250/1314, Loss  106.6593, NLL-Loss  106.6593\n",
      "TRAIN Batch 0300/1314, Loss  105.6090, NLL-Loss  105.6090\n",
      "TRAIN Batch 0350/1314, Loss   99.8341, NLL-Loss   99.8341\n",
      "TRAIN Batch 0400/1314, Loss   97.6527, NLL-Loss   97.6527\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN Batch 0450/1314, Loss  119.9247, NLL-Loss  119.9247\n",
      "TRAIN Batch 0500/1314, Loss   94.4199, NLL-Loss   94.4199\n",
      "TRAIN Batch 0550/1314, Loss   99.8148, NLL-Loss   99.8148\n",
      "TRAIN Batch 0600/1314, Loss  106.3227, NLL-Loss  106.3227\n",
      "TRAIN Batch 0650/1314, Loss  112.3697, NLL-Loss  112.3697\n",
      "TRAIN Batch 0700/1314, Loss  114.4986, NLL-Loss  114.4986\n",
      "TRAIN Batch 0750/1314, Loss   97.5975, NLL-Loss   97.5975\n",
      "TRAIN Batch 0800/1314, Loss  118.6654, NLL-Loss  118.6654\n",
      "TRAIN Batch 0850/1314, Loss  114.9674, NLL-Loss  114.9674\n",
      "TRAIN Batch 0900/1314, Loss  114.9333, NLL-Loss  114.9333\n",
      "TRAIN Batch 0950/1314, Loss  110.2089, NLL-Loss  110.2089\n",
      "TRAIN Batch 1000/1314, Loss  110.4870, NLL-Loss  110.4870\n",
      "TRAIN Batch 1050/1314, Loss  115.2768, NLL-Loss  115.2768\n",
      "TRAIN Batch 1100/1314, Loss  123.6629, NLL-Loss  123.6629\n",
      "TRAIN Batch 1150/1314, Loss  101.3371, NLL-Loss  101.3371\n",
      "TRAIN Batch 1200/1314, Loss  101.9059, NLL-Loss  101.9059\n",
      "TRAIN Batch 1250/1314, Loss  120.6894, NLL-Loss  120.6894\n",
      "TRAIN Batch 1300/1314, Loss  125.5896, NLL-Loss  125.5896\n",
      "TRAIN Batch 1314/1314, Loss  113.0600, NLL-Loss  113.0600\n",
      "TRAIN Epoch 51/300, Mean NLL  108.4785\n",
      "Model saved at bin/2019-May-17-21:02:23/E51.pytorch\n",
      "VALID Batch 0000/105, Loss  133.6229, NLL-Loss  133.6229\n",
      "VALID Batch 0050/105, Loss  127.7717, NLL-Loss  127.7717\n",
      "VALID Batch 0100/105, Loss   98.3992, NLL-Loss   98.3992\n",
      "VALID Batch 0105/105, Loss   99.4107, NLL-Loss   99.4107\n",
      "VALID Epoch 51/300, Mean NLL  111.6323\n",
      "TEST Batch 0000/117, Loss  103.9431, NLL-Loss  103.9431\n",
      "TEST Batch 0050/117, Loss  117.6376, NLL-Loss  117.6376\n",
      "TEST Batch 0100/117, Loss  138.4506, NLL-Loss  138.4506\n",
      "TEST Batch 0117/117, Loss  162.3743, NLL-Loss  162.3743\n",
      "TEST Epoch 51/300, Mean NLL  111.7403\n",
      "TRAIN Batch 0000/1314, Loss  100.7682, NLL-Loss  100.7682\n",
      "TRAIN Batch 0050/1314, Loss  101.4592, NLL-Loss  101.4592\n",
      "TRAIN Batch 0100/1314, Loss  102.6922, NLL-Loss  102.6922\n",
      "TRAIN Batch 0150/1314, Loss  126.6843, NLL-Loss  126.6843\n",
      "TRAIN Batch 0200/1314, Loss  111.7610, NLL-Loss  111.7610\n",
      "TRAIN Batch 0250/1314, Loss  115.8795, NLL-Loss  115.8795\n",
      "TRAIN Batch 0300/1314, Loss  102.6883, NLL-Loss  102.6883\n",
      "TRAIN Batch 0350/1314, Loss  107.4773, NLL-Loss  107.4773\n",
      "TRAIN Batch 0400/1314, Loss  114.0753, NLL-Loss  114.0753\n",
      "TRAIN Batch 0450/1314, Loss  111.4142, NLL-Loss  111.4142\n",
      "TRAIN Batch 0500/1314, Loss   91.0209, NLL-Loss   91.0209\n",
      "TRAIN Batch 0550/1314, Loss  100.0647, NLL-Loss  100.0647\n",
      "TRAIN Batch 0600/1314, Loss   95.9804, NLL-Loss   95.9804\n",
      "TRAIN Batch 0650/1314, Loss  113.5648, NLL-Loss  113.5648\n",
      "TRAIN Batch 0700/1314, Loss  105.5624, NLL-Loss  105.5624\n",
      "TRAIN Batch 0750/1314, Loss  110.5818, NLL-Loss  110.5818\n",
      "TRAIN Batch 0800/1314, Loss  118.7159, NLL-Loss  118.7159\n",
      "TRAIN Batch 0850/1314, Loss  118.4891, NLL-Loss  118.4891\n",
      "TRAIN Batch 0900/1314, Loss  106.5171, NLL-Loss  106.5171\n",
      "TRAIN Batch 0950/1314, Loss  119.1385, NLL-Loss  119.1385\n",
      "TRAIN Batch 1000/1314, Loss  120.0681, NLL-Loss  120.0681\n",
      "TRAIN Batch 1050/1314, Loss  113.5070, NLL-Loss  113.5070\n",
      "TRAIN Batch 1100/1314, Loss  112.9735, NLL-Loss  112.9735\n",
      "TRAIN Batch 1150/1314, Loss   88.4300, NLL-Loss   88.4300\n",
      "TRAIN Batch 1200/1314, Loss  102.3841, NLL-Loss  102.3841\n",
      "TRAIN Batch 1250/1314, Loss  106.1631, NLL-Loss  106.1631\n",
      "TRAIN Batch 1300/1314, Loss  115.8090, NLL-Loss  115.8090\n",
      "TRAIN Batch 1314/1314, Loss  124.7269, NLL-Loss  124.7269\n",
      "TRAIN Epoch 52/300, Mean NLL  108.2844\n",
      "Model saved at bin/2019-May-17-21:02:23/E52.pytorch\n",
      "VALID Batch 0000/105, Loss  133.7217, NLL-Loss  133.7217\n",
      "VALID Batch 0050/105, Loss  127.6097, NLL-Loss  127.6097\n",
      "VALID Batch 0100/105, Loss   98.1820, NLL-Loss   98.1820\n",
      "VALID Batch 0105/105, Loss   99.6508, NLL-Loss   99.6508\n",
      "VALID Epoch 52/300, Mean NLL  111.5078\n",
      "TEST Batch 0000/117, Loss  103.6542, NLL-Loss  103.6542\n",
      "TEST Batch 0050/117, Loss  117.4857, NLL-Loss  117.4857\n",
      "TEST Batch 0100/117, Loss  138.4511, NLL-Loss  138.4511\n",
      "TEST Batch 0117/117, Loss  162.2906, NLL-Loss  162.2906\n",
      "TEST Epoch 52/300, Mean NLL  111.6295\n",
      "TRAIN Batch 0000/1314, Loss  105.1126, NLL-Loss  105.1126\n",
      "TRAIN Batch 0050/1314, Loss  136.4423, NLL-Loss  136.4423\n",
      "TRAIN Batch 0100/1314, Loss  101.5379, NLL-Loss  101.5379\n",
      "TRAIN Batch 0150/1314, Loss  110.5920, NLL-Loss  110.5920\n",
      "TRAIN Batch 0200/1314, Loss  124.3259, NLL-Loss  124.3259\n",
      "TRAIN Batch 0250/1314, Loss  105.8345, NLL-Loss  105.8345\n",
      "TRAIN Batch 0300/1314, Loss   95.0902, NLL-Loss   95.0902\n",
      "TRAIN Batch 0350/1314, Loss  111.7259, NLL-Loss  111.7259\n",
      "TRAIN Batch 0400/1314, Loss   95.1519, NLL-Loss   95.1519\n",
      "TRAIN Batch 0450/1314, Loss  112.0445, NLL-Loss  112.0445\n",
      "TRAIN Batch 0500/1314, Loss  110.7529, NLL-Loss  110.7529\n",
      "TRAIN Batch 0550/1314, Loss  119.9838, NLL-Loss  119.9838\n",
      "TRAIN Batch 0600/1314, Loss  120.5822, NLL-Loss  120.5822\n",
      "TRAIN Batch 0650/1314, Loss  110.7617, NLL-Loss  110.7617\n",
      "TRAIN Batch 0700/1314, Loss   93.5138, NLL-Loss   93.5138\n",
      "TRAIN Batch 0750/1314, Loss   85.1456, NLL-Loss   85.1456\n",
      "TRAIN Batch 0800/1314, Loss  120.5824, NLL-Loss  120.5824\n",
      "TRAIN Batch 0850/1314, Loss  121.3033, NLL-Loss  121.3033\n",
      "TRAIN Batch 0900/1314, Loss  102.2216, NLL-Loss  102.2216\n",
      "TRAIN Batch 0950/1314, Loss  119.4378, NLL-Loss  119.4378\n",
      "TRAIN Batch 1000/1314, Loss  108.9098, NLL-Loss  108.9098\n",
      "TRAIN Batch 1050/1314, Loss  117.1012, NLL-Loss  117.1012\n",
      "TRAIN Batch 1100/1314, Loss  110.3866, NLL-Loss  110.3866\n",
      "TRAIN Batch 1150/1314, Loss  112.2433, NLL-Loss  112.2433\n",
      "TRAIN Batch 1200/1314, Loss   90.2792, NLL-Loss   90.2792\n",
      "TRAIN Batch 1250/1314, Loss  130.4917, NLL-Loss  130.4917\n",
      "TRAIN Batch 1300/1314, Loss  112.4019, NLL-Loss  112.4019\n",
      "TRAIN Batch 1314/1314, Loss  117.1921, NLL-Loss  117.1921\n",
      "TRAIN Epoch 53/300, Mean NLL  108.1262\n",
      "Model saved at bin/2019-May-17-21:02:23/E53.pytorch\n",
      "VALID Batch 0000/105, Loss  133.6622, NLL-Loss  133.6622\n",
      "VALID Batch 0050/105, Loss  127.3157, NLL-Loss  127.3157\n",
      "VALID Batch 0100/105, Loss   98.1127, NLL-Loss   98.1127\n",
      "VALID Batch 0105/105, Loss   99.6586, NLL-Loss   99.6586\n",
      "VALID Epoch 53/300, Mean NLL  111.4036\n",
      "TEST Batch 0000/117, Loss  103.5905, NLL-Loss  103.5905\n",
      "TEST Batch 0050/117, Loss  117.3377, NLL-Loss  117.3377\n",
      "TEST Batch 0100/117, Loss  138.4373, NLL-Loss  138.4373\n",
      "TEST Batch 0117/117, Loss  162.7964, NLL-Loss  162.7964\n",
      "TEST Epoch 53/300, Mean NLL  111.5307\n",
      "TRAIN Batch 0000/1314, Loss  108.4452, NLL-Loss  108.4452\n",
      "TRAIN Batch 0050/1314, Loss  124.4786, NLL-Loss  124.4786\n",
      "TRAIN Batch 0100/1314, Loss  101.1921, NLL-Loss  101.1921\n",
      "TRAIN Batch 0150/1314, Loss  103.5528, NLL-Loss  103.5528\n",
      "TRAIN Batch 0200/1314, Loss  118.4002, NLL-Loss  118.4002\n",
      "TRAIN Batch 0250/1314, Loss  118.1436, NLL-Loss  118.1436\n",
      "TRAIN Batch 0300/1314, Loss  117.1917, NLL-Loss  117.1917\n",
      "TRAIN Batch 0350/1314, Loss  110.1686, NLL-Loss  110.1686\n",
      "TRAIN Batch 0400/1314, Loss  124.1454, NLL-Loss  124.1454\n",
      "TRAIN Batch 0450/1314, Loss  114.1983, NLL-Loss  114.1983\n",
      "TRAIN Batch 0500/1314, Loss  113.1945, NLL-Loss  113.1945\n",
      "TRAIN Batch 0550/1314, Loss  109.0644, NLL-Loss  109.0644\n",
      "TRAIN Batch 0600/1314, Loss   99.7612, NLL-Loss   99.7612\n",
      "TRAIN Batch 0650/1314, Loss  120.8686, NLL-Loss  120.8686\n",
      "TRAIN Batch 0700/1314, Loss  110.2341, NLL-Loss  110.2341\n",
      "TRAIN Batch 0750/1314, Loss  104.1452, NLL-Loss  104.1452\n",
      "TRAIN Batch 0800/1314, Loss  103.1038, NLL-Loss  103.1038\n",
      "TRAIN Batch 0850/1314, Loss  113.5384, NLL-Loss  113.5384\n",
      "TRAIN Batch 0900/1314, Loss  101.5945, NLL-Loss  101.5945\n",
      "TRAIN Batch 0950/1314, Loss   99.3902, NLL-Loss   99.3902\n",
      "TRAIN Batch 1000/1314, Loss  107.7309, NLL-Loss  107.7309\n",
      "TRAIN Batch 1050/1314, Loss  118.6697, NLL-Loss  118.6697\n",
      "TRAIN Batch 1100/1314, Loss  103.6722, NLL-Loss  103.6722\n",
      "TRAIN Batch 1150/1314, Loss  103.4945, NLL-Loss  103.4945\n",
      "TRAIN Batch 1200/1314, Loss  133.2049, NLL-Loss  133.2049\n",
      "TRAIN Batch 1250/1314, Loss  121.1001, NLL-Loss  121.1001\n",
      "TRAIN Batch 1300/1314, Loss  134.6332, NLL-Loss  134.6332\n",
      "TRAIN Batch 1314/1314, Loss   87.3427, NLL-Loss   87.3427\n",
      "TRAIN Epoch 54/300, Mean NLL  107.9278\n",
      "Model saved at bin/2019-May-17-21:02:23/E54.pytorch\n",
      "VALID Batch 0000/105, Loss  133.5974, NLL-Loss  133.5974\n",
      "VALID Batch 0050/105, Loss  127.1944, NLL-Loss  127.1944\n",
      "VALID Batch 0100/105, Loss   98.0077, NLL-Loss   98.0077\n",
      "VALID Batch 0105/105, Loss   99.4256, NLL-Loss   99.4256\n",
      "VALID Epoch 54/300, Mean NLL  111.2859\n",
      "TEST Batch 0000/117, Loss  103.3799, NLL-Loss  103.3799\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST Batch 0050/117, Loss  117.1705, NLL-Loss  117.1705\n",
      "TEST Batch 0100/117, Loss  138.2115, NLL-Loss  138.2115\n",
      "TEST Batch 0117/117, Loss  162.1406, NLL-Loss  162.1406\n",
      "TEST Epoch 54/300, Mean NLL  111.4054\n",
      "TRAIN Batch 0000/1314, Loss  108.4107, NLL-Loss  108.4107\n",
      "TRAIN Batch 0050/1314, Loss   93.7970, NLL-Loss   93.7970\n",
      "TRAIN Batch 0100/1314, Loss  108.9125, NLL-Loss  108.9125\n",
      "TRAIN Batch 0150/1314, Loss  122.7630, NLL-Loss  122.7630\n",
      "TRAIN Batch 0200/1314, Loss  115.5789, NLL-Loss  115.5789\n",
      "TRAIN Batch 0250/1314, Loss  108.1899, NLL-Loss  108.1899\n",
      "TRAIN Batch 0300/1314, Loss  109.3245, NLL-Loss  109.3245\n",
      "TRAIN Batch 0350/1314, Loss  102.8484, NLL-Loss  102.8484\n",
      "TRAIN Batch 0400/1314, Loss  109.0097, NLL-Loss  109.0097\n",
      "TRAIN Batch 0450/1314, Loss  127.0521, NLL-Loss  127.0521\n",
      "TRAIN Batch 0500/1314, Loss  105.6446, NLL-Loss  105.6446\n",
      "TRAIN Batch 0550/1314, Loss  104.2150, NLL-Loss  104.2150\n",
      "TRAIN Batch 0600/1314, Loss  117.5695, NLL-Loss  117.5695\n",
      "TRAIN Batch 0650/1314, Loss  102.4576, NLL-Loss  102.4576\n",
      "TRAIN Batch 0700/1314, Loss  101.7076, NLL-Loss  101.7076\n",
      "TRAIN Batch 0750/1314, Loss   93.1770, NLL-Loss   93.1770\n",
      "TRAIN Batch 0800/1314, Loss  117.3028, NLL-Loss  117.3028\n",
      "TRAIN Batch 0850/1314, Loss  100.6206, NLL-Loss  100.6206\n",
      "TRAIN Batch 0900/1314, Loss  107.1073, NLL-Loss  107.1073\n",
      "TRAIN Batch 0950/1314, Loss  118.2401, NLL-Loss  118.2401\n",
      "TRAIN Batch 1000/1314, Loss  116.3285, NLL-Loss  116.3285\n",
      "TRAIN Batch 1050/1314, Loss  106.0728, NLL-Loss  106.0728\n",
      "TRAIN Batch 1100/1314, Loss  103.8770, NLL-Loss  103.8770\n",
      "TRAIN Batch 1150/1314, Loss  116.8640, NLL-Loss  116.8640\n",
      "TRAIN Batch 1200/1314, Loss   86.1549, NLL-Loss   86.1549\n",
      "TRAIN Batch 1250/1314, Loss  110.8762, NLL-Loss  110.8762\n",
      "TRAIN Batch 1300/1314, Loss  104.0396, NLL-Loss  104.0396\n",
      "TRAIN Batch 1314/1314, Loss  112.5990, NLL-Loss  112.5990\n",
      "TRAIN Epoch 55/300, Mean NLL  107.7157\n",
      "Model saved at bin/2019-May-17-21:02:23/E55.pytorch\n",
      "VALID Batch 0000/105, Loss  133.4565, NLL-Loss  133.4565\n",
      "VALID Batch 0050/105, Loss  127.0673, NLL-Loss  127.0673\n",
      "VALID Batch 0100/105, Loss   98.0636, NLL-Loss   98.0636\n",
      "VALID Batch 0105/105, Loss   99.3090, NLL-Loss   99.3090\n",
      "VALID Epoch 55/300, Mean NLL  111.2341\n",
      "TEST Batch 0000/117, Loss  103.1729, NLL-Loss  103.1729\n",
      "TEST Batch 0050/117, Loss  117.3321, NLL-Loss  117.3321\n",
      "TEST Batch 0100/117, Loss  138.2392, NLL-Loss  138.2392\n",
      "TEST Batch 0117/117, Loss  162.0951, NLL-Loss  162.0951\n",
      "TEST Epoch 55/300, Mean NLL  111.3437\n",
      "TRAIN Batch 0000/1314, Loss  111.6834, NLL-Loss  111.6834\n",
      "TRAIN Batch 0050/1314, Loss  119.2773, NLL-Loss  119.2773\n",
      "TRAIN Batch 0100/1314, Loss  103.8778, NLL-Loss  103.8778\n",
      "TRAIN Batch 0150/1314, Loss  106.4685, NLL-Loss  106.4685\n",
      "TRAIN Batch 0200/1314, Loss  107.6677, NLL-Loss  107.6677\n",
      "TRAIN Batch 0250/1314, Loss  129.4232, NLL-Loss  129.4232\n",
      "TRAIN Batch 0300/1314, Loss  110.6187, NLL-Loss  110.6187\n",
      "TRAIN Batch 0350/1314, Loss  116.5150, NLL-Loss  116.5150\n",
      "TRAIN Batch 0400/1314, Loss  100.7364, NLL-Loss  100.7364\n",
      "TRAIN Batch 0450/1314, Loss  107.4016, NLL-Loss  107.4016\n",
      "TRAIN Batch 0500/1314, Loss  103.8883, NLL-Loss  103.8883\n",
      "TRAIN Batch 0550/1314, Loss   97.5417, NLL-Loss   97.5417\n",
      "TRAIN Batch 0600/1314, Loss   93.2656, NLL-Loss   93.2656\n",
      "TRAIN Batch 0650/1314, Loss  102.5275, NLL-Loss  102.5275\n",
      "TRAIN Batch 0700/1314, Loss  116.4607, NLL-Loss  116.4607\n",
      "TRAIN Batch 0750/1314, Loss   97.9705, NLL-Loss   97.9705\n",
      "TRAIN Batch 0800/1314, Loss  112.1818, NLL-Loss  112.1818\n",
      "TRAIN Batch 0850/1314, Loss  116.6852, NLL-Loss  116.6852\n",
      "TRAIN Batch 0900/1314, Loss  113.6190, NLL-Loss  113.6190\n",
      "TRAIN Batch 0950/1314, Loss  121.3934, NLL-Loss  121.3934\n",
      "TRAIN Batch 1000/1314, Loss  108.1876, NLL-Loss  108.1876\n",
      "TRAIN Batch 1050/1314, Loss   96.4501, NLL-Loss   96.4501\n",
      "TRAIN Batch 1100/1314, Loss  116.4847, NLL-Loss  116.4847\n",
      "TRAIN Batch 1150/1314, Loss   99.7896, NLL-Loss   99.7896\n",
      "TRAIN Batch 1200/1314, Loss  115.5425, NLL-Loss  115.5425\n",
      "TRAIN Batch 1250/1314, Loss   97.3341, NLL-Loss   97.3341\n",
      "TRAIN Batch 1300/1314, Loss  104.5119, NLL-Loss  104.5119\n",
      "TRAIN Batch 1314/1314, Loss   98.4144, NLL-Loss   98.4144\n",
      "TRAIN Epoch 56/300, Mean NLL  107.5921\n",
      "Model saved at bin/2019-May-17-21:02:23/E56.pytorch\n",
      "VALID Batch 0000/105, Loss  133.4796, NLL-Loss  133.4796\n",
      "VALID Batch 0050/105, Loss  126.9279, NLL-Loss  126.9279\n",
      "VALID Batch 0100/105, Loss   97.9022, NLL-Loss   97.9022\n",
      "VALID Batch 0105/105, Loss   99.2387, NLL-Loss   99.2387\n",
      "VALID Epoch 56/300, Mean NLL  111.1470\n",
      "TEST Batch 0000/117, Loss  103.0995, NLL-Loss  103.0995\n",
      "TEST Batch 0050/117, Loss  117.0930, NLL-Loss  117.0930\n",
      "TEST Batch 0100/117, Loss  138.2130, NLL-Loss  138.2130\n",
      "TEST Batch 0117/117, Loss  162.2828, NLL-Loss  162.2828\n",
      "TEST Epoch 56/300, Mean NLL  111.2825\n",
      "TRAIN Batch 0000/1314, Loss  107.2877, NLL-Loss  107.2877\n",
      "TRAIN Batch 0050/1314, Loss  120.7953, NLL-Loss  120.7953\n",
      "TRAIN Batch 0100/1314, Loss  128.4976, NLL-Loss  128.4976\n",
      "TRAIN Batch 0150/1314, Loss   95.6262, NLL-Loss   95.6262\n",
      "TRAIN Batch 0200/1314, Loss  107.0992, NLL-Loss  107.0992\n",
      "TRAIN Batch 0250/1314, Loss  122.1580, NLL-Loss  122.1580\n",
      "TRAIN Batch 0300/1314, Loss   94.3221, NLL-Loss   94.3221\n",
      "TRAIN Batch 0350/1314, Loss  117.3671, NLL-Loss  117.3671\n",
      "TRAIN Batch 0400/1314, Loss   98.1794, NLL-Loss   98.1794\n",
      "TRAIN Batch 0450/1314, Loss  112.2668, NLL-Loss  112.2668\n",
      "TRAIN Batch 0500/1314, Loss  121.6005, NLL-Loss  121.6005\n",
      "TRAIN Batch 0550/1314, Loss  117.4336, NLL-Loss  117.4336\n",
      "TRAIN Batch 0600/1314, Loss  128.2706, NLL-Loss  128.2706\n",
      "TRAIN Batch 0650/1314, Loss  103.5904, NLL-Loss  103.5904\n",
      "TRAIN Batch 0700/1314, Loss  102.1296, NLL-Loss  102.1296\n",
      "TRAIN Batch 0750/1314, Loss  113.3418, NLL-Loss  113.3418\n",
      "TRAIN Batch 0800/1314, Loss  116.1672, NLL-Loss  116.1672\n",
      "TRAIN Batch 0850/1314, Loss   98.7302, NLL-Loss   98.7302\n",
      "TRAIN Batch 0900/1314, Loss  108.2692, NLL-Loss  108.2692\n",
      "TRAIN Batch 0950/1314, Loss  109.5489, NLL-Loss  109.5489\n",
      "TRAIN Batch 1000/1314, Loss  103.6536, NLL-Loss  103.6536\n",
      "TRAIN Batch 1050/1314, Loss   95.7525, NLL-Loss   95.7525\n",
      "TRAIN Batch 1100/1314, Loss   94.3195, NLL-Loss   94.3195\n",
      "TRAIN Batch 1150/1314, Loss  103.7165, NLL-Loss  103.7165\n",
      "TRAIN Batch 1200/1314, Loss  114.3630, NLL-Loss  114.3630\n",
      "TRAIN Batch 1250/1314, Loss  123.5659, NLL-Loss  123.5659\n",
      "TRAIN Batch 1300/1314, Loss  111.5594, NLL-Loss  111.5594\n",
      "TRAIN Batch 1314/1314, Loss  105.6687, NLL-Loss  105.6687\n",
      "TRAIN Epoch 57/300, Mean NLL  107.4227\n",
      "Model saved at bin/2019-May-17-21:02:23/E57.pytorch\n",
      "VALID Batch 0000/105, Loss  133.4239, NLL-Loss  133.4239\n",
      "VALID Batch 0050/105, Loss  126.7768, NLL-Loss  126.7768\n",
      "VALID Batch 0100/105, Loss   98.0110, NLL-Loss   98.0110\n",
      "VALID Batch 0105/105, Loss   98.9535, NLL-Loss   98.9535\n",
      "VALID Epoch 57/300, Mean NLL  111.0221\n",
      "TEST Batch 0000/117, Loss  103.0529, NLL-Loss  103.0529\n",
      "TEST Batch 0050/117, Loss  117.0088, NLL-Loss  117.0088\n",
      "TEST Batch 0100/117, Loss  138.0228, NLL-Loss  138.0228\n",
      "TEST Batch 0117/117, Loss  161.8447, NLL-Loss  161.8447\n",
      "TEST Epoch 57/300, Mean NLL  111.1282\n",
      "TRAIN Batch 0000/1314, Loss  108.7751, NLL-Loss  108.7751\n",
      "TRAIN Batch 0050/1314, Loss  102.5526, NLL-Loss  102.5526\n",
      "TRAIN Batch 0100/1314, Loss  121.3379, NLL-Loss  121.3379\n",
      "TRAIN Batch 0150/1314, Loss  104.0281, NLL-Loss  104.0281\n",
      "TRAIN Batch 0200/1314, Loss   95.7331, NLL-Loss   95.7331\n",
      "TRAIN Batch 0250/1314, Loss  114.6096, NLL-Loss  114.6096\n",
      "TRAIN Batch 0300/1314, Loss  110.9661, NLL-Loss  110.9661\n",
      "TRAIN Batch 0350/1314, Loss  112.9983, NLL-Loss  112.9983\n",
      "TRAIN Batch 0400/1314, Loss   95.2425, NLL-Loss   95.2425\n",
      "TRAIN Batch 0450/1314, Loss  125.5668, NLL-Loss  125.5668\n",
      "TRAIN Batch 0500/1314, Loss   95.9234, NLL-Loss   95.9234\n",
      "TRAIN Batch 0550/1314, Loss   98.3027, NLL-Loss   98.3027\n",
      "TRAIN Batch 0600/1314, Loss   88.6784, NLL-Loss   88.6784\n",
      "TRAIN Batch 0650/1314, Loss  112.6867, NLL-Loss  112.6867\n",
      "TRAIN Batch 0700/1314, Loss  101.1258, NLL-Loss  101.1258\n",
      "TRAIN Batch 0750/1314, Loss  112.5567, NLL-Loss  112.5567\n",
      "TRAIN Batch 0800/1314, Loss  107.8082, NLL-Loss  107.8082\n",
      "TRAIN Batch 0850/1314, Loss   94.4408, NLL-Loss   94.4408\n",
      "TRAIN Batch 0900/1314, Loss   93.6425, NLL-Loss   93.6425\n",
      "TRAIN Batch 0950/1314, Loss  107.6464, NLL-Loss  107.6464\n",
      "TRAIN Batch 1000/1314, Loss  107.6270, NLL-Loss  107.6270\n",
      "TRAIN Batch 1050/1314, Loss  106.1796, NLL-Loss  106.1796\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN Batch 1100/1314, Loss  111.6986, NLL-Loss  111.6986\n",
      "TRAIN Batch 1150/1314, Loss   98.4408, NLL-Loss   98.4408\n",
      "TRAIN Batch 1200/1314, Loss  104.1619, NLL-Loss  104.1619\n",
      "TRAIN Batch 1250/1314, Loss  102.5964, NLL-Loss  102.5964\n",
      "TRAIN Batch 1300/1314, Loss  126.6654, NLL-Loss  126.6654\n",
      "TRAIN Batch 1314/1314, Loss  116.9771, NLL-Loss  116.9771\n",
      "TRAIN Epoch 58/300, Mean NLL  107.2204\n",
      "Model saved at bin/2019-May-17-21:02:23/E58.pytorch\n",
      "VALID Batch 0000/105, Loss  133.4116, NLL-Loss  133.4116\n",
      "VALID Batch 0050/105, Loss  126.7863, NLL-Loss  126.7863\n",
      "VALID Batch 0100/105, Loss   98.0065, NLL-Loss   98.0065\n",
      "VALID Batch 0105/105, Loss   99.0635, NLL-Loss   99.0635\n",
      "VALID Epoch 58/300, Mean NLL  111.0238\n",
      "TEST Batch 0000/117, Loss  103.1349, NLL-Loss  103.1349\n",
      "TEST Batch 0050/117, Loss  117.0869, NLL-Loss  117.0869\n",
      "TEST Batch 0100/117, Loss  137.9765, NLL-Loss  137.9765\n",
      "TEST Batch 0117/117, Loss  161.8233, NLL-Loss  161.8233\n",
      "TEST Epoch 58/300, Mean NLL  111.1521\n",
      "TRAIN Batch 0000/1314, Loss  119.9234, NLL-Loss  119.9234\n",
      "TRAIN Batch 0050/1314, Loss  117.8744, NLL-Loss  117.8744\n",
      "TRAIN Batch 0100/1314, Loss  112.9785, NLL-Loss  112.9785\n",
      "TRAIN Batch 0150/1314, Loss   98.8759, NLL-Loss   98.8759\n",
      "TRAIN Batch 0200/1314, Loss  110.3681, NLL-Loss  110.3681\n",
      "TRAIN Batch 0250/1314, Loss  102.1491, NLL-Loss  102.1491\n",
      "TRAIN Batch 0300/1314, Loss  105.6034, NLL-Loss  105.6034\n",
      "TRAIN Batch 0350/1314, Loss  106.1289, NLL-Loss  106.1289\n",
      "TRAIN Batch 0400/1314, Loss  110.9781, NLL-Loss  110.9781\n",
      "TRAIN Batch 0450/1314, Loss   92.8531, NLL-Loss   92.8531\n",
      "TRAIN Batch 0500/1314, Loss  106.7227, NLL-Loss  106.7227\n",
      "TRAIN Batch 0550/1314, Loss  116.3031, NLL-Loss  116.3031\n",
      "TRAIN Batch 0600/1314, Loss  116.6695, NLL-Loss  116.6695\n",
      "TRAIN Batch 0650/1314, Loss  107.1257, NLL-Loss  107.1257\n",
      "TRAIN Batch 0700/1314, Loss   97.7417, NLL-Loss   97.7417\n",
      "TRAIN Batch 0750/1314, Loss  111.4937, NLL-Loss  111.4937\n",
      "TRAIN Batch 0800/1314, Loss  103.4898, NLL-Loss  103.4898\n",
      "TRAIN Batch 0850/1314, Loss   95.1098, NLL-Loss   95.1098\n",
      "TRAIN Batch 0900/1314, Loss  110.7661, NLL-Loss  110.7661\n",
      "TRAIN Batch 0950/1314, Loss  104.7248, NLL-Loss  104.7248\n",
      "TRAIN Batch 1000/1314, Loss  108.1887, NLL-Loss  108.1887\n",
      "TRAIN Batch 1050/1314, Loss  106.6520, NLL-Loss  106.6520\n",
      "TRAIN Batch 1100/1314, Loss   87.4420, NLL-Loss   87.4420\n",
      "TRAIN Batch 1150/1314, Loss  106.1856, NLL-Loss  106.1856\n",
      "TRAIN Batch 1200/1314, Loss   98.3904, NLL-Loss   98.3904\n",
      "TRAIN Batch 1250/1314, Loss  101.2142, NLL-Loss  101.2142\n",
      "TRAIN Batch 1300/1314, Loss   97.4277, NLL-Loss   97.4277\n",
      "TRAIN Batch 1314/1314, Loss  126.0618, NLL-Loss  126.0618\n",
      "TRAIN Epoch 59/300, Mean NLL  107.0597\n",
      "Model saved at bin/2019-May-17-21:02:23/E59.pytorch\n",
      "VALID Batch 0000/105, Loss  133.2856, NLL-Loss  133.2856\n",
      "VALID Batch 0050/105, Loss  126.3746, NLL-Loss  126.3746\n",
      "VALID Batch 0100/105, Loss   97.7636, NLL-Loss   97.7636\n",
      "VALID Batch 0105/105, Loss   99.1237, NLL-Loss   99.1237\n",
      "VALID Epoch 59/300, Mean NLL  110.8576\n",
      "TEST Batch 0000/117, Loss  102.8717, NLL-Loss  102.8717\n",
      "TEST Batch 0050/117, Loss  117.1841, NLL-Loss  117.1841\n",
      "TEST Batch 0100/117, Loss  137.9636, NLL-Loss  137.9636\n",
      "TEST Batch 0117/117, Loss  161.8842, NLL-Loss  161.8841\n",
      "TEST Epoch 59/300, Mean NLL  110.9840\n",
      "TRAIN Batch 0000/1314, Loss  102.5764, NLL-Loss  102.5764\n",
      "TRAIN Batch 0050/1314, Loss  122.6771, NLL-Loss  122.6771\n",
      "TRAIN Batch 0100/1314, Loss  101.6083, NLL-Loss  101.6083\n",
      "TRAIN Batch 0150/1314, Loss  114.2785, NLL-Loss  114.2785\n",
      "TRAIN Batch 0200/1314, Loss  111.0712, NLL-Loss  111.0712\n",
      "TRAIN Batch 0250/1314, Loss   86.8899, NLL-Loss   86.8899\n",
      "TRAIN Batch 0300/1314, Loss  105.4105, NLL-Loss  105.4105\n",
      "TRAIN Batch 0350/1314, Loss  104.3794, NLL-Loss  104.3794\n",
      "TRAIN Batch 0400/1314, Loss  106.5460, NLL-Loss  106.5460\n",
      "TRAIN Batch 0450/1314, Loss  103.4690, NLL-Loss  103.4690\n",
      "TRAIN Batch 0500/1314, Loss  128.4938, NLL-Loss  128.4938\n",
      "TRAIN Batch 0550/1314, Loss  122.9768, NLL-Loss  122.9768\n",
      "TRAIN Batch 0600/1314, Loss  106.7820, NLL-Loss  106.7820\n",
      "TRAIN Batch 0650/1314, Loss   97.3570, NLL-Loss   97.3570\n",
      "TRAIN Batch 0700/1314, Loss   96.2596, NLL-Loss   96.2596\n",
      "TRAIN Batch 0750/1314, Loss   93.0625, NLL-Loss   93.0625\n",
      "TRAIN Batch 0800/1314, Loss  101.2827, NLL-Loss  101.2827\n",
      "TRAIN Batch 0850/1314, Loss  124.0880, NLL-Loss  124.0880\n",
      "TRAIN Batch 0900/1314, Loss  125.9203, NLL-Loss  125.9203\n",
      "TRAIN Batch 0950/1314, Loss  108.1907, NLL-Loss  108.1907\n",
      "TRAIN Batch 1000/1314, Loss  101.9154, NLL-Loss  101.9154\n",
      "TRAIN Batch 1050/1314, Loss   94.8726, NLL-Loss   94.8726\n",
      "TRAIN Batch 1100/1314, Loss  115.3747, NLL-Loss  115.3747\n",
      "TRAIN Batch 1150/1314, Loss  101.7043, NLL-Loss  101.7043\n",
      "TRAIN Batch 1200/1314, Loss  114.0905, NLL-Loss  114.0905\n",
      "TRAIN Batch 1250/1314, Loss  111.2038, NLL-Loss  111.2038\n",
      "TRAIN Batch 1300/1314, Loss  103.3869, NLL-Loss  103.3869\n",
      "TRAIN Batch 1314/1314, Loss   90.8456, NLL-Loss   90.8456\n",
      "TRAIN Epoch 60/300, Mean NLL  106.8848\n",
      "Model saved at bin/2019-May-17-21:02:23/E60.pytorch\n",
      "VALID Batch 0000/105, Loss  133.3673, NLL-Loss  133.3673\n",
      "VALID Batch 0050/105, Loss  126.4482, NLL-Loss  126.4482\n",
      "VALID Batch 0100/105, Loss   97.7676, NLL-Loss   97.7676\n",
      "VALID Batch 0105/105, Loss   99.4006, NLL-Loss   99.4006\n",
      "VALID Epoch 60/300, Mean NLL  110.8730\n",
      "TEST Batch 0000/117, Loss  102.8349, NLL-Loss  102.8349\n",
      "TEST Batch 0050/117, Loss  117.0814, NLL-Loss  117.0814\n",
      "TEST Batch 0100/117, Loss  138.1499, NLL-Loss  138.1499\n",
      "TEST Batch 0117/117, Loss  162.2141, NLL-Loss  162.2141\n",
      "TEST Epoch 60/300, Mean NLL  111.0106\n",
      "TRAIN Batch 0000/1314, Loss  101.0124, NLL-Loss  101.0124\n",
      "TRAIN Batch 0050/1314, Loss  111.5422, NLL-Loss  111.5422\n",
      "TRAIN Batch 0100/1314, Loss   96.5251, NLL-Loss   96.5251\n",
      "TRAIN Batch 0150/1314, Loss  101.1759, NLL-Loss  101.1759\n",
      "TRAIN Batch 0200/1314, Loss   97.4765, NLL-Loss   97.4765\n",
      "TRAIN Batch 0250/1314, Loss  105.4948, NLL-Loss  105.4948\n",
      "TRAIN Batch 0300/1314, Loss  103.1685, NLL-Loss  103.1685\n",
      "TRAIN Batch 0350/1314, Loss  112.9353, NLL-Loss  112.9353\n",
      "TRAIN Batch 0400/1314, Loss  106.1469, NLL-Loss  106.1469\n",
      "TRAIN Batch 0450/1314, Loss  114.9278, NLL-Loss  114.9278\n",
      "TRAIN Batch 0500/1314, Loss  116.6445, NLL-Loss  116.6445\n",
      "TRAIN Batch 0550/1314, Loss  109.9140, NLL-Loss  109.9140\n",
      "TRAIN Batch 0600/1314, Loss  111.5496, NLL-Loss  111.5496\n",
      "TRAIN Batch 0650/1314, Loss  111.7725, NLL-Loss  111.7725\n",
      "TRAIN Batch 0700/1314, Loss   94.9566, NLL-Loss   94.9566\n",
      "TRAIN Batch 0750/1314, Loss  101.0811, NLL-Loss  101.0811\n",
      "TRAIN Batch 0800/1314, Loss  120.8653, NLL-Loss  120.8653\n",
      "TRAIN Batch 0850/1314, Loss  104.3808, NLL-Loss  104.3808\n",
      "TRAIN Batch 0900/1314, Loss   97.7296, NLL-Loss   97.7296\n",
      "TRAIN Batch 0950/1314, Loss   88.4425, NLL-Loss   88.4425\n",
      "TRAIN Batch 1000/1314, Loss   92.7102, NLL-Loss   92.7102\n",
      "TRAIN Batch 1050/1314, Loss  107.2069, NLL-Loss  107.2069\n",
      "TRAIN Batch 1100/1314, Loss  109.5246, NLL-Loss  109.5246\n",
      "TRAIN Batch 1150/1314, Loss   94.2772, NLL-Loss   94.2772\n",
      "TRAIN Batch 1200/1314, Loss  118.0856, NLL-Loss  118.0856\n",
      "TRAIN Batch 1250/1314, Loss  100.6391, NLL-Loss  100.6391\n",
      "TRAIN Batch 1300/1314, Loss  111.4890, NLL-Loss  111.4890\n",
      "TRAIN Batch 1314/1314, Loss  106.9841, NLL-Loss  106.9841\n",
      "TRAIN Epoch 61/300, Mean NLL  106.7249\n",
      "Model saved at bin/2019-May-17-21:02:23/E61.pytorch\n",
      "VALID Batch 0000/105, Loss  133.2517, NLL-Loss  133.2517\n",
      "VALID Batch 0050/105, Loss  125.9958, NLL-Loss  125.9958\n",
      "VALID Batch 0100/105, Loss   97.5509, NLL-Loss   97.5509\n",
      "VALID Batch 0105/105, Loss   99.3368, NLL-Loss   99.3368\n",
      "VALID Epoch 61/300, Mean NLL  110.6943\n",
      "TEST Batch 0000/117, Loss  102.5452, NLL-Loss  102.5452\n",
      "TEST Batch 0050/117, Loss  116.9154, NLL-Loss  116.9154\n",
      "TEST Batch 0100/117, Loss  138.2301, NLL-Loss  138.2301\n",
      "TEST Batch 0117/117, Loss  162.1848, NLL-Loss  162.1848\n",
      "TEST Epoch 61/300, Mean NLL  110.8320\n",
      "TRAIN Batch 0000/1314, Loss  104.1291, NLL-Loss  104.1291\n",
      "TRAIN Batch 0050/1314, Loss   98.5536, NLL-Loss   98.5536\n",
      "TRAIN Batch 0100/1314, Loss   99.0553, NLL-Loss   99.0553\n",
      "TRAIN Batch 0150/1314, Loss  110.4549, NLL-Loss  110.4549\n",
      "TRAIN Batch 0200/1314, Loss  112.4318, NLL-Loss  112.4318\n",
      "TRAIN Batch 0250/1314, Loss  104.6493, NLL-Loss  104.6493\n",
      "TRAIN Batch 0300/1314, Loss  113.7588, NLL-Loss  113.7588\n",
      "TRAIN Batch 0350/1314, Loss  109.8028, NLL-Loss  109.8028\n",
      "TRAIN Batch 0400/1314, Loss  112.5164, NLL-Loss  112.5164\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN Batch 0450/1314, Loss  104.3456, NLL-Loss  104.3456\n",
      "TRAIN Batch 0500/1314, Loss   90.9414, NLL-Loss   90.9414\n",
      "TRAIN Batch 0550/1314, Loss   99.9717, NLL-Loss   99.9717\n",
      "TRAIN Batch 0600/1314, Loss  111.4509, NLL-Loss  111.4509\n",
      "TRAIN Batch 0650/1314, Loss   94.6110, NLL-Loss   94.6110\n",
      "TRAIN Batch 0700/1314, Loss  114.1437, NLL-Loss  114.1437\n",
      "TRAIN Batch 0750/1314, Loss  118.8778, NLL-Loss  118.8778\n",
      "TRAIN Batch 0800/1314, Loss  123.9192, NLL-Loss  123.9192\n",
      "TRAIN Batch 0850/1314, Loss  117.7987, NLL-Loss  117.7987\n",
      "TRAIN Batch 0900/1314, Loss  103.1283, NLL-Loss  103.1283\n",
      "TRAIN Batch 0950/1314, Loss  110.5905, NLL-Loss  110.5905\n",
      "TRAIN Batch 1000/1314, Loss  101.7784, NLL-Loss  101.7784\n",
      "TRAIN Batch 1050/1314, Loss   94.7776, NLL-Loss   94.7776\n",
      "TRAIN Batch 1100/1314, Loss  120.1820, NLL-Loss  120.1820\n",
      "TRAIN Batch 1150/1314, Loss  116.9036, NLL-Loss  116.9036\n",
      "TRAIN Batch 1200/1314, Loss   99.4455, NLL-Loss   99.4455\n",
      "TRAIN Batch 1250/1314, Loss   91.3812, NLL-Loss   91.3812\n",
      "TRAIN Batch 1300/1314, Loss  104.8754, NLL-Loss  104.8754\n",
      "TRAIN Batch 1314/1314, Loss  103.1636, NLL-Loss  103.1635\n",
      "TRAIN Epoch 62/300, Mean NLL  106.5709\n",
      "Model saved at bin/2019-May-17-21:02:23/E62.pytorch\n",
      "VALID Batch 0000/105, Loss  133.3165, NLL-Loss  133.3165\n",
      "VALID Batch 0050/105, Loss  126.0850, NLL-Loss  126.0850\n",
      "VALID Batch 0100/105, Loss   97.6963, NLL-Loss   97.6963\n",
      "VALID Batch 0105/105, Loss   99.3305, NLL-Loss   99.3305\n",
      "VALID Epoch 62/300, Mean NLL  110.7412\n",
      "TEST Batch 0000/117, Loss  102.6167, NLL-Loss  102.6167\n",
      "TEST Batch 0050/117, Loss  116.9362, NLL-Loss  116.9362\n",
      "TEST Batch 0100/117, Loss  137.9821, NLL-Loss  137.9821\n",
      "TEST Batch 0117/117, Loss  161.9852, NLL-Loss  161.9852\n",
      "TEST Epoch 62/300, Mean NLL  110.8569\n",
      "TRAIN Batch 0000/1314, Loss  100.0143, NLL-Loss  100.0143\n",
      "TRAIN Batch 0050/1314, Loss  104.6591, NLL-Loss  104.6591\n",
      "TRAIN Batch 0100/1314, Loss  114.9341, NLL-Loss  114.9341\n",
      "TRAIN Batch 0150/1314, Loss  109.9823, NLL-Loss  109.9823\n",
      "TRAIN Batch 0200/1314, Loss  111.6876, NLL-Loss  111.6876\n",
      "TRAIN Batch 0250/1314, Loss  100.6745, NLL-Loss  100.6745\n",
      "TRAIN Batch 0300/1314, Loss  108.5307, NLL-Loss  108.5307\n",
      "TRAIN Batch 0350/1314, Loss  103.8192, NLL-Loss  103.8192\n",
      "TRAIN Batch 0400/1314, Loss   95.4037, NLL-Loss   95.4037\n",
      "TRAIN Batch 0450/1314, Loss  107.8127, NLL-Loss  107.8127\n",
      "TRAIN Batch 0500/1314, Loss  107.8198, NLL-Loss  107.8198\n",
      "TRAIN Batch 0550/1314, Loss   97.7348, NLL-Loss   97.7348\n",
      "TRAIN Batch 0600/1314, Loss   98.3310, NLL-Loss   98.3310\n",
      "TRAIN Batch 0650/1314, Loss  108.5727, NLL-Loss  108.5727\n",
      "TRAIN Batch 0700/1314, Loss  115.2952, NLL-Loss  115.2952\n",
      "TRAIN Batch 0750/1314, Loss   93.2670, NLL-Loss   93.2670\n",
      "TRAIN Batch 0800/1314, Loss   94.9047, NLL-Loss   94.9047\n",
      "TRAIN Batch 0850/1314, Loss  108.7741, NLL-Loss  108.7741\n",
      "TRAIN Batch 0900/1314, Loss  110.8029, NLL-Loss  110.8029\n",
      "TRAIN Batch 0950/1314, Loss   85.4894, NLL-Loss   85.4894\n",
      "TRAIN Batch 1000/1314, Loss  107.7461, NLL-Loss  107.7461\n",
      "TRAIN Batch 1050/1314, Loss   97.8142, NLL-Loss   97.8142\n",
      "TRAIN Batch 1100/1314, Loss  100.8148, NLL-Loss  100.8148\n",
      "TRAIN Batch 1150/1314, Loss   95.3151, NLL-Loss   95.3151\n",
      "TRAIN Batch 1200/1314, Loss   99.3829, NLL-Loss   99.3829\n",
      "TRAIN Batch 1250/1314, Loss  107.7230, NLL-Loss  107.7230\n",
      "TRAIN Batch 1300/1314, Loss  101.5669, NLL-Loss  101.5669\n",
      "TRAIN Batch 1314/1314, Loss  143.9854, NLL-Loss  143.9854\n",
      "TRAIN Epoch 63/300, Mean NLL  106.4197\n",
      "Model saved at bin/2019-May-17-21:02:23/E63.pytorch\n",
      "VALID Batch 0000/105, Loss  133.2196, NLL-Loss  133.2196\n",
      "VALID Batch 0050/105, Loss  125.8984, NLL-Loss  125.8984\n",
      "VALID Batch 0100/105, Loss   97.6992, NLL-Loss   97.6992\n",
      "VALID Batch 0105/105, Loss   99.1060, NLL-Loss   99.1060\n",
      "VALID Epoch 63/300, Mean NLL  110.6121\n",
      "TEST Batch 0000/117, Loss  102.5606, NLL-Loss  102.5606\n",
      "TEST Batch 0050/117, Loss  116.7779, NLL-Loss  116.7779\n",
      "TEST Batch 0100/117, Loss  137.8544, NLL-Loss  137.8544\n",
      "TEST Batch 0117/117, Loss  161.8992, NLL-Loss  161.8992\n",
      "TEST Epoch 63/300, Mean NLL  110.7283\n",
      "TRAIN Batch 0000/1314, Loss  101.7513, NLL-Loss  101.7513\n",
      "TRAIN Batch 0050/1314, Loss  119.8424, NLL-Loss  119.8424\n",
      "TRAIN Batch 0100/1314, Loss  109.3755, NLL-Loss  109.3755\n",
      "TRAIN Batch 0150/1314, Loss   86.4159, NLL-Loss   86.4159\n",
      "TRAIN Batch 0200/1314, Loss  104.1863, NLL-Loss  104.1863\n",
      "TRAIN Batch 0250/1314, Loss   93.6201, NLL-Loss   93.6201\n",
      "TRAIN Batch 0300/1314, Loss   98.9922, NLL-Loss   98.9922\n",
      "TRAIN Batch 0350/1314, Loss  101.3619, NLL-Loss  101.3619\n",
      "TRAIN Batch 0400/1314, Loss  100.8437, NLL-Loss  100.8437\n",
      "TRAIN Batch 0450/1314, Loss   92.9781, NLL-Loss   92.9781\n",
      "TRAIN Batch 0500/1314, Loss  112.5015, NLL-Loss  112.5015\n",
      "TRAIN Batch 0550/1314, Loss  100.8201, NLL-Loss  100.8201\n",
      "TRAIN Batch 0600/1314, Loss  107.3413, NLL-Loss  107.3413\n",
      "TRAIN Batch 0650/1314, Loss   98.1616, NLL-Loss   98.1616\n",
      "TRAIN Batch 0700/1314, Loss  104.8218, NLL-Loss  104.8218\n",
      "TRAIN Batch 0750/1314, Loss  102.9405, NLL-Loss  102.9405\n",
      "TRAIN Batch 0800/1314, Loss  118.1602, NLL-Loss  118.1602\n",
      "TRAIN Batch 0850/1314, Loss  112.4289, NLL-Loss  112.4289\n",
      "TRAIN Batch 0900/1314, Loss  121.2113, NLL-Loss  121.2113\n",
      "TRAIN Batch 0950/1314, Loss  101.6579, NLL-Loss  101.6579\n",
      "TRAIN Batch 1000/1314, Loss  102.7613, NLL-Loss  102.7613\n",
      "TRAIN Batch 1050/1314, Loss  113.6029, NLL-Loss  113.6029\n",
      "TRAIN Batch 1100/1314, Loss  122.8204, NLL-Loss  122.8204\n",
      "TRAIN Batch 1150/1314, Loss  115.5934, NLL-Loss  115.5934\n",
      "TRAIN Batch 1200/1314, Loss  101.8653, NLL-Loss  101.8653\n",
      "TRAIN Batch 1250/1314, Loss  107.0920, NLL-Loss  107.0920\n",
      "TRAIN Batch 1300/1314, Loss  117.7011, NLL-Loss  117.7011\n",
      "TRAIN Batch 1314/1314, Loss  114.1127, NLL-Loss  114.1127\n",
      "TRAIN Epoch 64/300, Mean NLL  106.2700\n",
      "Model saved at bin/2019-May-17-21:02:23/E64.pytorch\n",
      "VALID Batch 0000/105, Loss  133.1722, NLL-Loss  133.1722\n",
      "VALID Batch 0050/105, Loss  125.8467, NLL-Loss  125.8467\n",
      "VALID Batch 0100/105, Loss   97.6919, NLL-Loss   97.6919\n",
      "VALID Batch 0105/105, Loss   98.8899, NLL-Loss   98.8899\n",
      "VALID Epoch 64/300, Mean NLL  110.5539\n",
      "TEST Batch 0000/117, Loss  102.4706, NLL-Loss  102.4706\n",
      "TEST Batch 0050/117, Loss  116.6958, NLL-Loss  116.6958\n",
      "TEST Batch 0100/117, Loss  137.9305, NLL-Loss  137.9305\n",
      "TEST Batch 0117/117, Loss  161.9657, NLL-Loss  161.9657\n",
      "TEST Epoch 64/300, Mean NLL  110.6775\n",
      "TRAIN Batch 0000/1314, Loss  104.5542, NLL-Loss  104.5542\n",
      "TRAIN Batch 0050/1314, Loss  109.3804, NLL-Loss  109.3804\n",
      "TRAIN Batch 0100/1314, Loss   92.6862, NLL-Loss   92.6862\n",
      "TRAIN Batch 0150/1314, Loss  101.6751, NLL-Loss  101.6751\n",
      "TRAIN Batch 0200/1314, Loss  109.9244, NLL-Loss  109.9244\n",
      "TRAIN Batch 0250/1314, Loss  107.4721, NLL-Loss  107.4721\n",
      "TRAIN Batch 0300/1314, Loss  111.0904, NLL-Loss  111.0904\n",
      "TRAIN Batch 0350/1314, Loss  102.0541, NLL-Loss  102.0541\n",
      "TRAIN Batch 0400/1314, Loss   94.1870, NLL-Loss   94.1870\n",
      "TRAIN Batch 0450/1314, Loss  108.5090, NLL-Loss  108.5090\n",
      "TRAIN Batch 0500/1314, Loss  117.6829, NLL-Loss  117.6829\n",
      "TRAIN Batch 0550/1314, Loss  103.4250, NLL-Loss  103.4250\n",
      "TRAIN Batch 0600/1314, Loss   98.5069, NLL-Loss   98.5069\n",
      "TRAIN Batch 0650/1314, Loss   86.8755, NLL-Loss   86.8755\n",
      "TRAIN Batch 0700/1314, Loss   97.9179, NLL-Loss   97.9179\n",
      "TRAIN Batch 0750/1314, Loss  102.5664, NLL-Loss  102.5664\n",
      "TRAIN Batch 0800/1314, Loss  101.5687, NLL-Loss  101.5687\n",
      "TRAIN Batch 0850/1314, Loss  105.5843, NLL-Loss  105.5843\n",
      "TRAIN Batch 0900/1314, Loss  107.1618, NLL-Loss  107.1618\n",
      "TRAIN Batch 0950/1314, Loss  110.7067, NLL-Loss  110.7067\n",
      "TRAIN Batch 1000/1314, Loss  101.4201, NLL-Loss  101.4201\n",
      "TRAIN Batch 1050/1314, Loss  108.5910, NLL-Loss  108.5910\n",
      "TRAIN Batch 1100/1314, Loss  120.2401, NLL-Loss  120.2401\n",
      "TRAIN Batch 1150/1314, Loss  116.2675, NLL-Loss  116.2675\n",
      "TRAIN Batch 1200/1314, Loss  122.2360, NLL-Loss  122.2360\n",
      "TRAIN Batch 1250/1314, Loss  107.0200, NLL-Loss  107.0200\n",
      "TRAIN Batch 1300/1314, Loss  102.3740, NLL-Loss  102.3740\n",
      "TRAIN Batch 1314/1314, Loss  101.1264, NLL-Loss  101.1264\n",
      "TRAIN Epoch 65/300, Mean NLL  106.0815\n",
      "Model saved at bin/2019-May-17-21:02:23/E65.pytorch\n",
      "VALID Batch 0000/105, Loss  133.2517, NLL-Loss  133.2517\n",
      "VALID Batch 0050/105, Loss  125.7918, NLL-Loss  125.7918\n",
      "VALID Batch 0100/105, Loss   97.4022, NLL-Loss   97.4022\n",
      "VALID Batch 0105/105, Loss   98.8726, NLL-Loss   98.8726\n",
      "VALID Epoch 65/300, Mean NLL  110.4629\n",
      "TEST Batch 0000/117, Loss  102.1717, NLL-Loss  102.1717\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST Batch 0050/117, Loss  116.7396, NLL-Loss  116.7396\n",
      "TEST Batch 0100/117, Loss  137.8737, NLL-Loss  137.8737\n",
      "TEST Batch 0117/117, Loss  161.7799, NLL-Loss  161.7799\n",
      "TEST Epoch 65/300, Mean NLL  110.5847\n",
      "TRAIN Batch 0000/1314, Loss  108.2019, NLL-Loss  108.2019\n",
      "TRAIN Batch 0050/1314, Loss   98.1575, NLL-Loss   98.1575\n",
      "TRAIN Batch 0100/1314, Loss  100.2822, NLL-Loss  100.2822\n",
      "TRAIN Batch 0150/1314, Loss  123.7286, NLL-Loss  123.7286\n",
      "TRAIN Batch 0200/1314, Loss  104.3249, NLL-Loss  104.3249\n",
      "TRAIN Batch 0250/1314, Loss   98.4612, NLL-Loss   98.4612\n",
      "TRAIN Batch 0300/1314, Loss  110.1935, NLL-Loss  110.1935\n",
      "TRAIN Batch 0350/1314, Loss  116.2043, NLL-Loss  116.2043\n",
      "TRAIN Batch 0400/1314, Loss   98.3364, NLL-Loss   98.3364\n",
      "TRAIN Batch 0450/1314, Loss  110.7626, NLL-Loss  110.7626\n",
      "TRAIN Batch 0500/1314, Loss  100.1954, NLL-Loss  100.1954\n",
      "TRAIN Batch 0550/1314, Loss  107.7777, NLL-Loss  107.7777\n",
      "TRAIN Batch 0600/1314, Loss  103.0093, NLL-Loss  103.0093\n",
      "TRAIN Batch 0650/1314, Loss  119.2802, NLL-Loss  119.2802\n",
      "TRAIN Batch 0700/1314, Loss   97.2401, NLL-Loss   97.2401\n",
      "TRAIN Batch 0750/1314, Loss   93.2269, NLL-Loss   93.2269\n",
      "TRAIN Batch 0800/1314, Loss  102.0747, NLL-Loss  102.0747\n",
      "TRAIN Batch 0850/1314, Loss  110.0564, NLL-Loss  110.0564\n",
      "TRAIN Batch 0900/1314, Loss   99.7743, NLL-Loss   99.7743\n",
      "TRAIN Batch 0950/1314, Loss  110.6521, NLL-Loss  110.6521\n",
      "TRAIN Batch 1000/1314, Loss  104.5522, NLL-Loss  104.5522\n",
      "TRAIN Batch 1050/1314, Loss   96.7415, NLL-Loss   96.7415\n",
      "TRAIN Batch 1100/1314, Loss  103.5805, NLL-Loss  103.5805\n",
      "TRAIN Batch 1150/1314, Loss  109.3895, NLL-Loss  109.3895\n",
      "TRAIN Batch 1200/1314, Loss  107.6621, NLL-Loss  107.6621\n",
      "TRAIN Batch 1250/1314, Loss  101.8534, NLL-Loss  101.8534\n",
      "TRAIN Batch 1300/1314, Loss  106.0137, NLL-Loss  106.0137\n",
      "TRAIN Batch 1314/1314, Loss   94.2521, NLL-Loss   94.2521\n",
      "TRAIN Epoch 66/300, Mean NLL  105.9441\n",
      "Model saved at bin/2019-May-17-21:02:23/E66.pytorch\n",
      "VALID Batch 0000/105, Loss  133.4160, NLL-Loss  133.4160\n",
      "VALID Batch 0050/105, Loss  125.5289, NLL-Loss  125.5289\n",
      "VALID Batch 0100/105, Loss   97.3805, NLL-Loss   97.3805\n",
      "VALID Batch 0105/105, Loss   98.8619, NLL-Loss   98.8619\n",
      "VALID Epoch 66/300, Mean NLL  110.4617\n",
      "TEST Batch 0000/117, Loss  102.2382, NLL-Loss  102.2382\n",
      "TEST Batch 0050/117, Loss  116.5684, NLL-Loss  116.5684\n",
      "TEST Batch 0100/117, Loss  137.9257, NLL-Loss  137.9257\n",
      "TEST Batch 0117/117, Loss  162.2319, NLL-Loss  162.2319\n",
      "TEST Epoch 66/300, Mean NLL  110.5647\n",
      "TRAIN Batch 0000/1314, Loss  106.7264, NLL-Loss  106.7264\n",
      "TRAIN Batch 0050/1314, Loss  112.5261, NLL-Loss  112.5261\n",
      "TRAIN Batch 0100/1314, Loss  107.3414, NLL-Loss  107.3414\n",
      "TRAIN Batch 0150/1314, Loss  116.4931, NLL-Loss  116.4931\n",
      "TRAIN Batch 0200/1314, Loss  110.9261, NLL-Loss  110.9261\n",
      "TRAIN Batch 0250/1314, Loss   95.2285, NLL-Loss   95.2285\n",
      "TRAIN Batch 0300/1314, Loss  113.7130, NLL-Loss  113.7130\n",
      "TRAIN Batch 0350/1314, Loss  107.7190, NLL-Loss  107.7190\n",
      "TRAIN Batch 0400/1314, Loss  116.4769, NLL-Loss  116.4769\n",
      "TRAIN Batch 0450/1314, Loss  104.0981, NLL-Loss  104.0981\n",
      "TRAIN Batch 0500/1314, Loss   95.7841, NLL-Loss   95.7841\n",
      "TRAIN Batch 0550/1314, Loss  104.2212, NLL-Loss  104.2212\n",
      "TRAIN Batch 0600/1314, Loss  109.0251, NLL-Loss  109.0251\n",
      "TRAIN Batch 0650/1314, Loss   99.0908, NLL-Loss   99.0908\n",
      "TRAIN Batch 0700/1314, Loss  105.7642, NLL-Loss  105.7642\n",
      "TRAIN Batch 0750/1314, Loss  113.0292, NLL-Loss  113.0292\n",
      "TRAIN Batch 0800/1314, Loss  105.3770, NLL-Loss  105.3770\n",
      "TRAIN Batch 0850/1314, Loss  117.2345, NLL-Loss  117.2345\n",
      "TRAIN Batch 0900/1314, Loss  107.8415, NLL-Loss  107.8415\n",
      "TRAIN Batch 0950/1314, Loss  106.9230, NLL-Loss  106.9230\n",
      "TRAIN Batch 1000/1314, Loss  103.1046, NLL-Loss  103.1046\n",
      "TRAIN Batch 1050/1314, Loss  112.7992, NLL-Loss  112.7992\n",
      "TRAIN Batch 1100/1314, Loss  104.2029, NLL-Loss  104.2029\n",
      "TRAIN Batch 1150/1314, Loss  100.2793, NLL-Loss  100.2793\n",
      "TRAIN Batch 1200/1314, Loss  101.0286, NLL-Loss  101.0286\n",
      "TRAIN Batch 1250/1314, Loss  100.2222, NLL-Loss  100.2222\n",
      "TRAIN Batch 1300/1314, Loss  120.5050, NLL-Loss  120.5050\n",
      "TRAIN Batch 1314/1314, Loss  109.8825, NLL-Loss  109.8825\n",
      "TRAIN Epoch 67/300, Mean NLL  105.7989\n",
      "Model saved at bin/2019-May-17-21:02:23/E67.pytorch\n",
      "VALID Batch 0000/105, Loss  133.1324, NLL-Loss  133.1324\n",
      "VALID Batch 0050/105, Loss  125.6738, NLL-Loss  125.6738\n",
      "VALID Batch 0100/105, Loss   97.6048, NLL-Loss   97.6048\n",
      "VALID Batch 0105/105, Loss   98.7876, NLL-Loss   98.7876\n",
      "VALID Epoch 67/300, Mean NLL  110.3661\n",
      "TEST Batch 0000/117, Loss  102.2478, NLL-Loss  102.2478\n",
      "TEST Batch 0050/117, Loss  116.5118, NLL-Loss  116.5118\n",
      "TEST Batch 0100/117, Loss  137.6087, NLL-Loss  137.6087\n",
      "TEST Batch 0117/117, Loss  161.8640, NLL-Loss  161.8640\n",
      "TEST Epoch 67/300, Mean NLL  110.4664\n",
      "TRAIN Batch 0000/1314, Loss  107.6280, NLL-Loss  107.6280\n",
      "TRAIN Batch 0050/1314, Loss   96.2142, NLL-Loss   96.2142\n",
      "TRAIN Batch 0100/1314, Loss  110.2422, NLL-Loss  110.2422\n",
      "TRAIN Batch 0150/1314, Loss   95.2423, NLL-Loss   95.2423\n",
      "TRAIN Batch 0200/1314, Loss  118.9066, NLL-Loss  118.9066\n",
      "TRAIN Batch 0250/1314, Loss   96.5766, NLL-Loss   96.5766\n",
      "TRAIN Batch 0300/1314, Loss   91.5197, NLL-Loss   91.5197\n",
      "TRAIN Batch 0350/1314, Loss  116.3543, NLL-Loss  116.3543\n",
      "TRAIN Batch 0400/1314, Loss  121.2842, NLL-Loss  121.2842\n",
      "TRAIN Batch 0450/1314, Loss   95.0148, NLL-Loss   95.0148\n",
      "TRAIN Batch 0500/1314, Loss   92.6335, NLL-Loss   92.6335\n",
      "TRAIN Batch 0550/1314, Loss  102.7291, NLL-Loss  102.7291\n",
      "TRAIN Batch 0600/1314, Loss   94.7646, NLL-Loss   94.7646\n",
      "TRAIN Batch 0650/1314, Loss   86.9527, NLL-Loss   86.9527\n",
      "TRAIN Batch 0700/1314, Loss  107.6309, NLL-Loss  107.6309\n",
      "TRAIN Batch 0750/1314, Loss  126.2405, NLL-Loss  126.2405\n",
      "TRAIN Batch 0800/1314, Loss  106.0486, NLL-Loss  106.0486\n",
      "TRAIN Batch 0850/1314, Loss  111.5265, NLL-Loss  111.5265\n",
      "TRAIN Batch 0900/1314, Loss   94.4057, NLL-Loss   94.4057\n",
      "TRAIN Batch 0950/1314, Loss   96.8245, NLL-Loss   96.8245\n",
      "TRAIN Batch 1000/1314, Loss  113.6404, NLL-Loss  113.6404\n",
      "TRAIN Batch 1050/1314, Loss  104.6823, NLL-Loss  104.6823\n",
      "TRAIN Batch 1100/1314, Loss  121.2145, NLL-Loss  121.2145\n",
      "TRAIN Batch 1150/1314, Loss  114.0379, NLL-Loss  114.0379\n",
      "TRAIN Batch 1200/1314, Loss   93.7254, NLL-Loss   93.7254\n",
      "TRAIN Batch 1250/1314, Loss  110.2440, NLL-Loss  110.2440\n",
      "TRAIN Batch 1300/1314, Loss  116.4945, NLL-Loss  116.4945\n",
      "TRAIN Batch 1314/1314, Loss   81.7338, NLL-Loss   81.7338\n",
      "TRAIN Epoch 68/300, Mean NLL  105.6485\n",
      "Model saved at bin/2019-May-17-21:02:23/E68.pytorch\n",
      "VALID Batch 0000/105, Loss  133.2278, NLL-Loss  133.2278\n",
      "VALID Batch 0050/105, Loss  125.4056, NLL-Loss  125.4056\n",
      "VALID Batch 0100/105, Loss   97.5263, NLL-Loss   97.5263\n",
      "VALID Batch 0105/105, Loss   98.8708, NLL-Loss   98.8708\n",
      "VALID Epoch 68/300, Mean NLL  110.3534\n",
      "TEST Batch 0000/117, Loss  102.0994, NLL-Loss  102.0994\n",
      "TEST Batch 0050/117, Loss  116.5760, NLL-Loss  116.5760\n",
      "TEST Batch 0100/117, Loss  137.7694, NLL-Loss  137.7694\n",
      "TEST Batch 0117/117, Loss  162.1176, NLL-Loss  162.1176\n",
      "TEST Epoch 68/300, Mean NLL  110.4581\n",
      "TRAIN Batch 0000/1314, Loss  106.3661, NLL-Loss  106.3661\n",
      "TRAIN Batch 0050/1314, Loss  118.7383, NLL-Loss  118.7383\n",
      "TRAIN Batch 0100/1314, Loss   86.3502, NLL-Loss   86.3502\n",
      "TRAIN Batch 0150/1314, Loss   93.9397, NLL-Loss   93.9397\n",
      "TRAIN Batch 0200/1314, Loss  105.8848, NLL-Loss  105.8848\n",
      "TRAIN Batch 0250/1314, Loss  111.8028, NLL-Loss  111.8028\n",
      "TRAIN Batch 0300/1314, Loss  110.9122, NLL-Loss  110.9122\n",
      "TRAIN Batch 0350/1314, Loss  101.8023, NLL-Loss  101.8023\n",
      "TRAIN Batch 0400/1314, Loss   95.0613, NLL-Loss   95.0613\n",
      "TRAIN Batch 0450/1314, Loss   97.2204, NLL-Loss   97.2204\n",
      "TRAIN Batch 0500/1314, Loss  113.6140, NLL-Loss  113.6140\n",
      "TRAIN Batch 0550/1314, Loss  124.4293, NLL-Loss  124.4293\n",
      "TRAIN Batch 0600/1314, Loss   99.5110, NLL-Loss   99.5110\n",
      "TRAIN Batch 0650/1314, Loss   97.1945, NLL-Loss   97.1945\n",
      "TRAIN Batch 0700/1314, Loss  102.8696, NLL-Loss  102.8696\n",
      "TRAIN Batch 0750/1314, Loss  100.7908, NLL-Loss  100.7908\n",
      "TRAIN Batch 0800/1314, Loss  106.9123, NLL-Loss  106.9123\n",
      "TRAIN Batch 0850/1314, Loss  105.5820, NLL-Loss  105.5820\n",
      "TRAIN Batch 0900/1314, Loss  106.3688, NLL-Loss  106.3688\n",
      "TRAIN Batch 0950/1314, Loss   98.2048, NLL-Loss   98.2048\n",
      "TRAIN Batch 1000/1314, Loss  109.0702, NLL-Loss  109.0702\n",
      "TRAIN Batch 1050/1314, Loss  104.5852, NLL-Loss  104.5852\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN Batch 1100/1314, Loss  102.2834, NLL-Loss  102.2834\n",
      "TRAIN Batch 1150/1314, Loss  107.5475, NLL-Loss  107.5475\n",
      "TRAIN Batch 1200/1314, Loss  106.6199, NLL-Loss  106.6199\n",
      "TRAIN Batch 1250/1314, Loss  103.6476, NLL-Loss  103.6476\n",
      "TRAIN Batch 1300/1314, Loss  105.1075, NLL-Loss  105.1075\n",
      "TRAIN Batch 1314/1314, Loss  110.8854, NLL-Loss  110.8854\n",
      "TRAIN Epoch 69/300, Mean NLL  105.5216\n",
      "Model saved at bin/2019-May-17-21:02:23/E69.pytorch\n",
      "VALID Batch 0000/105, Loss  132.9962, NLL-Loss  132.9962\n",
      "VALID Batch 0050/105, Loss  125.0819, NLL-Loss  125.0819\n",
      "VALID Batch 0100/105, Loss   97.2048, NLL-Loss   97.2048\n",
      "VALID Batch 0105/105, Loss   98.6678, NLL-Loss   98.6678\n",
      "VALID Epoch 69/300, Mean NLL  110.1433\n",
      "TEST Batch 0000/117, Loss  101.7955, NLL-Loss  101.7955\n",
      "TEST Batch 0050/117, Loss  116.5695, NLL-Loss  116.5695\n",
      "TEST Batch 0100/117, Loss  137.5600, NLL-Loss  137.5600\n",
      "TEST Batch 0117/117, Loss  161.5491, NLL-Loss  161.5491\n",
      "TEST Epoch 69/300, Mean NLL  110.2761\n",
      "TRAIN Batch 0000/1314, Loss  110.3502, NLL-Loss  110.3502\n",
      "TRAIN Batch 0050/1314, Loss  104.5634, NLL-Loss  104.5634\n",
      "TRAIN Batch 0100/1314, Loss  111.8189, NLL-Loss  111.8189\n",
      "TRAIN Batch 0150/1314, Loss  104.7609, NLL-Loss  104.7609\n",
      "TRAIN Batch 0200/1314, Loss  105.1102, NLL-Loss  105.1102\n",
      "TRAIN Batch 0250/1314, Loss  108.4747, NLL-Loss  108.4747\n",
      "TRAIN Batch 0300/1314, Loss  113.7755, NLL-Loss  113.7755\n",
      "TRAIN Batch 0350/1314, Loss  113.8220, NLL-Loss  113.8220\n",
      "TRAIN Batch 0400/1314, Loss  108.6668, NLL-Loss  108.6668\n",
      "TRAIN Batch 0450/1314, Loss  108.4623, NLL-Loss  108.4623\n",
      "TRAIN Batch 0500/1314, Loss  100.9402, NLL-Loss  100.9402\n",
      "TRAIN Batch 0550/1314, Loss  104.0626, NLL-Loss  104.0626\n",
      "TRAIN Batch 0600/1314, Loss  105.2240, NLL-Loss  105.2240\n",
      "TRAIN Batch 0650/1314, Loss   91.7679, NLL-Loss   91.7679\n",
      "TRAIN Batch 0700/1314, Loss  117.0833, NLL-Loss  117.0833\n",
      "TRAIN Batch 0750/1314, Loss  104.7465, NLL-Loss  104.7465\n",
      "TRAIN Batch 0800/1314, Loss  116.0601, NLL-Loss  116.0601\n",
      "TRAIN Batch 0850/1314, Loss  120.8721, NLL-Loss  120.8721\n",
      "TRAIN Batch 0900/1314, Loss  112.5833, NLL-Loss  112.5833\n",
      "TRAIN Batch 0950/1314, Loss  100.2485, NLL-Loss  100.2485\n",
      "TRAIN Batch 1000/1314, Loss  102.1392, NLL-Loss  102.1392\n",
      "TRAIN Batch 1050/1314, Loss   93.6119, NLL-Loss   93.6119\n",
      "TRAIN Batch 1100/1314, Loss  108.8991, NLL-Loss  108.8991\n",
      "TRAIN Batch 1150/1314, Loss  104.8488, NLL-Loss  104.8488\n",
      "TRAIN Batch 1200/1314, Loss   92.9901, NLL-Loss   92.9901\n",
      "TRAIN Batch 1250/1314, Loss  105.6007, NLL-Loss  105.6007\n",
      "TRAIN Batch 1300/1314, Loss   84.3991, NLL-Loss   84.3991\n",
      "TRAIN Batch 1314/1314, Loss  109.8883, NLL-Loss  109.8883\n",
      "TRAIN Epoch 70/300, Mean NLL  105.3510\n",
      "Model saved at bin/2019-May-17-21:02:23/E70.pytorch\n",
      "VALID Batch 0000/105, Loss  133.0023, NLL-Loss  133.0023\n",
      "VALID Batch 0050/105, Loss  124.9366, NLL-Loss  124.9366\n",
      "VALID Batch 0100/105, Loss   97.3817, NLL-Loss   97.3817\n",
      "VALID Batch 0105/105, Loss   98.7631, NLL-Loss   98.7631\n",
      "VALID Epoch 70/300, Mean NLL  110.1806\n",
      "TEST Batch 0000/117, Loss  101.9263, NLL-Loss  101.9263\n",
      "TEST Batch 0050/117, Loss  116.4482, NLL-Loss  116.4482\n",
      "TEST Batch 0100/117, Loss  137.6014, NLL-Loss  137.6014\n",
      "TEST Batch 0117/117, Loss  162.0766, NLL-Loss  162.0766\n",
      "TEST Epoch 70/300, Mean NLL  110.2794\n",
      "TRAIN Batch 0000/1314, Loss  100.0695, NLL-Loss  100.0695\n",
      "TRAIN Batch 0050/1314, Loss  109.7055, NLL-Loss  109.7055\n",
      "TRAIN Batch 0100/1314, Loss   93.0849, NLL-Loss   93.0849\n",
      "TRAIN Batch 0150/1314, Loss  113.9826, NLL-Loss  113.9826\n",
      "TRAIN Batch 0200/1314, Loss  111.8598, NLL-Loss  111.8598\n",
      "TRAIN Batch 0250/1314, Loss  111.5584, NLL-Loss  111.5584\n",
      "TRAIN Batch 0300/1314, Loss  100.8003, NLL-Loss  100.8003\n",
      "TRAIN Batch 0350/1314, Loss   94.5947, NLL-Loss   94.5947\n",
      "TRAIN Batch 0400/1314, Loss  127.3393, NLL-Loss  127.3393\n",
      "TRAIN Batch 0450/1314, Loss  121.3699, NLL-Loss  121.3699\n",
      "TRAIN Batch 0500/1314, Loss  114.7887, NLL-Loss  114.7887\n",
      "TRAIN Batch 0550/1314, Loss   95.9185, NLL-Loss   95.9185\n",
      "TRAIN Batch 0600/1314, Loss  114.2234, NLL-Loss  114.2234\n",
      "TRAIN Batch 0650/1314, Loss   94.6187, NLL-Loss   94.6187\n",
      "TRAIN Batch 0700/1314, Loss  100.7878, NLL-Loss  100.7878\n",
      "TRAIN Batch 0750/1314, Loss  104.6084, NLL-Loss  104.6084\n",
      "TRAIN Batch 0800/1314, Loss   99.6337, NLL-Loss   99.6337\n",
      "TRAIN Batch 0850/1314, Loss  112.2207, NLL-Loss  112.2207\n",
      "TRAIN Batch 0900/1314, Loss  113.6324, NLL-Loss  113.6324\n",
      "TRAIN Batch 0950/1314, Loss   99.8514, NLL-Loss   99.8514\n",
      "TRAIN Batch 1000/1314, Loss  112.3363, NLL-Loss  112.3363\n",
      "TRAIN Batch 1050/1314, Loss  111.7970, NLL-Loss  111.7970\n",
      "TRAIN Batch 1100/1314, Loss  109.1853, NLL-Loss  109.1853\n",
      "TRAIN Batch 1150/1314, Loss   94.6517, NLL-Loss   94.6517\n",
      "TRAIN Batch 1200/1314, Loss  107.2692, NLL-Loss  107.2692\n",
      "TRAIN Batch 1250/1314, Loss   99.9940, NLL-Loss   99.9940\n",
      "TRAIN Batch 1300/1314, Loss  105.5008, NLL-Loss  105.5008\n",
      "TRAIN Batch 1314/1314, Loss  119.7648, NLL-Loss  119.7648\n",
      "TRAIN Epoch 71/300, Mean NLL  105.2130\n",
      "Model saved at bin/2019-May-17-21:02:23/E71.pytorch\n",
      "VALID Batch 0000/105, Loss  133.0309, NLL-Loss  133.0309\n",
      "VALID Batch 0050/105, Loss  124.9539, NLL-Loss  124.9539\n",
      "VALID Batch 0100/105, Loss   97.2277, NLL-Loss   97.2277\n",
      "VALID Batch 0105/105, Loss   98.5925, NLL-Loss   98.5925\n",
      "VALID Epoch 71/300, Mean NLL  110.0727\n",
      "TEST Batch 0000/117, Loss  101.5981, NLL-Loss  101.5981\n",
      "TEST Batch 0050/117, Loss  116.4661, NLL-Loss  116.4661\n",
      "TEST Batch 0100/117, Loss  137.7248, NLL-Loss  137.7248\n",
      "TEST Batch 0117/117, Loss  161.4878, NLL-Loss  161.4878\n",
      "TEST Epoch 71/300, Mean NLL  110.2102\n",
      "TRAIN Batch 0000/1314, Loss  101.9341, NLL-Loss  101.9341\n",
      "TRAIN Batch 0050/1314, Loss  120.6948, NLL-Loss  120.6948\n",
      "TRAIN Batch 0100/1314, Loss  120.1623, NLL-Loss  120.1623\n",
      "TRAIN Batch 0150/1314, Loss  107.7952, NLL-Loss  107.7952\n",
      "TRAIN Batch 0200/1314, Loss  106.4045, NLL-Loss  106.4045\n",
      "TRAIN Batch 0250/1314, Loss   99.1242, NLL-Loss   99.1242\n",
      "TRAIN Batch 0300/1314, Loss  108.3848, NLL-Loss  108.3848\n",
      "TRAIN Batch 0350/1314, Loss  113.0355, NLL-Loss  113.0355\n",
      "TRAIN Batch 0400/1314, Loss  105.2680, NLL-Loss  105.2680\n",
      "TRAIN Batch 0450/1314, Loss  106.9297, NLL-Loss  106.9297\n",
      "TRAIN Batch 0500/1314, Loss  112.8905, NLL-Loss  112.8905\n",
      "TRAIN Batch 0550/1314, Loss  112.7775, NLL-Loss  112.7775\n",
      "TRAIN Batch 0600/1314, Loss  107.7794, NLL-Loss  107.7794\n",
      "TRAIN Batch 0650/1314, Loss  110.6618, NLL-Loss  110.6618\n",
      "TRAIN Batch 0700/1314, Loss   95.0675, NLL-Loss   95.0675\n",
      "TRAIN Batch 0750/1314, Loss   92.2672, NLL-Loss   92.2672\n",
      "TRAIN Batch 0800/1314, Loss   99.2858, NLL-Loss   99.2858\n",
      "TRAIN Batch 0850/1314, Loss   95.1457, NLL-Loss   95.1457\n",
      "TRAIN Batch 0900/1314, Loss   81.3673, NLL-Loss   81.3673\n",
      "TRAIN Batch 0950/1314, Loss  123.9060, NLL-Loss  123.9060\n",
      "TRAIN Batch 1000/1314, Loss  102.3105, NLL-Loss  102.3105\n",
      "TRAIN Batch 1050/1314, Loss  101.7420, NLL-Loss  101.7420\n",
      "TRAIN Batch 1100/1314, Loss  101.5376, NLL-Loss  101.5376\n",
      "TRAIN Batch 1150/1314, Loss  107.4552, NLL-Loss  107.4552\n",
      "TRAIN Batch 1200/1314, Loss  111.8628, NLL-Loss  111.8628\n",
      "TRAIN Batch 1250/1314, Loss  113.8471, NLL-Loss  113.8471\n",
      "TRAIN Batch 1300/1314, Loss   96.1876, NLL-Loss   96.1876\n",
      "TRAIN Batch 1314/1314, Loss  106.4072, NLL-Loss  106.4072\n",
      "TRAIN Epoch 72/300, Mean NLL  105.0745\n",
      "Model saved at bin/2019-May-17-21:02:23/E72.pytorch\n",
      "VALID Batch 0000/105, Loss  133.2424, NLL-Loss  133.2424\n",
      "VALID Batch 0050/105, Loss  125.0293, NLL-Loss  125.0293\n",
      "VALID Batch 0100/105, Loss   97.1522, NLL-Loss   97.1522\n",
      "VALID Batch 0105/105, Loss   98.7796, NLL-Loss   98.7796\n",
      "VALID Epoch 72/300, Mean NLL  110.0556\n",
      "TEST Batch 0000/117, Loss  101.6372, NLL-Loss  101.6372\n",
      "TEST Batch 0050/117, Loss  116.6134, NLL-Loss  116.6134\n",
      "TEST Batch 0100/117, Loss  137.7452, NLL-Loss  137.7452\n",
      "TEST Batch 0117/117, Loss  161.6062, NLL-Loss  161.6062\n",
      "TEST Epoch 72/300, Mean NLL  110.2032\n",
      "TRAIN Batch 0000/1314, Loss  116.1147, NLL-Loss  116.1147\n",
      "TRAIN Batch 0050/1314, Loss  102.9812, NLL-Loss  102.9812\n",
      "TRAIN Batch 0100/1314, Loss   88.6375, NLL-Loss   88.6375\n",
      "TRAIN Batch 0150/1314, Loss   98.5753, NLL-Loss   98.5753\n",
      "TRAIN Batch 0200/1314, Loss  104.0273, NLL-Loss  104.0273\n",
      "TRAIN Batch 0250/1314, Loss   94.1874, NLL-Loss   94.1874\n",
      "TRAIN Batch 0300/1314, Loss   97.3043, NLL-Loss   97.3043\n",
      "TRAIN Batch 0350/1314, Loss   98.1574, NLL-Loss   98.1574\n",
      "TRAIN Batch 0400/1314, Loss  122.7710, NLL-Loss  122.7710\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN Batch 0450/1314, Loss   99.1499, NLL-Loss   99.1499\n",
      "TRAIN Batch 0500/1314, Loss  110.5379, NLL-Loss  110.5379\n",
      "TRAIN Batch 0550/1314, Loss   96.6710, NLL-Loss   96.6710\n",
      "TRAIN Batch 0600/1314, Loss  106.7548, NLL-Loss  106.7548\n",
      "TRAIN Batch 0650/1314, Loss  106.9872, NLL-Loss  106.9872\n",
      "TRAIN Batch 0700/1314, Loss  100.8891, NLL-Loss  100.8891\n",
      "TRAIN Batch 0750/1314, Loss  109.6793, NLL-Loss  109.6793\n",
      "TRAIN Batch 0800/1314, Loss  101.1394, NLL-Loss  101.1394\n",
      "TRAIN Batch 0850/1314, Loss  114.3623, NLL-Loss  114.3623\n",
      "TRAIN Batch 0900/1314, Loss   90.5176, NLL-Loss   90.5176\n",
      "TRAIN Batch 0950/1314, Loss   94.1810, NLL-Loss   94.1810\n",
      "TRAIN Batch 1000/1314, Loss  101.4712, NLL-Loss  101.4712\n",
      "TRAIN Batch 1050/1314, Loss  108.9761, NLL-Loss  108.9761\n",
      "TRAIN Batch 1100/1314, Loss   96.9541, NLL-Loss   96.9541\n",
      "TRAIN Batch 1150/1314, Loss   96.0032, NLL-Loss   96.0032\n",
      "TRAIN Batch 1200/1314, Loss   95.1740, NLL-Loss   95.1740\n",
      "TRAIN Batch 1250/1314, Loss  109.7218, NLL-Loss  109.7218\n",
      "TRAIN Batch 1300/1314, Loss   99.4529, NLL-Loss   99.4529\n",
      "TRAIN Batch 1314/1314, Loss  108.0261, NLL-Loss  108.0261\n",
      "TRAIN Epoch 73/300, Mean NLL  104.9449\n",
      "Model saved at bin/2019-May-17-21:02:23/E73.pytorch\n",
      "VALID Batch 0000/105, Loss  132.9200, NLL-Loss  132.9200\n",
      "VALID Batch 0050/105, Loss  124.8367, NLL-Loss  124.8367\n",
      "VALID Batch 0100/105, Loss   97.1350, NLL-Loss   97.1350\n",
      "VALID Batch 0105/105, Loss   98.6021, NLL-Loss   98.6021\n",
      "VALID Epoch 73/300, Mean NLL  109.9682\n",
      "TEST Batch 0000/117, Loss  101.4764, NLL-Loss  101.4764\n",
      "TEST Batch 0050/117, Loss  116.4121, NLL-Loss  116.4121\n",
      "TEST Batch 0100/117, Loss  137.4725, NLL-Loss  137.4725\n",
      "TEST Batch 0117/117, Loss  161.3927, NLL-Loss  161.3927\n",
      "TEST Epoch 73/300, Mean NLL  110.1213\n",
      "TRAIN Batch 0000/1314, Loss  110.8450, NLL-Loss  110.8450\n",
      "TRAIN Batch 0050/1314, Loss  101.3632, NLL-Loss  101.3632\n",
      "TRAIN Batch 0100/1314, Loss  118.3815, NLL-Loss  118.3815\n",
      "TRAIN Batch 0150/1314, Loss  105.9168, NLL-Loss  105.9168\n",
      "TRAIN Batch 0200/1314, Loss  118.4692, NLL-Loss  118.4692\n",
      "TRAIN Batch 0250/1314, Loss   91.7048, NLL-Loss   91.7048\n",
      "TRAIN Batch 0300/1314, Loss   99.3984, NLL-Loss   99.3984\n",
      "TRAIN Batch 0350/1314, Loss  117.5654, NLL-Loss  117.5654\n",
      "TRAIN Batch 0400/1314, Loss  102.6054, NLL-Loss  102.6054\n",
      "TRAIN Batch 0450/1314, Loss  109.0830, NLL-Loss  109.0830\n",
      "TRAIN Batch 0500/1314, Loss   92.4321, NLL-Loss   92.4321\n",
      "TRAIN Batch 0550/1314, Loss  115.2432, NLL-Loss  115.2432\n",
      "TRAIN Batch 0600/1314, Loss  121.3602, NLL-Loss  121.3602\n",
      "TRAIN Batch 0650/1314, Loss  118.3721, NLL-Loss  118.3721\n",
      "TRAIN Batch 0700/1314, Loss   93.9803, NLL-Loss   93.9803\n",
      "TRAIN Batch 0750/1314, Loss  104.3918, NLL-Loss  104.3918\n",
      "TRAIN Batch 0800/1314, Loss   91.4209, NLL-Loss   91.4209\n",
      "TRAIN Batch 0850/1314, Loss  120.3671, NLL-Loss  120.3671\n",
      "TRAIN Batch 0900/1314, Loss  105.6752, NLL-Loss  105.6752\n",
      "TRAIN Batch 0950/1314, Loss   90.9779, NLL-Loss   90.9779\n",
      "TRAIN Batch 1000/1314, Loss  103.3522, NLL-Loss  103.3522\n",
      "TRAIN Batch 1050/1314, Loss  113.0867, NLL-Loss  113.0867\n",
      "TRAIN Batch 1100/1314, Loss  105.6617, NLL-Loss  105.6617\n",
      "TRAIN Batch 1150/1314, Loss   94.6600, NLL-Loss   94.6600\n",
      "TRAIN Batch 1200/1314, Loss  105.7773, NLL-Loss  105.7773\n",
      "TRAIN Batch 1250/1314, Loss  118.2557, NLL-Loss  118.2557\n",
      "TRAIN Batch 1300/1314, Loss   99.9467, NLL-Loss   99.9467\n",
      "TRAIN Batch 1314/1314, Loss  101.6435, NLL-Loss  101.6435\n",
      "TRAIN Epoch 74/300, Mean NLL  104.8021\n",
      "Model saved at bin/2019-May-17-21:02:23/E74.pytorch\n",
      "VALID Batch 0000/105, Loss  132.8956, NLL-Loss  132.8956\n",
      "VALID Batch 0050/105, Loss  124.7157, NLL-Loss  124.7157\n",
      "VALID Batch 0100/105, Loss   97.1567, NLL-Loss   97.1567\n",
      "VALID Batch 0105/105, Loss   98.5333, NLL-Loss   98.5333\n",
      "VALID Epoch 74/300, Mean NLL  109.9072\n",
      "TEST Batch 0000/117, Loss  101.5118, NLL-Loss  101.5118\n",
      "TEST Batch 0050/117, Loss  116.2295, NLL-Loss  116.2295\n",
      "TEST Batch 0100/117, Loss  137.3451, NLL-Loss  137.3451\n",
      "TEST Batch 0117/117, Loss  161.4535, NLL-Loss  161.4535\n",
      "TEST Epoch 74/300, Mean NLL  110.0127\n",
      "TRAIN Batch 0000/1314, Loss  103.5531, NLL-Loss  103.5531\n",
      "TRAIN Batch 0050/1314, Loss   97.8434, NLL-Loss   97.8434\n",
      "TRAIN Batch 0100/1314, Loss  100.2655, NLL-Loss  100.2655\n",
      "TRAIN Batch 0150/1314, Loss  112.0695, NLL-Loss  112.0695\n",
      "TRAIN Batch 0200/1314, Loss  107.1367, NLL-Loss  107.1367\n",
      "TRAIN Batch 0250/1314, Loss  118.8771, NLL-Loss  118.8771\n",
      "TRAIN Batch 0300/1314, Loss  107.5949, NLL-Loss  107.5949\n",
      "TRAIN Batch 0350/1314, Loss   89.6486, NLL-Loss   89.6486\n",
      "TRAIN Batch 0400/1314, Loss  119.9775, NLL-Loss  119.9775\n",
      "TRAIN Batch 0450/1314, Loss  111.7522, NLL-Loss  111.7522\n",
      "TRAIN Batch 0500/1314, Loss  102.0304, NLL-Loss  102.0304\n",
      "TRAIN Batch 0550/1314, Loss   99.8650, NLL-Loss   99.8650\n",
      "TRAIN Batch 0600/1314, Loss  115.0303, NLL-Loss  115.0303\n",
      "TRAIN Batch 0650/1314, Loss  102.3659, NLL-Loss  102.3659\n",
      "TRAIN Batch 0700/1314, Loss  115.4480, NLL-Loss  115.4480\n",
      "TRAIN Batch 0750/1314, Loss  106.4362, NLL-Loss  106.4362\n",
      "TRAIN Batch 0800/1314, Loss   92.6154, NLL-Loss   92.6154\n",
      "TRAIN Batch 0850/1314, Loss  120.1976, NLL-Loss  120.1976\n",
      "TRAIN Batch 0900/1314, Loss   82.6949, NLL-Loss   82.6949\n",
      "TRAIN Batch 0950/1314, Loss   86.5942, NLL-Loss   86.5942\n",
      "TRAIN Batch 1000/1314, Loss  100.7664, NLL-Loss  100.7664\n",
      "TRAIN Batch 1050/1314, Loss  116.6771, NLL-Loss  116.6771\n",
      "TRAIN Batch 1100/1314, Loss  116.9672, NLL-Loss  116.9672\n",
      "TRAIN Batch 1150/1314, Loss  103.0980, NLL-Loss  103.0980\n",
      "TRAIN Batch 1200/1314, Loss  104.0264, NLL-Loss  104.0264\n",
      "TRAIN Batch 1250/1314, Loss  108.7905, NLL-Loss  108.7905\n",
      "TRAIN Batch 1300/1314, Loss  112.4501, NLL-Loss  112.4501\n",
      "TRAIN Batch 1314/1314, Loss  102.4395, NLL-Loss  102.4395\n",
      "TRAIN Epoch 75/300, Mean NLL  104.6460\n",
      "Model saved at bin/2019-May-17-21:02:23/E75.pytorch\n",
      "VALID Batch 0000/105, Loss  133.0972, NLL-Loss  133.0972\n",
      "VALID Batch 0050/105, Loss  124.6056, NLL-Loss  124.6056\n",
      "VALID Batch 0100/105, Loss   96.9417, NLL-Loss   96.9417\n",
      "VALID Batch 0105/105, Loss   98.5025, NLL-Loss   98.5025\n",
      "VALID Epoch 75/300, Mean NLL  109.8764\n",
      "TEST Batch 0000/117, Loss  101.3812, NLL-Loss  101.3812\n",
      "TEST Batch 0050/117, Loss  116.4627, NLL-Loss  116.4627\n",
      "TEST Batch 0100/117, Loss  137.5640, NLL-Loss  137.5640\n",
      "TEST Batch 0117/117, Loss  161.5058, NLL-Loss  161.5058\n",
      "TEST Epoch 75/300, Mean NLL  110.0250\n",
      "TRAIN Batch 0000/1314, Loss   91.7945, NLL-Loss   91.7945\n",
      "TRAIN Batch 0050/1314, Loss   91.0384, NLL-Loss   91.0384\n",
      "TRAIN Batch 0100/1314, Loss  107.5967, NLL-Loss  107.5967\n",
      "TRAIN Batch 0150/1314, Loss  123.0891, NLL-Loss  123.0891\n",
      "TRAIN Batch 0200/1314, Loss   95.6389, NLL-Loss   95.6389\n",
      "TRAIN Batch 0250/1314, Loss  107.6105, NLL-Loss  107.6105\n",
      "TRAIN Batch 0300/1314, Loss  117.5136, NLL-Loss  117.5136\n",
      "TRAIN Batch 0350/1314, Loss  103.3863, NLL-Loss  103.3863\n",
      "TRAIN Batch 0400/1314, Loss  130.2846, NLL-Loss  130.2846\n",
      "TRAIN Batch 0450/1314, Loss  111.6854, NLL-Loss  111.6854\n",
      "TRAIN Batch 0500/1314, Loss  114.4866, NLL-Loss  114.4866\n",
      "TRAIN Batch 0550/1314, Loss   98.9464, NLL-Loss   98.9464\n",
      "TRAIN Batch 0600/1314, Loss   97.6351, NLL-Loss   97.6351\n",
      "TRAIN Batch 0650/1314, Loss  106.7500, NLL-Loss  106.7500\n",
      "TRAIN Batch 0700/1314, Loss  106.4690, NLL-Loss  106.4690\n",
      "TRAIN Batch 0750/1314, Loss  104.8055, NLL-Loss  104.8055\n",
      "TRAIN Batch 0800/1314, Loss  112.0449, NLL-Loss  112.0449\n",
      "TRAIN Batch 0850/1314, Loss   95.3781, NLL-Loss   95.3781\n",
      "TRAIN Batch 0900/1314, Loss  115.7317, NLL-Loss  115.7317\n",
      "TRAIN Batch 0950/1314, Loss   90.8240, NLL-Loss   90.8240\n",
      "TRAIN Batch 1000/1314, Loss  110.5696, NLL-Loss  110.5696\n",
      "TRAIN Batch 1050/1314, Loss  100.9419, NLL-Loss  100.9419\n",
      "TRAIN Batch 1100/1314, Loss  100.5984, NLL-Loss  100.5984\n",
      "TRAIN Batch 1150/1314, Loss   97.8458, NLL-Loss   97.8458\n",
      "TRAIN Batch 1200/1314, Loss  106.5665, NLL-Loss  106.5665\n",
      "TRAIN Batch 1250/1314, Loss   87.9333, NLL-Loss   87.9333\n",
      "TRAIN Batch 1300/1314, Loss  100.2988, NLL-Loss  100.2988\n",
      "TRAIN Batch 1314/1314, Loss  101.4961, NLL-Loss  101.4961\n",
      "TRAIN Epoch 76/300, Mean NLL  104.5338\n",
      "Model saved at bin/2019-May-17-21:02:23/E76.pytorch\n",
      "VALID Batch 0000/105, Loss  132.9222, NLL-Loss  132.9222\n",
      "VALID Batch 0050/105, Loss  124.7497, NLL-Loss  124.7497\n",
      "VALID Batch 0100/105, Loss   97.1192, NLL-Loss   97.1192\n",
      "VALID Batch 0105/105, Loss   98.3410, NLL-Loss   98.3410\n",
      "VALID Epoch 76/300, Mean NLL  109.8602\n",
      "TEST Batch 0000/117, Loss  101.4374, NLL-Loss  101.4374\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST Batch 0050/117, Loss  116.3385, NLL-Loss  116.3385\n",
      "TEST Batch 0100/117, Loss  137.4935, NLL-Loss  137.4935\n",
      "TEST Batch 0117/117, Loss  161.4025, NLL-Loss  161.4025\n",
      "TEST Epoch 76/300, Mean NLL  109.9950\n",
      "TRAIN Batch 0000/1314, Loss   87.8965, NLL-Loss   87.8965\n",
      "TRAIN Batch 0050/1314, Loss  125.5304, NLL-Loss  125.5304\n",
      "TRAIN Batch 0100/1314, Loss  111.0785, NLL-Loss  111.0785\n",
      "TRAIN Batch 0150/1314, Loss   99.3852, NLL-Loss   99.3852\n",
      "TRAIN Batch 0200/1314, Loss   99.4215, NLL-Loss   99.4215\n",
      "TRAIN Batch 0250/1314, Loss   96.7959, NLL-Loss   96.7959\n",
      "TRAIN Batch 0300/1314, Loss  110.1311, NLL-Loss  110.1311\n",
      "TRAIN Batch 0350/1314, Loss  118.9375, NLL-Loss  118.9375\n",
      "TRAIN Batch 0400/1314, Loss  104.3406, NLL-Loss  104.3406\n",
      "TRAIN Batch 0450/1314, Loss  105.9704, NLL-Loss  105.9704\n",
      "TRAIN Batch 0500/1314, Loss  106.9295, NLL-Loss  106.9295\n",
      "TRAIN Batch 0550/1314, Loss  120.1378, NLL-Loss  120.1378\n",
      "TRAIN Batch 0600/1314, Loss  104.3125, NLL-Loss  104.3125\n",
      "TRAIN Batch 0650/1314, Loss  106.2513, NLL-Loss  106.2513\n",
      "TRAIN Batch 0700/1314, Loss   90.5631, NLL-Loss   90.5631\n",
      "TRAIN Batch 0750/1314, Loss   88.9262, NLL-Loss   88.9262\n",
      "TRAIN Batch 0800/1314, Loss   86.4245, NLL-Loss   86.4245\n",
      "TRAIN Batch 0850/1314, Loss  112.7454, NLL-Loss  112.7454\n",
      "TRAIN Batch 0900/1314, Loss   90.3869, NLL-Loss   90.3869\n",
      "TRAIN Batch 0950/1314, Loss  117.7379, NLL-Loss  117.7379\n",
      "TRAIN Batch 1000/1314, Loss  106.6857, NLL-Loss  106.6857\n",
      "TRAIN Batch 1050/1314, Loss  106.4403, NLL-Loss  106.4403\n",
      "TRAIN Batch 1100/1314, Loss  103.4067, NLL-Loss  103.4067\n",
      "TRAIN Batch 1150/1314, Loss  103.2450, NLL-Loss  103.2450\n",
      "TRAIN Batch 1200/1314, Loss  122.6495, NLL-Loss  122.6495\n",
      "TRAIN Batch 1250/1314, Loss  103.2788, NLL-Loss  103.2788\n",
      "TRAIN Batch 1300/1314, Loss  111.1240, NLL-Loss  111.1240\n",
      "TRAIN Batch 1314/1314, Loss  103.2185, NLL-Loss  103.2185\n",
      "TRAIN Epoch 77/300, Mean NLL  104.3920\n",
      "Model saved at bin/2019-May-17-21:02:23/E77.pytorch\n",
      "VALID Batch 0000/105, Loss  132.9758, NLL-Loss  132.9758\n",
      "VALID Batch 0050/105, Loss  124.5602, NLL-Loss  124.5602\n",
      "VALID Batch 0100/105, Loss   97.2163, NLL-Loss   97.2163\n",
      "VALID Batch 0105/105, Loss   98.4606, NLL-Loss   98.4606\n",
      "VALID Epoch 77/300, Mean NLL  109.7949\n",
      "TEST Batch 0000/117, Loss  101.4120, NLL-Loss  101.4120\n",
      "TEST Batch 0050/117, Loss  116.3227, NLL-Loss  116.3227\n",
      "TEST Batch 0100/117, Loss  137.3153, NLL-Loss  137.3153\n",
      "TEST Batch 0117/117, Loss  161.7103, NLL-Loss  161.7102\n",
      "TEST Epoch 77/300, Mean NLL  109.9294\n",
      "TRAIN Batch 0000/1314, Loss   98.3165, NLL-Loss   98.3165\n",
      "TRAIN Batch 0050/1314, Loss  101.6648, NLL-Loss  101.6648\n",
      "TRAIN Batch 0100/1314, Loss   79.9885, NLL-Loss   79.9885\n",
      "TRAIN Batch 0150/1314, Loss  100.2440, NLL-Loss  100.2440\n",
      "TRAIN Batch 0200/1314, Loss  104.8509, NLL-Loss  104.8509\n",
      "TRAIN Batch 0250/1314, Loss   98.2584, NLL-Loss   98.2584\n",
      "TRAIN Batch 0300/1314, Loss  113.0951, NLL-Loss  113.0951\n",
      "TRAIN Batch 0350/1314, Loss  114.1833, NLL-Loss  114.1833\n",
      "TRAIN Batch 0400/1314, Loss  104.1216, NLL-Loss  104.1216\n",
      "TRAIN Batch 0450/1314, Loss  101.3919, NLL-Loss  101.3919\n",
      "TRAIN Batch 0500/1314, Loss  118.6419, NLL-Loss  118.6419\n",
      "TRAIN Batch 0550/1314, Loss  117.9543, NLL-Loss  117.9543\n",
      "TRAIN Batch 0600/1314, Loss   98.9723, NLL-Loss   98.9723\n",
      "TRAIN Batch 0650/1314, Loss  106.3952, NLL-Loss  106.3952\n",
      "TRAIN Batch 0700/1314, Loss  111.1801, NLL-Loss  111.1801\n",
      "TRAIN Batch 0750/1314, Loss   99.3880, NLL-Loss   99.3880\n",
      "TRAIN Batch 0800/1314, Loss   97.4558, NLL-Loss   97.4558\n",
      "TRAIN Batch 0850/1314, Loss  117.4807, NLL-Loss  117.4807\n",
      "TRAIN Batch 0900/1314, Loss  102.8760, NLL-Loss  102.8760\n",
      "TRAIN Batch 0950/1314, Loss  103.7451, NLL-Loss  103.7451\n",
      "TRAIN Batch 1000/1314, Loss  109.3364, NLL-Loss  109.3364\n",
      "TRAIN Batch 1050/1314, Loss  100.9577, NLL-Loss  100.9577\n",
      "TRAIN Batch 1100/1314, Loss  105.1460, NLL-Loss  105.1460\n",
      "TRAIN Batch 1150/1314, Loss   91.0653, NLL-Loss   91.0653\n",
      "TRAIN Batch 1200/1314, Loss   95.6062, NLL-Loss   95.6062\n",
      "TRAIN Batch 1250/1314, Loss  109.4675, NLL-Loss  109.4675\n",
      "TRAIN Batch 1300/1314, Loss   86.5987, NLL-Loss   86.5987\n",
      "TRAIN Batch 1314/1314, Loss  106.1360, NLL-Loss  106.1360\n",
      "TRAIN Epoch 78/300, Mean NLL  104.2818\n",
      "Model saved at bin/2019-May-17-21:02:23/E78.pytorch\n",
      "VALID Batch 0000/105, Loss  133.0155, NLL-Loss  133.0155\n",
      "VALID Batch 0050/105, Loss  124.2699, NLL-Loss  124.2699\n",
      "VALID Batch 0100/105, Loss   97.0866, NLL-Loss   97.0866\n",
      "VALID Batch 0105/105, Loss   98.2822, NLL-Loss   98.2822\n",
      "VALID Epoch 78/300, Mean NLL  109.7240\n",
      "TEST Batch 0000/117, Loss  101.4018, NLL-Loss  101.4018\n",
      "TEST Batch 0050/117, Loss  115.9938, NLL-Loss  115.9938\n",
      "TEST Batch 0100/117, Loss  137.4516, NLL-Loss  137.4516\n",
      "TEST Batch 0117/117, Loss  161.7431, NLL-Loss  161.7431\n",
      "TEST Epoch 78/300, Mean NLL  109.8523\n",
      "TRAIN Batch 0000/1314, Loss  106.2231, NLL-Loss  106.2231\n",
      "TRAIN Batch 0050/1314, Loss   95.4207, NLL-Loss   95.4207\n",
      "TRAIN Batch 0100/1314, Loss  102.1323, NLL-Loss  102.1323\n",
      "TRAIN Batch 0150/1314, Loss   95.0490, NLL-Loss   95.0490\n",
      "TRAIN Batch 0200/1314, Loss   92.9779, NLL-Loss   92.9779\n",
      "TRAIN Batch 0250/1314, Loss  100.7295, NLL-Loss  100.7295\n",
      "TRAIN Batch 0300/1314, Loss  102.0274, NLL-Loss  102.0274\n",
      "TRAIN Batch 0350/1314, Loss   96.2359, NLL-Loss   96.2359\n",
      "TRAIN Batch 0400/1314, Loss  101.9516, NLL-Loss  101.9516\n",
      "TRAIN Batch 0450/1314, Loss  109.7219, NLL-Loss  109.7219\n",
      "TRAIN Batch 0500/1314, Loss  105.6047, NLL-Loss  105.6047\n",
      "TRAIN Batch 0550/1314, Loss  106.1779, NLL-Loss  106.1779\n",
      "TRAIN Batch 0600/1314, Loss  109.8906, NLL-Loss  109.8906\n",
      "TRAIN Batch 0650/1314, Loss   95.5448, NLL-Loss   95.5448\n",
      "TRAIN Batch 0700/1314, Loss  103.2126, NLL-Loss  103.2126\n",
      "TRAIN Batch 0750/1314, Loss  121.2303, NLL-Loss  121.2303\n",
      "TRAIN Batch 0800/1314, Loss   98.5493, NLL-Loss   98.5493\n",
      "TRAIN Batch 0850/1314, Loss  111.3964, NLL-Loss  111.3964\n",
      "TRAIN Batch 0900/1314, Loss  110.4892, NLL-Loss  110.4892\n",
      "TRAIN Batch 0950/1314, Loss  110.9766, NLL-Loss  110.9766\n",
      "TRAIN Batch 1000/1314, Loss  113.6633, NLL-Loss  113.6633\n",
      "TRAIN Batch 1050/1314, Loss   86.4800, NLL-Loss   86.4800\n",
      "TRAIN Batch 1100/1314, Loss  111.5123, NLL-Loss  111.5123\n",
      "TRAIN Batch 1150/1314, Loss  101.9548, NLL-Loss  101.9548\n",
      "TRAIN Batch 1200/1314, Loss  115.9923, NLL-Loss  115.9923\n",
      "TRAIN Batch 1250/1314, Loss   92.6389, NLL-Loss   92.6389\n",
      "TRAIN Batch 1300/1314, Loss  106.0659, NLL-Loss  106.0659\n",
      "TRAIN Batch 1314/1314, Loss  111.1969, NLL-Loss  111.1969\n",
      "TRAIN Epoch 79/300, Mean NLL  104.1215\n",
      "Model saved at bin/2019-May-17-21:02:23/E79.pytorch\n",
      "VALID Batch 0000/105, Loss  133.1090, NLL-Loss  133.1090\n",
      "VALID Batch 0050/105, Loss  124.3041, NLL-Loss  124.3041\n",
      "VALID Batch 0100/105, Loss   97.1317, NLL-Loss   97.1317\n",
      "VALID Batch 0105/105, Loss   98.3779, NLL-Loss   98.3779\n",
      "VALID Epoch 79/300, Mean NLL  109.6987\n",
      "TEST Batch 0000/117, Loss  101.2044, NLL-Loss  101.2044\n",
      "TEST Batch 0050/117, Loss  115.9274, NLL-Loss  115.9274\n",
      "TEST Batch 0100/117, Loss  137.4848, NLL-Loss  137.4848\n",
      "TEST Batch 0117/117, Loss  161.7178, NLL-Loss  161.7178\n",
      "TEST Epoch 79/300, Mean NLL  109.8131\n",
      "TRAIN Batch 0000/1314, Loss  108.9082, NLL-Loss  108.9082\n",
      "TRAIN Batch 0050/1314, Loss   98.9012, NLL-Loss   98.9012\n",
      "TRAIN Batch 0100/1314, Loss  105.5425, NLL-Loss  105.5425\n",
      "TRAIN Batch 0150/1314, Loss  120.1513, NLL-Loss  120.1513\n",
      "TRAIN Batch 0200/1314, Loss   93.5480, NLL-Loss   93.5480\n",
      "TRAIN Batch 0250/1314, Loss  118.2136, NLL-Loss  118.2136\n",
      "TRAIN Batch 0300/1314, Loss  102.3577, NLL-Loss  102.3577\n",
      "TRAIN Batch 0350/1314, Loss  114.7290, NLL-Loss  114.7290\n",
      "TRAIN Batch 0400/1314, Loss  100.6849, NLL-Loss  100.6849\n",
      "TRAIN Batch 0450/1314, Loss   89.3631, NLL-Loss   89.3631\n",
      "TRAIN Batch 0500/1314, Loss   99.1059, NLL-Loss   99.1059\n",
      "TRAIN Batch 0550/1314, Loss  100.0980, NLL-Loss  100.0980\n",
      "TRAIN Batch 0600/1314, Loss  111.0720, NLL-Loss  111.0720\n",
      "TRAIN Batch 0650/1314, Loss  115.4731, NLL-Loss  115.4731\n",
      "TRAIN Batch 0700/1314, Loss   93.6044, NLL-Loss   93.6044\n",
      "TRAIN Batch 0750/1314, Loss  106.8029, NLL-Loss  106.8029\n",
      "TRAIN Batch 0800/1314, Loss  112.8096, NLL-Loss  112.8096\n",
      "TRAIN Batch 0850/1314, Loss  102.1764, NLL-Loss  102.1764\n",
      "TRAIN Batch 0900/1314, Loss  112.7300, NLL-Loss  112.7300\n",
      "TRAIN Batch 0950/1314, Loss  102.6063, NLL-Loss  102.6063\n",
      "TRAIN Batch 1000/1314, Loss  105.8821, NLL-Loss  105.8821\n",
      "TRAIN Batch 1050/1314, Loss  106.2445, NLL-Loss  106.2445\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN Batch 1100/1314, Loss   93.8768, NLL-Loss   93.8768\n",
      "TRAIN Batch 1150/1314, Loss  113.5317, NLL-Loss  113.5317\n",
      "TRAIN Batch 1200/1314, Loss  110.8896, NLL-Loss  110.8896\n",
      "TRAIN Batch 1250/1314, Loss   98.4363, NLL-Loss   98.4363\n",
      "TRAIN Batch 1300/1314, Loss  118.8896, NLL-Loss  118.8896\n",
      "TRAIN Batch 1314/1314, Loss  114.0210, NLL-Loss  114.0210\n",
      "TRAIN Epoch 80/300, Mean NLL  103.9933\n",
      "Model saved at bin/2019-May-17-21:02:23/E80.pytorch\n",
      "VALID Batch 0000/105, Loss  132.9598, NLL-Loss  132.9598\n",
      "VALID Batch 0050/105, Loss  124.2491, NLL-Loss  124.2491\n",
      "VALID Batch 0100/105, Loss   97.1167, NLL-Loss   97.1167\n",
      "VALID Batch 0105/105, Loss   98.1986, NLL-Loss   98.1986\n",
      "VALID Epoch 80/300, Mean NLL  109.6241\n",
      "TEST Batch 0000/117, Loss  101.1255, NLL-Loss  101.1255\n",
      "TEST Batch 0050/117, Loss  116.0576, NLL-Loss  116.0576\n",
      "TEST Batch 0100/117, Loss  137.1995, NLL-Loss  137.1995\n",
      "TEST Batch 0117/117, Loss  161.2512, NLL-Loss  161.2512\n",
      "TEST Epoch 80/300, Mean NLL  109.7777\n",
      "TRAIN Batch 0000/1314, Loss  117.0782, NLL-Loss  117.0782\n",
      "TRAIN Batch 0050/1314, Loss  104.4239, NLL-Loss  104.4239\n",
      "TRAIN Batch 0100/1314, Loss  100.0798, NLL-Loss  100.0798\n",
      "TRAIN Batch 0150/1314, Loss   89.9306, NLL-Loss   89.9306\n",
      "TRAIN Batch 0200/1314, Loss  113.1181, NLL-Loss  113.1181\n",
      "TRAIN Batch 0250/1314, Loss  100.3832, NLL-Loss  100.3832\n",
      "TRAIN Batch 0300/1314, Loss  101.1559, NLL-Loss  101.1559\n",
      "TRAIN Batch 0350/1314, Loss  108.5221, NLL-Loss  108.5221\n",
      "TRAIN Batch 0400/1314, Loss  109.9234, NLL-Loss  109.9234\n",
      "TRAIN Batch 0450/1314, Loss  105.4291, NLL-Loss  105.4291\n",
      "TRAIN Batch 0500/1314, Loss  105.0811, NLL-Loss  105.0811\n",
      "TRAIN Batch 0550/1314, Loss   97.6538, NLL-Loss   97.6538\n",
      "TRAIN Batch 0600/1314, Loss  102.1364, NLL-Loss  102.1364\n",
      "TRAIN Batch 0650/1314, Loss  106.3643, NLL-Loss  106.3643\n",
      "TRAIN Batch 0700/1314, Loss   98.9017, NLL-Loss   98.9017\n",
      "TRAIN Batch 0750/1314, Loss  108.7397, NLL-Loss  108.7397\n",
      "TRAIN Batch 0800/1314, Loss  114.7480, NLL-Loss  114.7480\n",
      "TRAIN Batch 0850/1314, Loss  105.8179, NLL-Loss  105.8179\n",
      "TRAIN Batch 0900/1314, Loss   89.2385, NLL-Loss   89.2385\n",
      "TRAIN Batch 0950/1314, Loss  102.3563, NLL-Loss  102.3563\n",
      "TRAIN Batch 1000/1314, Loss  104.7315, NLL-Loss  104.7315\n",
      "TRAIN Batch 1050/1314, Loss   97.1980, NLL-Loss   97.1980\n",
      "TRAIN Batch 1100/1314, Loss  114.8960, NLL-Loss  114.8960\n",
      "TRAIN Batch 1150/1314, Loss  107.6937, NLL-Loss  107.6937\n",
      "TRAIN Batch 1200/1314, Loss   92.9379, NLL-Loss   92.9379\n",
      "TRAIN Batch 1250/1314, Loss  103.9894, NLL-Loss  103.9894\n",
      "TRAIN Batch 1300/1314, Loss   84.8358, NLL-Loss   84.8358\n",
      "TRAIN Batch 1314/1314, Loss   97.2271, NLL-Loss   97.2271\n",
      "TRAIN Epoch 81/300, Mean NLL  103.8652\n",
      "Model saved at bin/2019-May-17-21:02:23/E81.pytorch\n",
      "VALID Batch 0000/105, Loss  133.1062, NLL-Loss  133.1062\n",
      "VALID Batch 0050/105, Loss  123.9888, NLL-Loss  123.9888\n",
      "VALID Batch 0100/105, Loss   96.9545, NLL-Loss   96.9545\n",
      "VALID Batch 0105/105, Loss   98.3264, NLL-Loss   98.3264\n",
      "VALID Epoch 81/300, Mean NLL  109.6093\n",
      "TEST Batch 0000/117, Loss  101.0557, NLL-Loss  101.0557\n",
      "TEST Batch 0050/117, Loss  115.9786, NLL-Loss  115.9786\n",
      "TEST Batch 0100/117, Loss  137.2971, NLL-Loss  137.2971\n",
      "TEST Batch 0117/117, Loss  161.6830, NLL-Loss  161.6829\n",
      "TEST Epoch 81/300, Mean NLL  109.7362\n",
      "TRAIN Batch 0000/1314, Loss   86.3553, NLL-Loss   86.3553\n",
      "TRAIN Batch 0050/1314, Loss   95.5636, NLL-Loss   95.5636\n",
      "TRAIN Batch 0100/1314, Loss  106.3725, NLL-Loss  106.3725\n",
      "TRAIN Batch 0150/1314, Loss  102.3780, NLL-Loss  102.3780\n",
      "TRAIN Batch 0200/1314, Loss   89.5664, NLL-Loss   89.5664\n",
      "TRAIN Batch 0250/1314, Loss  105.1289, NLL-Loss  105.1289\n",
      "TRAIN Batch 0300/1314, Loss   96.0581, NLL-Loss   96.0581\n",
      "TRAIN Batch 0350/1314, Loss  105.1606, NLL-Loss  105.1606\n",
      "TRAIN Batch 0400/1314, Loss  127.3855, NLL-Loss  127.3855\n",
      "TRAIN Batch 0450/1314, Loss  105.5497, NLL-Loss  105.5497\n",
      "TRAIN Batch 0500/1314, Loss  104.2695, NLL-Loss  104.2695\n",
      "TRAIN Batch 0550/1314, Loss  120.7577, NLL-Loss  120.7577\n",
      "TRAIN Batch 0600/1314, Loss   97.2116, NLL-Loss   97.2116\n",
      "TRAIN Batch 0650/1314, Loss  107.9405, NLL-Loss  107.9405\n",
      "TRAIN Batch 0700/1314, Loss  111.0368, NLL-Loss  111.0368\n",
      "TRAIN Batch 0750/1314, Loss   98.4765, NLL-Loss   98.4765\n",
      "TRAIN Batch 0800/1314, Loss   90.4525, NLL-Loss   90.4525\n",
      "TRAIN Batch 0850/1314, Loss   85.5981, NLL-Loss   85.5981\n",
      "TRAIN Batch 0900/1314, Loss  102.7854, NLL-Loss  102.7854\n",
      "TRAIN Batch 0950/1314, Loss  101.4063, NLL-Loss  101.4063\n",
      "TRAIN Batch 1000/1314, Loss  120.5312, NLL-Loss  120.5312\n",
      "TRAIN Batch 1050/1314, Loss  112.9452, NLL-Loss  112.9452\n",
      "TRAIN Batch 1100/1314, Loss  109.5141, NLL-Loss  109.5141\n",
      "TRAIN Batch 1150/1314, Loss  111.1535, NLL-Loss  111.1535\n",
      "TRAIN Batch 1200/1314, Loss  105.2371, NLL-Loss  105.2371\n",
      "TRAIN Batch 1250/1314, Loss   98.2248, NLL-Loss   98.2248\n",
      "TRAIN Batch 1300/1314, Loss  124.5150, NLL-Loss  124.5150\n",
      "TRAIN Batch 1314/1314, Loss  108.6005, NLL-Loss  108.6005\n",
      "TRAIN Epoch 82/300, Mean NLL  103.7448\n",
      "Model saved at bin/2019-May-17-21:02:23/E82.pytorch\n",
      "VALID Batch 0000/105, Loss  132.9165, NLL-Loss  132.9165\n",
      "VALID Batch 0050/105, Loss  123.9730, NLL-Loss  123.9730\n",
      "VALID Batch 0100/105, Loss   96.8061, NLL-Loss   96.8061\n",
      "VALID Batch 0105/105, Loss   98.4630, NLL-Loss   98.4630\n",
      "VALID Epoch 82/300, Mean NLL  109.5297\n",
      "TEST Batch 0000/117, Loss  100.9963, NLL-Loss  100.9963\n",
      "TEST Batch 0050/117, Loss  116.0065, NLL-Loss  116.0065\n",
      "TEST Batch 0100/117, Loss  137.3327, NLL-Loss  137.3327\n",
      "TEST Batch 0117/117, Loss  161.9065, NLL-Loss  161.9065\n",
      "TEST Epoch 82/300, Mean NLL  109.6716\n",
      "TRAIN Batch 0000/1314, Loss   99.7766, NLL-Loss   99.7766\n",
      "TRAIN Batch 0050/1314, Loss   97.8419, NLL-Loss   97.8419\n",
      "TRAIN Batch 0100/1314, Loss   98.8192, NLL-Loss   98.8192\n",
      "TRAIN Batch 0150/1314, Loss  105.7326, NLL-Loss  105.7326\n",
      "TRAIN Batch 0200/1314, Loss  101.5202, NLL-Loss  101.5202\n",
      "TRAIN Batch 0250/1314, Loss  100.1949, NLL-Loss  100.1949\n",
      "TRAIN Batch 0300/1314, Loss  112.8407, NLL-Loss  112.8407\n",
      "TRAIN Batch 0350/1314, Loss  100.6867, NLL-Loss  100.6867\n",
      "TRAIN Batch 0400/1314, Loss   95.6296, NLL-Loss   95.6296\n",
      "TRAIN Batch 0450/1314, Loss  104.1527, NLL-Loss  104.1527\n",
      "TRAIN Batch 0500/1314, Loss  111.2004, NLL-Loss  111.2004\n",
      "TRAIN Batch 0550/1314, Loss  116.6669, NLL-Loss  116.6669\n",
      "TRAIN Batch 0600/1314, Loss  100.5515, NLL-Loss  100.5515\n",
      "TRAIN Batch 0650/1314, Loss  104.3671, NLL-Loss  104.3671\n",
      "TRAIN Batch 0700/1314, Loss  104.9341, NLL-Loss  104.9341\n",
      "TRAIN Batch 0750/1314, Loss  111.8268, NLL-Loss  111.8268\n",
      "TRAIN Batch 0800/1314, Loss   91.7716, NLL-Loss   91.7716\n",
      "TRAIN Batch 0850/1314, Loss   99.2896, NLL-Loss   99.2896\n",
      "TRAIN Batch 0900/1314, Loss  105.2624, NLL-Loss  105.2624\n",
      "TRAIN Batch 0950/1314, Loss   94.2438, NLL-Loss   94.2438\n",
      "TRAIN Batch 1000/1314, Loss   87.1540, NLL-Loss   87.1540\n",
      "TRAIN Batch 1050/1314, Loss  101.1780, NLL-Loss  101.1780\n",
      "TRAIN Batch 1100/1314, Loss   89.3590, NLL-Loss   89.3590\n",
      "TRAIN Batch 1150/1314, Loss  104.2320, NLL-Loss  104.2320\n",
      "TRAIN Batch 1200/1314, Loss  102.1218, NLL-Loss  102.1218\n",
      "TRAIN Batch 1250/1314, Loss   92.6401, NLL-Loss   92.6401\n",
      "TRAIN Batch 1300/1314, Loss  106.4803, NLL-Loss  106.4803\n",
      "TRAIN Batch 1314/1314, Loss  109.7698, NLL-Loss  109.7698\n",
      "TRAIN Epoch 83/300, Mean NLL  103.6292\n",
      "Model saved at bin/2019-May-17-21:02:23/E83.pytorch\n",
      "VALID Batch 0000/105, Loss  132.9431, NLL-Loss  132.9431\n",
      "VALID Batch 0050/105, Loss  123.9859, NLL-Loss  123.9859\n",
      "VALID Batch 0100/105, Loss   96.9185, NLL-Loss   96.9185\n",
      "VALID Batch 0105/105, Loss   98.2600, NLL-Loss   98.2600\n",
      "VALID Epoch 83/300, Mean NLL  109.5182\n",
      "TEST Batch 0000/117, Loss  100.9972, NLL-Loss  100.9972\n",
      "TEST Batch 0050/117, Loss  116.0942, NLL-Loss  116.0942\n",
      "TEST Batch 0100/117, Loss  137.1765, NLL-Loss  137.1765\n",
      "TEST Batch 0117/117, Loss  161.5374, NLL-Loss  161.5374\n",
      "TEST Epoch 83/300, Mean NLL  109.6579\n",
      "TRAIN Batch 0000/1314, Loss   84.4138, NLL-Loss   84.4138\n",
      "TRAIN Batch 0050/1314, Loss  106.3606, NLL-Loss  106.3606\n",
      "TRAIN Batch 0100/1314, Loss  108.2980, NLL-Loss  108.2980\n",
      "TRAIN Batch 0150/1314, Loss   89.5855, NLL-Loss   89.5855\n",
      "TRAIN Batch 0200/1314, Loss  109.2273, NLL-Loss  109.2273\n",
      "TRAIN Batch 0250/1314, Loss  101.3682, NLL-Loss  101.3682\n",
      "TRAIN Batch 0300/1314, Loss  106.2203, NLL-Loss  106.2203\n",
      "TRAIN Batch 0350/1314, Loss  127.6658, NLL-Loss  127.6658\n",
      "TRAIN Batch 0400/1314, Loss  114.5171, NLL-Loss  114.5171\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN Batch 0450/1314, Loss   97.0402, NLL-Loss   97.0402\n",
      "TRAIN Batch 0500/1314, Loss  114.0181, NLL-Loss  114.0181\n",
      "TRAIN Batch 0550/1314, Loss   95.1480, NLL-Loss   95.1480\n",
      "TRAIN Batch 0600/1314, Loss   99.7936, NLL-Loss   99.7936\n",
      "TRAIN Batch 0650/1314, Loss   96.1037, NLL-Loss   96.1037\n",
      "TRAIN Batch 0700/1314, Loss   93.2076, NLL-Loss   93.2076\n",
      "TRAIN Batch 0750/1314, Loss   97.9175, NLL-Loss   97.9175\n",
      "TRAIN Batch 0800/1314, Loss   98.1705, NLL-Loss   98.1705\n",
      "TRAIN Batch 0850/1314, Loss  109.8604, NLL-Loss  109.8604\n",
      "TRAIN Batch 0900/1314, Loss  108.6538, NLL-Loss  108.6538\n",
      "TRAIN Batch 0950/1314, Loss  101.5463, NLL-Loss  101.5463\n",
      "TRAIN Batch 1000/1314, Loss  111.2147, NLL-Loss  111.2147\n",
      "TRAIN Batch 1050/1314, Loss  103.5270, NLL-Loss  103.5270\n",
      "TRAIN Batch 1100/1314, Loss  100.1051, NLL-Loss  100.1051\n",
      "TRAIN Batch 1150/1314, Loss   91.7148, NLL-Loss   91.7148\n",
      "TRAIN Batch 1200/1314, Loss   87.2136, NLL-Loss   87.2136\n",
      "TRAIN Batch 1250/1314, Loss  103.1336, NLL-Loss  103.1336\n",
      "TRAIN Batch 1300/1314, Loss   97.5860, NLL-Loss   97.5860\n",
      "TRAIN Batch 1314/1314, Loss  128.6008, NLL-Loss  128.6008\n",
      "TRAIN Epoch 84/300, Mean NLL  103.5094\n",
      "Model saved at bin/2019-May-17-21:02:23/E84.pytorch\n",
      "VALID Batch 0000/105, Loss  132.8040, NLL-Loss  132.8040\n",
      "VALID Batch 0050/105, Loss  123.8734, NLL-Loss  123.8734\n",
      "VALID Batch 0100/105, Loss   96.9347, NLL-Loss   96.9347\n",
      "VALID Batch 0105/105, Loss   98.0470, NLL-Loss   98.0469\n",
      "VALID Epoch 84/300, Mean NLL  109.4745\n",
      "TEST Batch 0000/117, Loss  101.1544, NLL-Loss  101.1544\n",
      "TEST Batch 0050/117, Loss  115.9422, NLL-Loss  115.9422\n",
      "TEST Batch 0100/117, Loss  137.1371, NLL-Loss  137.1371\n",
      "TEST Batch 0117/117, Loss  161.2638, NLL-Loss  161.2638\n",
      "TEST Epoch 84/300, Mean NLL  109.6086\n",
      "TRAIN Batch 0000/1314, Loss   98.2297, NLL-Loss   98.2297\n",
      "TRAIN Batch 0050/1314, Loss  112.8002, NLL-Loss  112.8002\n",
      "TRAIN Batch 0100/1314, Loss  107.1429, NLL-Loss  107.1429\n",
      "TRAIN Batch 0150/1314, Loss  121.9033, NLL-Loss  121.9033\n",
      "TRAIN Batch 0200/1314, Loss   95.4928, NLL-Loss   95.4928\n",
      "TRAIN Batch 0250/1314, Loss   97.4192, NLL-Loss   97.4192\n",
      "TRAIN Batch 0300/1314, Loss   92.4402, NLL-Loss   92.4402\n",
      "TRAIN Batch 0350/1314, Loss   96.0607, NLL-Loss   96.0607\n",
      "TRAIN Batch 0400/1314, Loss   83.1909, NLL-Loss   83.1909\n",
      "TRAIN Batch 0450/1314, Loss  101.1029, NLL-Loss  101.1029\n",
      "TRAIN Batch 0500/1314, Loss  100.9516, NLL-Loss  100.9516\n",
      "TRAIN Batch 0550/1314, Loss   98.5794, NLL-Loss   98.5794\n",
      "TRAIN Batch 0600/1314, Loss  103.1972, NLL-Loss  103.1972\n",
      "TRAIN Batch 0650/1314, Loss  106.2610, NLL-Loss  106.2610\n",
      "TRAIN Batch 0700/1314, Loss  110.1922, NLL-Loss  110.1922\n",
      "TRAIN Batch 0750/1314, Loss   92.9429, NLL-Loss   92.9429\n",
      "TRAIN Batch 0800/1314, Loss  101.9484, NLL-Loss  101.9484\n",
      "TRAIN Batch 0850/1314, Loss   87.4632, NLL-Loss   87.4632\n",
      "TRAIN Batch 0900/1314, Loss  116.7090, NLL-Loss  116.7090\n",
      "TRAIN Batch 0950/1314, Loss   97.8625, NLL-Loss   97.8625\n",
      "TRAIN Batch 1000/1314, Loss  101.0278, NLL-Loss  101.0278\n",
      "TRAIN Batch 1050/1314, Loss   98.0474, NLL-Loss   98.0474\n",
      "TRAIN Batch 1100/1314, Loss  103.8543, NLL-Loss  103.8543\n",
      "TRAIN Batch 1150/1314, Loss   96.7194, NLL-Loss   96.7194\n",
      "TRAIN Batch 1200/1314, Loss  112.1719, NLL-Loss  112.1719\n",
      "TRAIN Batch 1250/1314, Loss  104.5575, NLL-Loss  104.5575\n",
      "TRAIN Batch 1300/1314, Loss  111.5056, NLL-Loss  111.5056\n",
      "TRAIN Batch 1314/1314, Loss   74.6864, NLL-Loss   74.6864\n",
      "TRAIN Epoch 85/300, Mean NLL  103.3600\n",
      "Model saved at bin/2019-May-17-21:02:23/E85.pytorch\n",
      "VALID Batch 0000/105, Loss  132.8880, NLL-Loss  132.8880\n",
      "VALID Batch 0050/105, Loss  123.8117, NLL-Loss  123.8117\n",
      "VALID Batch 0100/105, Loss   96.6359, NLL-Loss   96.6359\n",
      "VALID Batch 0105/105, Loss   98.3291, NLL-Loss   98.3291\n",
      "VALID Epoch 85/300, Mean NLL  109.4090\n",
      "TEST Batch 0000/117, Loss  100.7680, NLL-Loss  100.7680\n",
      "TEST Batch 0050/117, Loss  116.0731, NLL-Loss  116.0731\n",
      "TEST Batch 0100/117, Loss  137.4266, NLL-Loss  137.4266\n",
      "TEST Batch 0117/117, Loss  161.2648, NLL-Loss  161.2648\n",
      "TEST Epoch 85/300, Mean NLL  109.5559\n",
      "TRAIN Batch 0000/1314, Loss  104.5664, NLL-Loss  104.5664\n",
      "TRAIN Batch 0050/1314, Loss  111.2840, NLL-Loss  111.2840\n",
      "TRAIN Batch 0100/1314, Loss   91.6281, NLL-Loss   91.6281\n",
      "TRAIN Batch 0150/1314, Loss  101.4536, NLL-Loss  101.4536\n",
      "TRAIN Batch 0200/1314, Loss  109.4118, NLL-Loss  109.4118\n",
      "TRAIN Batch 0250/1314, Loss  104.1349, NLL-Loss  104.1349\n",
      "TRAIN Batch 0300/1314, Loss  101.1527, NLL-Loss  101.1527\n",
      "TRAIN Batch 0350/1314, Loss  106.1864, NLL-Loss  106.1864\n",
      "TRAIN Batch 0400/1314, Loss  104.4982, NLL-Loss  104.4982\n",
      "TRAIN Batch 0450/1314, Loss  108.1032, NLL-Loss  108.1032\n",
      "TRAIN Batch 0500/1314, Loss   97.7415, NLL-Loss   97.7415\n",
      "TRAIN Batch 0550/1314, Loss  107.2564, NLL-Loss  107.2564\n",
      "TRAIN Batch 0600/1314, Loss  101.2087, NLL-Loss  101.2087\n",
      "TRAIN Batch 0650/1314, Loss   90.4673, NLL-Loss   90.4673\n",
      "TRAIN Batch 0700/1314, Loss  107.3228, NLL-Loss  107.3228\n",
      "TRAIN Batch 0750/1314, Loss  106.1781, NLL-Loss  106.1781\n",
      "TRAIN Batch 0800/1314, Loss  104.6724, NLL-Loss  104.6724\n",
      "TRAIN Batch 0850/1314, Loss  102.2418, NLL-Loss  102.2418\n",
      "TRAIN Batch 0900/1314, Loss  111.8139, NLL-Loss  111.8139\n",
      "TRAIN Batch 0950/1314, Loss   97.1961, NLL-Loss   97.1961\n",
      "TRAIN Batch 1000/1314, Loss  117.7039, NLL-Loss  117.7039\n",
      "TRAIN Batch 1050/1314, Loss   94.4877, NLL-Loss   94.4877\n",
      "TRAIN Batch 1100/1314, Loss  117.8238, NLL-Loss  117.8238\n",
      "TRAIN Batch 1150/1314, Loss   97.6935, NLL-Loss   97.6935\n",
      "TRAIN Batch 1200/1314, Loss  109.6805, NLL-Loss  109.6805\n",
      "TRAIN Batch 1250/1314, Loss  101.1913, NLL-Loss  101.1913\n",
      "TRAIN Batch 1300/1314, Loss   90.7064, NLL-Loss   90.7064\n",
      "TRAIN Batch 1314/1314, Loss  112.2008, NLL-Loss  112.2008\n",
      "TRAIN Epoch 86/300, Mean NLL  103.2554\n",
      "Model saved at bin/2019-May-17-21:02:23/E86.pytorch\n",
      "VALID Batch 0000/105, Loss  132.9377, NLL-Loss  132.9377\n",
      "VALID Batch 0050/105, Loss  123.4195, NLL-Loss  123.4195\n",
      "VALID Batch 0100/105, Loss   96.5759, NLL-Loss   96.5759\n",
      "VALID Batch 0105/105, Loss   98.2026, NLL-Loss   98.2026\n",
      "VALID Epoch 86/300, Mean NLL  109.3217\n",
      "TEST Batch 0000/117, Loss  100.5566, NLL-Loss  100.5566\n",
      "TEST Batch 0050/117, Loss  115.8656, NLL-Loss  115.8656\n",
      "TEST Batch 0100/117, Loss  137.3687, NLL-Loss  137.3687\n",
      "TEST Batch 0117/117, Loss  161.4705, NLL-Loss  161.4705\n",
      "TEST Epoch 86/300, Mean NLL  109.4741\n",
      "TRAIN Batch 0000/1314, Loss  110.0618, NLL-Loss  110.0618\n",
      "TRAIN Batch 0050/1314, Loss   82.6437, NLL-Loss   82.6437\n",
      "TRAIN Batch 0100/1314, Loss  106.2028, NLL-Loss  106.2028\n",
      "TRAIN Batch 0150/1314, Loss  110.1011, NLL-Loss  110.1011\n",
      "TRAIN Batch 0200/1314, Loss  103.6019, NLL-Loss  103.6019\n",
      "TRAIN Batch 0250/1314, Loss  112.3089, NLL-Loss  112.3089\n",
      "TRAIN Batch 0300/1314, Loss  101.2001, NLL-Loss  101.2001\n",
      "TRAIN Batch 0350/1314, Loss  110.4984, NLL-Loss  110.4984\n",
      "TRAIN Batch 0400/1314, Loss  102.2836, NLL-Loss  102.2836\n",
      "TRAIN Batch 0450/1314, Loss   94.8367, NLL-Loss   94.8367\n",
      "TRAIN Batch 0500/1314, Loss  101.6464, NLL-Loss  101.6464\n",
      "TRAIN Batch 0550/1314, Loss   95.1628, NLL-Loss   95.1628\n",
      "TRAIN Batch 0600/1314, Loss   94.0151, NLL-Loss   94.0151\n",
      "TRAIN Batch 0650/1314, Loss  103.1819, NLL-Loss  103.1819\n",
      "TRAIN Batch 0700/1314, Loss   83.0616, NLL-Loss   83.0616\n",
      "TRAIN Batch 0750/1314, Loss   97.4225, NLL-Loss   97.4225\n",
      "TRAIN Batch 0800/1314, Loss  103.4035, NLL-Loss  103.4035\n",
      "TRAIN Batch 0850/1314, Loss  115.6724, NLL-Loss  115.6724\n",
      "TRAIN Batch 0900/1314, Loss  114.7294, NLL-Loss  114.7294\n",
      "TRAIN Batch 0950/1314, Loss  112.2106, NLL-Loss  112.2106\n",
      "TRAIN Batch 1000/1314, Loss  107.9479, NLL-Loss  107.9479\n",
      "TRAIN Batch 1050/1314, Loss   98.8800, NLL-Loss   98.8800\n",
      "TRAIN Batch 1100/1314, Loss   91.9541, NLL-Loss   91.9541\n",
      "TRAIN Batch 1150/1314, Loss  105.2871, NLL-Loss  105.2871\n",
      "TRAIN Batch 1200/1314, Loss  101.6262, NLL-Loss  101.6262\n",
      "TRAIN Batch 1250/1314, Loss  109.9549, NLL-Loss  109.9549\n",
      "TRAIN Batch 1300/1314, Loss  108.0297, NLL-Loss  108.0297\n",
      "TRAIN Batch 1314/1314, Loss  105.7573, NLL-Loss  105.7573\n",
      "TRAIN Epoch 87/300, Mean NLL  103.1234\n",
      "Model saved at bin/2019-May-17-21:02:23/E87.pytorch\n",
      "VALID Batch 0000/105, Loss  133.0048, NLL-Loss  133.0048\n",
      "VALID Batch 0050/105, Loss  123.4786, NLL-Loss  123.4786\n",
      "VALID Batch 0100/105, Loss   96.4039, NLL-Loss   96.4039\n",
      "VALID Batch 0105/105, Loss   98.3547, NLL-Loss   98.3547\n",
      "VALID Epoch 87/300, Mean NLL  109.2908\n",
      "TEST Batch 0000/117, Loss  100.5323, NLL-Loss  100.5323\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST Batch 0050/117, Loss  116.1005, NLL-Loss  116.1005\n",
      "TEST Batch 0100/117, Loss  137.2765, NLL-Loss  137.2765\n",
      "TEST Batch 0117/117, Loss  161.5880, NLL-Loss  161.5880\n",
      "TEST Epoch 87/300, Mean NLL  109.4226\n",
      "TRAIN Batch 0000/1314, Loss  103.4587, NLL-Loss  103.4587\n",
      "TRAIN Batch 0050/1314, Loss  100.1815, NLL-Loss  100.1815\n",
      "TRAIN Batch 0100/1314, Loss  112.7007, NLL-Loss  112.7007\n",
      "TRAIN Batch 0150/1314, Loss  101.1802, NLL-Loss  101.1802\n",
      "TRAIN Batch 0200/1314, Loss  103.4329, NLL-Loss  103.4329\n",
      "TRAIN Batch 0250/1314, Loss  102.8820, NLL-Loss  102.8820\n",
      "TRAIN Batch 0300/1314, Loss  107.8154, NLL-Loss  107.8154\n",
      "TRAIN Batch 0350/1314, Loss  100.2995, NLL-Loss  100.2995\n",
      "TRAIN Batch 0400/1314, Loss   92.4190, NLL-Loss   92.4190\n",
      "TRAIN Batch 0450/1314, Loss   99.0941, NLL-Loss   99.0941\n",
      "TRAIN Batch 0500/1314, Loss  101.5936, NLL-Loss  101.5936\n",
      "TRAIN Batch 0550/1314, Loss   81.6509, NLL-Loss   81.6509\n",
      "TRAIN Batch 0600/1314, Loss  111.2360, NLL-Loss  111.2360\n",
      "TRAIN Batch 0650/1314, Loss   93.2247, NLL-Loss   93.2247\n",
      "TRAIN Batch 0700/1314, Loss  108.7658, NLL-Loss  108.7658\n",
      "TRAIN Batch 0750/1314, Loss   97.2290, NLL-Loss   97.2290\n",
      "TRAIN Batch 0800/1314, Loss  104.4293, NLL-Loss  104.4293\n",
      "TRAIN Batch 0850/1314, Loss  102.9564, NLL-Loss  102.9564\n",
      "TRAIN Batch 0900/1314, Loss   91.1035, NLL-Loss   91.1035\n",
      "TRAIN Batch 0950/1314, Loss   98.7305, NLL-Loss   98.7305\n",
      "TRAIN Batch 1000/1314, Loss   85.2750, NLL-Loss   85.2750\n",
      "TRAIN Batch 1050/1314, Loss  113.0827, NLL-Loss  113.0827\n",
      "TRAIN Batch 1100/1314, Loss  101.0408, NLL-Loss  101.0408\n",
      "TRAIN Batch 1150/1314, Loss   92.2964, NLL-Loss   92.2964\n",
      "TRAIN Batch 1200/1314, Loss  118.8617, NLL-Loss  118.8617\n",
      "TRAIN Batch 1250/1314, Loss  105.6814, NLL-Loss  105.6814\n",
      "TRAIN Batch 1300/1314, Loss  110.1815, NLL-Loss  110.1815\n",
      "TRAIN Batch 1314/1314, Loss   98.5757, NLL-Loss   98.5757\n",
      "TRAIN Epoch 88/300, Mean NLL  103.0018\n",
      "Model saved at bin/2019-May-17-21:02:23/E88.pytorch\n",
      "VALID Batch 0000/105, Loss  132.8647, NLL-Loss  132.8647\n",
      "VALID Batch 0050/105, Loss  123.7066, NLL-Loss  123.7066\n",
      "VALID Batch 0100/105, Loss   96.7816, NLL-Loss   96.7816\n",
      "VALID Batch 0105/105, Loss   98.0353, NLL-Loss   98.0353\n",
      "VALID Epoch 88/300, Mean NLL  109.3496\n",
      "TEST Batch 0000/117, Loss  100.6558, NLL-Loss  100.6558\n",
      "TEST Batch 0050/117, Loss  115.9446, NLL-Loss  115.9446\n",
      "TEST Batch 0100/117, Loss  137.0663, NLL-Loss  137.0663\n",
      "TEST Batch 0117/117, Loss  161.2799, NLL-Loss  161.2799\n",
      "TEST Epoch 88/300, Mean NLL  109.4697\n",
      "TRAIN Batch 0000/1314, Loss  106.6014, NLL-Loss  106.6014\n",
      "TRAIN Batch 0050/1314, Loss  106.5131, NLL-Loss  106.5131\n",
      "TRAIN Batch 0100/1314, Loss   88.1840, NLL-Loss   88.1840\n",
      "TRAIN Batch 0150/1314, Loss   92.2709, NLL-Loss   92.2709\n",
      "TRAIN Batch 0200/1314, Loss  107.5904, NLL-Loss  107.5904\n",
      "TRAIN Batch 0250/1314, Loss   98.8585, NLL-Loss   98.8585\n",
      "TRAIN Batch 0300/1314, Loss  121.5000, NLL-Loss  121.5000\n",
      "TRAIN Batch 0350/1314, Loss  103.8155, NLL-Loss  103.8155\n",
      "TRAIN Batch 0400/1314, Loss  101.1341, NLL-Loss  101.1341\n",
      "TRAIN Batch 0450/1314, Loss  118.8289, NLL-Loss  118.8289\n",
      "TRAIN Batch 0500/1314, Loss  101.4885, NLL-Loss  101.4885\n",
      "TRAIN Batch 0550/1314, Loss  105.0101, NLL-Loss  105.0101\n",
      "TRAIN Batch 0600/1314, Loss  104.9907, NLL-Loss  104.9907\n",
      "TRAIN Batch 0650/1314, Loss  110.3276, NLL-Loss  110.3276\n",
      "TRAIN Batch 0700/1314, Loss   99.2050, NLL-Loss   99.2050\n",
      "TRAIN Batch 0750/1314, Loss  112.2230, NLL-Loss  112.2230\n",
      "TRAIN Batch 0800/1314, Loss  105.4498, NLL-Loss  105.4498\n",
      "TRAIN Batch 0850/1314, Loss  111.4926, NLL-Loss  111.4926\n",
      "TRAIN Batch 0900/1314, Loss  104.9079, NLL-Loss  104.9079\n",
      "TRAIN Batch 0950/1314, Loss  113.9486, NLL-Loss  113.9486\n",
      "TRAIN Batch 1000/1314, Loss   95.1790, NLL-Loss   95.1790\n",
      "TRAIN Batch 1050/1314, Loss  116.2180, NLL-Loss  116.2180\n",
      "TRAIN Batch 1100/1314, Loss   95.8788, NLL-Loss   95.8788\n",
      "TRAIN Batch 1150/1314, Loss  100.8625, NLL-Loss  100.8625\n",
      "TRAIN Batch 1200/1314, Loss   99.4821, NLL-Loss   99.4821\n",
      "TRAIN Batch 1250/1314, Loss   96.9612, NLL-Loss   96.9612\n",
      "TRAIN Batch 1300/1314, Loss   99.7290, NLL-Loss   99.7290\n",
      "TRAIN Batch 1314/1314, Loss  104.3729, NLL-Loss  104.3729\n",
      "TRAIN Epoch 89/300, Mean NLL  102.9065\n",
      "Model saved at bin/2019-May-17-21:02:23/E89.pytorch\n",
      "VALID Batch 0000/105, Loss  133.0555, NLL-Loss  133.0555\n",
      "VALID Batch 0050/105, Loss  123.5795, NLL-Loss  123.5795\n",
      "VALID Batch 0100/105, Loss   96.5541, NLL-Loss   96.5541\n",
      "VALID Batch 0105/105, Loss   98.1799, NLL-Loss   98.1799\n",
      "VALID Epoch 89/300, Mean NLL  109.2792\n",
      "TEST Batch 0000/117, Loss  100.4437, NLL-Loss  100.4437\n",
      "TEST Batch 0050/117, Loss  115.9606, NLL-Loss  115.9606\n",
      "TEST Batch 0100/117, Loss  137.2293, NLL-Loss  137.2293\n",
      "TEST Batch 0117/117, Loss  161.4042, NLL-Loss  161.4042\n",
      "TEST Epoch 89/300, Mean NLL  109.4152\n",
      "TRAIN Batch 0000/1314, Loss  110.3394, NLL-Loss  110.3394\n",
      "TRAIN Batch 0050/1314, Loss  107.3890, NLL-Loss  107.3890\n",
      "TRAIN Batch 0100/1314, Loss  104.7330, NLL-Loss  104.7330\n",
      "TRAIN Batch 0150/1314, Loss   94.2076, NLL-Loss   94.2076\n",
      "TRAIN Batch 0200/1314, Loss   97.7378, NLL-Loss   97.7378\n",
      "TRAIN Batch 0250/1314, Loss  101.2559, NLL-Loss  101.2559\n",
      "TRAIN Batch 0300/1314, Loss  111.3532, NLL-Loss  111.3532\n",
      "TRAIN Batch 0350/1314, Loss   99.9921, NLL-Loss   99.9921\n",
      "TRAIN Batch 0400/1314, Loss  106.8558, NLL-Loss  106.8558\n",
      "TRAIN Batch 0450/1314, Loss  108.0693, NLL-Loss  108.0693\n",
      "TRAIN Batch 0500/1314, Loss  102.9636, NLL-Loss  102.9636\n",
      "TRAIN Batch 0550/1314, Loss   82.1122, NLL-Loss   82.1122\n",
      "TRAIN Batch 0600/1314, Loss  106.9611, NLL-Loss  106.9611\n",
      "TRAIN Batch 0650/1314, Loss   99.3588, NLL-Loss   99.3588\n",
      "TRAIN Batch 0700/1314, Loss   92.4931, NLL-Loss   92.4931\n",
      "TRAIN Batch 0750/1314, Loss  102.4673, NLL-Loss  102.4673\n",
      "TRAIN Batch 0800/1314, Loss  119.1701, NLL-Loss  119.1701\n",
      "TRAIN Batch 0850/1314, Loss  104.7351, NLL-Loss  104.7351\n",
      "TRAIN Batch 0900/1314, Loss  119.4431, NLL-Loss  119.4431\n",
      "TRAIN Batch 0950/1314, Loss   92.8753, NLL-Loss   92.8753\n",
      "TRAIN Batch 1000/1314, Loss  112.2670, NLL-Loss  112.2670\n",
      "TRAIN Batch 1050/1314, Loss  126.1036, NLL-Loss  126.1036\n",
      "TRAIN Batch 1100/1314, Loss  108.6353, NLL-Loss  108.6353\n",
      "TRAIN Batch 1150/1314, Loss  108.5254, NLL-Loss  108.5254\n",
      "TRAIN Batch 1200/1314, Loss   89.1638, NLL-Loss   89.1638\n",
      "TRAIN Batch 1250/1314, Loss  113.4354, NLL-Loss  113.4354\n",
      "TRAIN Batch 1300/1314, Loss   92.2153, NLL-Loss   92.2153\n",
      "TRAIN Batch 1314/1314, Loss  138.4618, NLL-Loss  138.4618\n",
      "TRAIN Epoch 90/300, Mean NLL  102.7861\n",
      "Model saved at bin/2019-May-17-21:02:23/E90.pytorch\n",
      "VALID Batch 0000/105, Loss  133.0894, NLL-Loss  133.0894\n",
      "VALID Batch 0050/105, Loss  123.4744, NLL-Loss  123.4744\n",
      "VALID Batch 0100/105, Loss   96.6335, NLL-Loss   96.6335\n",
      "VALID Batch 0105/105, Loss   98.2737, NLL-Loss   98.2737\n",
      "VALID Epoch 90/300, Mean NLL  109.2510\n",
      "TEST Batch 0000/117, Loss  100.5743, NLL-Loss  100.5743\n",
      "TEST Batch 0050/117, Loss  115.7075, NLL-Loss  115.7075\n",
      "TEST Batch 0100/117, Loss  137.2367, NLL-Loss  137.2367\n",
      "TEST Batch 0117/117, Loss  161.5375, NLL-Loss  161.5375\n",
      "TEST Epoch 90/300, Mean NLL  109.3625\n",
      "TRAIN Batch 0000/1314, Loss  103.6335, NLL-Loss  103.6335\n",
      "TRAIN Batch 0050/1314, Loss  112.7679, NLL-Loss  112.7679\n",
      "TRAIN Batch 0100/1314, Loss  105.0629, NLL-Loss  105.0629\n",
      "TRAIN Batch 0150/1314, Loss  105.5742, NLL-Loss  105.5742\n",
      "TRAIN Batch 0200/1314, Loss  109.3326, NLL-Loss  109.3326\n",
      "TRAIN Batch 0250/1314, Loss   90.2076, NLL-Loss   90.2076\n",
      "TRAIN Batch 0300/1314, Loss  110.5916, NLL-Loss  110.5916\n",
      "TRAIN Batch 0350/1314, Loss  103.7839, NLL-Loss  103.7839\n",
      "TRAIN Batch 0400/1314, Loss   96.3758, NLL-Loss   96.3758\n",
      "TRAIN Batch 0450/1314, Loss   87.6674, NLL-Loss   87.6674\n",
      "TRAIN Batch 0500/1314, Loss  104.1741, NLL-Loss  104.1741\n",
      "TRAIN Batch 0550/1314, Loss  102.6170, NLL-Loss  102.6170\n",
      "TRAIN Batch 0600/1314, Loss  100.4723, NLL-Loss  100.4723\n",
      "TRAIN Batch 0650/1314, Loss   89.6772, NLL-Loss   89.6772\n",
      "TRAIN Batch 0700/1314, Loss  105.9268, NLL-Loss  105.9268\n",
      "TRAIN Batch 0750/1314, Loss  110.3498, NLL-Loss  110.3498\n",
      "TRAIN Batch 0800/1314, Loss   93.2685, NLL-Loss   93.2685\n"
     ]
    }
   ],
   "source": [
    "!python trainRNN.py --test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from ./bin/2019-May-17-03:14:19/E1.pytorch\n",
      "----------SAMPLES----------\n",
      "the <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\n",
      "the <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\n",
      "the <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\n",
      "the <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\n",
      "the <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\n",
      "the <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\n",
      "the <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\n",
      "the <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\n",
      "the <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\n",
      "the <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\n",
      "-------INTERPOLATION-------\n",
      "the <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\n",
      "the <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\n",
      "the <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\n",
      "the <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\n",
      "the <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\n",
      "the <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\n",
      "the <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\n",
      "the <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\n",
      "the <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\n",
      "the <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\n"
     ]
    }
   ],
   "source": [
    "!python inferenceRNN.py -c ./bin/2019-May-17-03\\:14\\:19/E1.pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorBoard 1.11.0 at http://yul358-13135:6006 (Press CTRL+C to quit)\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "# see tensorboard\n",
    "!tensorboard --logdir=./logs/BS\\=32_LR\\=0.001_EB\\=300_GRU_HS\\=256_L\\=1_BI\\=0_LS\\=16_WD\\=0_ANN\\=LOGISTIC_K\\=0.0025_X0\\=2500_TS\\=2019-May-16-03\\:39\\:19/events.out.tfevents.1557977965.yul358-13135"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from ./bin/2019-May-16-04:42:09/E9.pytorch\n",
      "----------SAMPLES----------\n",
      "warner said it filed in principle to acquire the acquisition of qintex australia ' s creditors <eos>\n",
      "but if you ' re trying to get a beating in the next few weeks <eos>\n",
      "in addition to the <unk> of the <unk> <unk> and <unk> <unk> that have been plagued together to seize the <unk> of the <unk> of the <unk> <unk> of the <unk> <unk> of the <unk> <unk> of the <unk> <unk> of the <unk> <unk> of the <unk> <unk> of the\n",
      "the fed ' s economic policy suggests that the fed has been a <unk> of the <unk> <eos>\n",
      "<unk> co . said it will acquire n million shares including <unk> <unk> & co . and <unk> <unk> <eos>\n",
      "the <unk> of the <unk> <unk> fund which has been <unk> by the <unk> of the <unk> <unk> and <unk> <unk> <unk> <unk> <unk> <eos>\n",
      "i do n't know what i ' m not going to be very careful <eos>\n",
      "foothills said it wants to make it easier to make it possible to make it possible to make it possible to make it possible to make it possible to make it possible to make it possible to make it possible to make it possible to make it possible to make\n",
      "but the u . s . is imported a <unk> <unk> <unk> with the <unk> <unk> <eos>\n",
      "in the past five years he said that the company ' s earnings were <unk> to the stock market <eos>\n",
      "-------INTERPOLATION-------\n",
      "ibm said it will focus on the <unk> <eos>\n",
      "ibm said it will focus on the market <eos>\n",
      "<unk> said it will remain on the basis <eos>\n",
      "but analysts say the company has n't yet been determined <eos>\n",
      "but the company has n't yet been determined <eos>\n",
      "but the company ' s earnings growth is n't expected to be a negative impact on the horizon <eos>\n",
      "but the <unk> of the underlying financial markets <eos>\n",
      "but the market was n't disclosed <eos>\n",
      "but the fund ' s earnings estimate will be more than n n <eos>\n",
      "but the fed ' s junk bond portfolio <eos>\n"
     ]
    }
   ],
   "source": [
    "# use original code Bowman\n",
    "!python inference.py -c ./bin/2019-May-16-04\\:42\\:09/E9.pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
